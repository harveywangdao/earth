useradd:
sudo groupadd groupname
sudo groupdel groupname
sudo useradd ¨Cd /usr/sam -m sam
sudo userdel sam

useradd -m thomas -s /bin/bash
passwd thomas
#vim /etc/sudoers
visudo ctrl+o enter ctrl+x
thomas  ALL=(root) NOPASSWD:ALL
cat /etc/passwd
userdel -r thomas
chroot

samba:
sudo apt-get update
sudo apt-get install samba samba-common
sudo mkdir /home/share
sudo chmod 777 /home/share
sudo vim /etc/samba/smb.conf
security = user
[myshare]
  comment = my share directory
  path = /home/share
  browseable = yes
  writable = yes
sudo useradd smbuser
sudo smbpasswd -a smbuser
sudo service smbd restart

sudo smbpasswd -x username

sudo apt-get install smbclient
smbclient -L //192.168.0.111
smbclient //192.168.0.111/myshare -U thomas

smbd:
TCP 139: NetBIOS Session Service 用于文件和打印机共享
TCP 445: Server Message Block (SMB) 用于文件和打印机共享
nmbd:
UDP 137: NetBIOS Name Service 用于计算机名称解析
UDP 138: NetBIOS Datagram Service 用于广播和组播数据传输

disk:
sudo gedit /etc/default/apport
sudo rm -rf /var/crash
sudo sed -i 's/enabled=1/enabled=0/g' /etc/default/apport
sudo df -hl
sudo du -sh dir
sudo du -ah --max-depth=1
sudo fdisk -l
ls -l --block-size=g
ls -hl
sudo du -sh
sudo du -sh * | sort -nr | head
sudo du -h --max-depth=1
sudo df -hl

apt-get:
apt-get update
apt-get install packagename
apt-get install packagename --reinstall
apt-get -f install
apt-get remove packagename
apt-get remove --purge packagename
apt-get autoremove packagename
apt-get autoremove --purge packagname
apt-get autoclean
apt-get clean 
apt-get upgrade
apt-get dist-upgrade
apt-cache search package
apt-cache show package
apt-cache depends package
apt-cache rdepends package
apt-get build-dep package
apt-get source package
apt-get clean && sudo apt-get autoclean
apt-get install --reinstall ca-certificates

dpkg -l
dpkg --force-all --purge packagename
apt-cache search onlyoffice-documentserver
apt-cache show onlyoffice-documentserver

cat <<EOF >/etc/apt/sources.list
deb http://mirrors.aliyun.com/ubuntu/ xenial main
deb-src http://mirrors.aliyun.com/ubuntu/ xenial main
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main
deb http://mirrors.aliyun.com/ubuntu/ xenial universe
deb-src http://mirrors.aliyun.com/ubuntu/ xenial universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-security main
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main
deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe
EOF

yum:
mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
yum clean all
yum makecache
yum install -y epel-release
yum install -y nginx
yum install -y supervisor
yum repolist all
yum repolist enabled
yum list | grep epel-release
#yum -y update
#yum -y upgrade
echo "export LC_ALL=en_US.UTF-8"  >>  /etc/profile
source /etc/profile

do-release-upgrade

zip:
zip xxx.zip dir
zip -r xxx.zip dir
unzip xxx.zip
tar -cvf xxx.tar dir
tar -xvf xxx.tar
tar -jcvf xxx.tar.bz2 dir
tar -jxvf xxx.tar.bz2 -C des_dir
tar -zcvf xxx.tar.gz dir
tar -zxvf xxx.tar.gz -C des_dir
xz -z xxx.tar.xz
xz -d xxx.tar.xz
tar -xvJf
unrar e file.rar
rar a xxx.rar dir

beego:
go get github.com/beego/bee
go get github.com/astaxie/beego
//golang.org/x/net/context
mkdir -p golang.org/x
git clone https://github.com/golang/net.git
go get github.com/mattn/go-sqlite3
go get github.com/Unknwon/com
go get -u github.com/beego/bee
go get -u github.com/astaxie/beego
bee fix
bee new projectname
bee run
localhost:8080

SSL/TLS:
go run $GOROOT/src/crypto/tls/generate_cert.go --host 127.0.0.1

PostgreSQL:
sudo apt-get update
sudo apt-get install postgresql postgresql-client
sudo -i -u postgres

sudo /etc/init.d/postgresql start
sudo /etc/init.d/postgresql stop
sudo /etc/init.d/postgresql restart

sudo su - postgres
psql
\password postgres

CREATE USER dbuser WITH PASSWORD 'password';
CREATE DATABASE exampledb OWNER dbuser;
GRANT ALL PRIVILEGES ON DATABASE exampledb to dbuser;

\q

psql -U dbuser -d exampledb -h 127.0.0.1 -p 5432

CREATE TABLE user_tbl(name VARCHAR(20), signup_date DATE);
INSERT INTO user_tbl(name, signup_date) VALUES('ÕÅÈý', '2013-12-22');
SELECT * FROM user_tbl;
UPDATE user_tbl set name = 'ÀîËÄ' WHERE name = 'ÕÅÈý';
DELETE FROM user_tbl WHERE name = 'ÀîËÄ' ;
ALTER TABLE user_tbl ADD email VARCHAR(40);
ALTER TABLE user_tbl ALTER COLUMN signup_date SET NOT NULL;
ALTER TABLE user_tbl RENAME COLUMN signup_date TO signup;
ALTER TABLE user_tbl DROP COLUMN email;
ALTER TABLE user_tbl RENAME TO backup_tbl;
DROP TABLE IF EXISTS backup_tbl;

sudo apt-get --purge remove postgresql-*
sudo rm -r /etc/postgresql/
sudo rm -r /etc/postgresql-common/
sudo rm -r /var/lib/postgresql/
sudo userdel -r postgres
sudo groupdel postgres
cat /etc/passwd | cut -f 1 -d:
grep bash /etc/passwd

sudo -i -u postgres
sudo -u postgres psql postgres
\password postgres

CREATE USER xiaoming WITH PASSWORD '123456';
CREATE DATABASE db01;
GRANT ALL PRIVILEGES ON DATABASE db01 TO xiaoming;

\c db01;
GRANT ALL PRIVILEGES ON all tables in schema public TO xiaoming;
GRANT USAGE,SELECT ON ALL SEQUENCES in schema public TO xiaoming;

\l
\d

psql -h 127.0.0.1 -p 5432 -U xiaoming db01
psql -h 127.0.0.1 -p 5432 -U postgres postgres

Schema
CREATE TYPE week AS ENUM ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun');
sequence

host=127.0.0.1 user=xiaoming password=123456 dbname=db01 port=5432 sslmode=disable

RabbitMQ && Erlang:
#sudo dpkg -i esl-erlang_22.2.2-1~ubuntu~xenial_amd64.deb
wget https://packages.erlang-solutions.com/erlang-solutions_2.0_all.deb
sudo dpkg -i erlang-solutions_2.0_all.deb
wget https://packages.erlang-solutions.com/ubuntu/erlang_solutions.asc
sudo apt-key add erlang_solutions.asc

sudo apt-get update
sudo apt-get install esl-erlang

erl --version

#wget https://packagecloud.io/install/repositories/rabbitmq/rabbitmq-server/script.deb.sh | sudo bash
sudo dpkg -i rabbitmq-server_3.8.2-1_all.deb

sudo service rabbitmq-server stop
sudo service rabbitmq-server start

sudo rabbitmqctl status
sudo rabbitmqctl list_queues

sudo rabbitmq-plugins enable rabbitmq_management

rabbitmqctl list_users
rabbitmqctl list_user_permissions test
rabbitmqctl list_permissions -p /test
rabbitmqctl clear_permissions -p  /test  test
rabbitmqctl delete_user test
rabbitmqctl delete_vhost /test

集群部署:
普通模式:
sudo vim .erlang.cookie
chmod 600 .erlang.cookie
sudo rabbitmqctl stop_app
sudo rabbitmqctl join_cluster rabbit@node01
sudo rabbitmqctl start_app
sudo rabbitmqctl cluster_status

高可用镜像: Quorum Queues:durable replicated raft
sudo rabbitmqctl set_policy ha-all "^" '{"ha-mode":"all"}'

Maven:
sudo mkdir /usr/local/maven
sudo tar -xzvf apache-maven-3.6.3-bin.tar.gz -C /usr/local/maven/

sudo vim /etc/profile
export M2_HOME=/usr/local/maven/apache-maven-3.6.3
export CLASSPATH=$CLASSPATH:$M2_HOME/lib
export PATH=$PATH:$M2_HOME/bin

mkdir -p /home/thomas/java/maven
sudo vim /usr/local/maven/apache-maven-3.6.3/conf/settings.xml
<localRepository>/home/thomas/java/maven</localRepository>

<mirror>
<id>nexus-aliyun</id>
<mirrorOf>central</mirrorOf>
<name>Nexus aliyun</name>
<url>http://maven.aliyun.com/nexus/content/groups/public</url>
</mirror>

RocketMQ:
mkdir -p /home/thomas/java/rocketmq
cp rocketmq-all-4.6.0-source-release.zip /home/thomas/java/rocketmq/
cd /home/thomas/java/rocketmq
unzip rocketmq-all-4.6.0-source-release.zip
rm rocketmq-all-4.6.0-source-release.zip
cd /home/thomas/java/rocketmq/rocketmq-all-4.6.0-source-release

mvn -Prelease-all -DskipTests clean install -U

cd distribution/target/rocketmq-4.6.0/rocketmq-4.6.0
vim bin/runbroker.sh
vim bin/runserver.sh
vim bin/tools.sh
JAVA_OPT="${JAVA_OPT} -server -Xms128m -Xmx256m -Xmn256m"

nohup sh bin/mqnamesrv &
nohup sh bin/mqbroker -n localhost:9876 &

export NAMESRV_ADDR=localhost:9876
sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer
sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer

sh bin/mqshutdown broker
sh bin/mqshutdown namesrv

sz:
tar czf lib.tar.gz lib
sz lib.tar.gz
telnet 10.13.82.101 873

pprof:
http://192.168.197.128:6060/debug/pprof/
http://192.168.197.128:5651/ui/flamegraph
go tool pprof http://192.168.197.128:6060/debug/pprof/profile?seconds=10
go tool pprof http://192.168.197.128:6060/debug/pprof/heap
go tool pprof http://192.168.197.128:6060/debug/pprof/goroutine
go tool pprof http://192.168.197.128:6060/debug/pprof/block
go tool pprof -http="0.0.0.0:5651" /home/thomas/golang/src/test/ppproftest/dangdang /home/thomas/pprof/pprof.dangdang.samples.cpu.002.pb.gz
lsof -i tcp:6060

tcpdump:
sudo tcpdump -i ens160
sudo tcpdump -i ens160 host account.wps.cn
sudo tcpdump -i ens160 host account.wps.cn and \(node2 or node3\)
sudo tcpdump -i ens160 dst host account.wps.cn

sudo tcpdump -s 0 -A -i ens160 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420'
sudo tcpdump -s 0 -A -i ens160 'tcp dst port 80 and (tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354)'
tcpdump过滤HTTP的请求和响应头信息,以及请求和响应消息体信息:
sudo tcpdump -A -s 0 -i ens160 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'
sudo tcpdump -X -s 0 -i ens160 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'

GET
sudo tcpdump -vvennSs 0 -i ens160 'tcp[((tcp[12:1]&0xf0) >> 2):4]=0x47455420' and host account.wps.cn
POST
sudo tcpdump -vvennSs 0 -i ens160 'tcp[((tcp[12:1]&0xf0) >> 2):4]=0x504f5354' and host account.wps.cn

sudo tcpdump -vvennSs 0 -i ens160 \('tcp[((tcp[12:1] & 0xf0) >> 2):4]=0x47455420' or 'tcp[((tcp[12:1]&0xf0) >> 2):4]=0x504f5354'\) and host account.wps.cn
sudo tcpdump -vvennSs 0 -i ens160 \('tcp[((tcp[12:1] & 0xf0) >> 2):4]=0x47455420' or 'tcp[((tcp[12:1]&0xf0) >> 2):4]=0x504f5354'\) and host 110.43.90.34

sysctl
sysctl -a

cd /proc/sys

永久修改:
vim /etc/sysctl.conf
net.ipv4.ip_forward=1
sysctl -p

临时修改:
sysctl -w net.ipv4.ip_forward=1
echo 1 > /proc/sys/net/ipv4/ip_forward

sysctl -a | grep "net.ipv4.ip_forward"

/proc/sys/net/ipv4/tcp_max_syn_backlog 对于还未获得对方确认的连接请求，可保存在队列中的最大数目。如果服务器经常出现过载，可以尝试增加这个数字
/proc/sys/net/ipv4/tcp_fin_timeout 本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间秒,对方可能会断开连接或一直不结束连接或不可预料的进程死亡
/proc/sys/net/ipv4/tcp_tw_recycle 能够更快地回收TIME-WAIT套接字
/proc/sys/net/ipv4/tcp_timestamps 启用时间戳,一般不开启
/proc/sys/net/ipv4/tcp_tw_reuse 表示是否允许将处于TIME-WAIT状态的socket（TIME-WAIT的端口）用于新的TCP连接

http-ping

ulimit:
ulimit -a
ulimit -n
进程可以打开的文件数目: ulimit -n 4096
数据段长度: ulimit -d unlimited
最大内存大小: ulimit -m unlimited
堆栈大小: ulimit -s unlimited
CPU 时间: ulimit -t unlimited
虚拟内存: ulimit -v unlimited
-u 用户最多可启动的进程数目

netstat:
yum install net-tools -y
apt-get install net-tools -y

netstat -s | grep -i listen

netstat -r 路由信息

traceroute
sudo apt-get install traceroute
traceroute www.baidu.com
traceroute -n -m 5 -q 4 -w 3 www.baidu.com

nslookup
www.sohu.com

nslookup www.baidu.com
nslookup www.baidu.com 114.114.114.114
nslookup 192.168.44.165 192.168.44.165

nslookup -type=type domain [dns-server]
A 地址记录(Ipv4)
AAAA 地址记录（Ipv6）
AFSDB Andrew文件系统数据库服务器记录
ATMA ATM地址记录
CNAME 别名记录
HINFO 硬件配置记录，包括CPU、操作系统信息
ISDN 域名对应的ISDN号码
MB 存放指定邮箱的服务器
MG 邮件组记录
MINFO 邮件组和邮箱的信息记录
MR 改名的邮箱记录
MX 邮件服务器记录
NS 名字服务器记录
PTR 反向记录
RP 负责人记录
RT 路由穿透记录
SRV TCP服务器信息记录
TXT 域名对应的文本信息
X25 域名对应的X.25地址记录

nslookup -type=mx www.baidu.com 8.8.8.8
nslookup -type=ns www.baidu.com
nslookup -type=ptr 14.18.240.22

dig
dig baidu.com
dig baidu.com CNAME
dig @8.8.8.8 abc.filterinto.com
cat /etc/resolv.conf

dig -x 8.8.8.8 +short
dig -x 39.156.69.79

dig +trace baidu.com
dig baidu.com NS +noall +answer
dig baidu.com ANY +noall +answer

host www.baidu.com
host 192.168.25.26

telnet
yum install telnet –y
telnet 192.168.1.2 8080

route
cat /etc/rc.local
route -n

ffmpeg
ffmpeg -i wind.mp3 what.mp3
ffmpeg -i wind.mp3 -f mp3 what.mp3
ffmpeg -i ./wind.mp3 -f mp3 -acodec libmp3lame ./what.mp3
ffmpeg -i foo.mp4 foobar.mp3
ffmpeg -i wind.mp3 what.wav
ffmpeg -i what.wav wind2.mp3

linux版本
cat /etc/os-release
cat /etc/issue
cat /etc/redhat-release
cat /proc/version
lsb_release -a
cat /etc/lsb-release
uname -a

openssl:
#openssl genrsa -out server.key 2048
openssl ecparam -genkey -name secp384r1 -out server.key
openssl req -new -x509 -sha256 -key server.key -out server.pem -days 36500

openssl genrsa -out ca.key 2048
openssl req -new -x509 -key ca.key -out ca.pem -days 7200

openssl ecparam -genkey -name secp384r1 -out server.key
openssl req -new -key server.key -out server.csr
openssl x509 -req -sha256 -CA ca.pem -CAkey ca.key -CAcreateserial -days 3650 -in server.csr -out server.pem

openssl ecparam -genkey -name secp384r1 -out client.key
openssl req -new -key client.key -out client.csr
openssl x509 -req -sha256 -CA ca.pem -CAkey ca.key -CAcreateserial -days 3650 -in client.csr -out client.pem

keytool -genkey -alias test -keyalg RSA -keystore testKeyStore.jks -validity 3650 -keysize 2048
keytool -list  -v -keystore testKeyStore.jks
keytool -export -alias test -keystore testKeyStore.jks -file testKeyStore.crt
keytool -export -alias test -keystore testKeyStore.jks -rfc -file testKeyStorePem.crt
keytool -alias test -v -importkeystore -srckeystore testKeyStore.jks -srcstoretype jks -destkeystore testKeyStore.pfx -deststoretype pkcs12
openssl pkcs12 -in testKeyStore.pfx -nocerts -nodes -out testKeyStore.key

keytool -genkey -alias test -keysize 2048 -validity 3650 -keyalg RSA -dname "CN=Aclie, OU=Developer,O=Iwgame, L=Shanghai, S=Shanghai, C=CH" -keypass 123456 -storepass 123456 -keystore clientKeys.jks
keytool -export -alias test -keystore clientKeys.jks -file client.cer -storepass 123456
keytool -export -alias test -keystore clientKeys.jks -rfc -file clientpem.cer -storepass 123456

keytool -genkey -alias test -keysize 2048 -validity 3650 -keyalg RSA -dname "CN=Aclie, OU=Developer,O=Iwgame, L=Shanghai, S=Shanghai, C=CH" -keypass 123456 -storepass 123456 -keystore serverKeys.jks
keytool -export -alias test -keystore serverKeys.jks -file server.cer -storepass 123456
keytool -export -alias test -keystore serverKeys.jks -rfc -file serverpem.cer -storepass 123456

keytool -import -alias test -keystore clientTrust.jks -file server.cer -keypass 123456 -storepass 123456
keytool -import -alias test -keystore serverTrust.jks -file client.cer -keypass 123456 -storepass 123456

keytool -genkey -alias test -keypass 123456 -keyalg RSA -keysize 1024 -validity 120 -storetype PKCS12 -keystore client.keystore -storepass 123456
keytool -export -alias test -keystore client.keystore -storetype PKCS12 -keypass 123456 -file client.cer
keytool -import -v -file client.cer -keystore server.keystore -storepass 123456

keytool -genkeypair -alias ddssingsong -keyalg RSA -keysize 2048 -keypass 123456 -sigalg SHA256withRSA -dname "cn=www.ddssingsong.com,ou=xxx,o=xxx,l=Beijing,st=Beijing,c=CN" -validity 3650 -keystore ddssingsong.p12 -storetype PKCS12 -storepass 123456
keytool -list -v -keystore ddssingsong.p12 -storepass 123456
keytool -list -rfc -keystore ddssingsong.p12 -storepass 123456
keytool -export -alias ddssingsong -keystore ddssingsong.p12 -file rootca.crt -storepass 123456
keytool -printcert -file rootca.crt

生成请求证书:
keytool -certreq -keyalg RSA -alias ddssingsong -keystore ddssingsong.p12 -storetype PKCS12 -storepass 123456 -file certreq.csr
keytool -printcertreq -file certreq.csr

openssl s_server -msg -verify -tls1_2 -state -cert server.crt -key server.key -accept 18444
openssl s_client -msg -verify -tls1_2 -state -showcerts -cert server.crt -key server.key -connect localhost:18444

openssl genrsa -out client.key 2048
openssl req -new -key client.key -out client.csr
openssl x509 -req -days 365 -in client.csr -signkey client.key -out client.crt
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=abc.com" -days 5000 -out ca.crt
openssl req -new -x509 -newkey rsa:2048 -keyout client.key -out client.crt

openssl enc -d -aes256 -in b.tar.gz -out a.tar.gz
openssl enc -e -aes256 -in a.tar.gz -out b.tar.gz

Redis:
sudo apt-get install redis-server
ps -aux|grep redis
netstat -nlt|grep 6379
sudo /etc/init.d/redis-server status
sudo /etc/init.d/redis-server restart
sudo /etc/init.d/redis-server start
sudo /etc/init.d/redis-server stop

sudo vi /etc/redis/redis.conf
requirepass redisredis
#bind 127.0.0.1
sudo /etc/init.d/redis-server restart

redis-cli -a 180498
redis-cli -h host -p port -a password

set key1 "hello"
get key1

set key2 1
INCR key2

LPUSH key3 a
LPUSH key3 b
LRANGE key3 0 3

HSET key4 name "John Smith"
HSET key4 email "abc@gmail.com"
HGET key4 name
HGETALL key4

del key1

主从、哨兵、集群
string -- int，raw或者embstr
hash   -- ziplist 或者 hashtable
list   -- ziplist(压缩列表) 和 linkedlist(双端链表)
set    -- intset 或者 hashtable
sorted set  --  ziplist 或者 skiplist
事务
hyperloglog
pub/sub
lua
pipelining

FIFO：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。
LRU：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。
LFU：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰。
redis缓存击穿(热点key不失效或加锁)、缓存穿透(不存在的key设置空value和过期时间)、缓存雪崩(key失效时间不同)

分布式锁
redlock
持久化RDB AOF

普通redis分布式锁：
上锁 set key value px 30000 nx # value是唯一值
解锁 先判断value是否对再删key，可用lua脚本保证原子性
if redis.call("get",KEYS[1]) == ARGV[1] then
  return redis.call("del",KEYS[1])
else
  return 0
end

存在单点问题

redlock：
上锁 多个master节点，相同key和唯一value，响应超时时间50ms，30s-获取锁的时间，只要在一个节点上获取锁失败就要在其它节点释放锁
所有节点key的超时时间都是30s，但由于设置key本身也需要时间，所以实际超时时间是30s-设置key时间

1.未上锁          获取锁
2.已上锁          锁失败
3.redis节点不够   锁失败，删除所有节点key
4.响应时间太长    锁失败，删除所有节点key

1.正常用完        释放锁，删除所有节点key
2.服务挂了        所有节点key等待超时
3.服务没挂但超时了 所有节点key，但要判断value，防止删除别人的锁

1.客户端分区方案 Redis Sharding 根据key的hash映射到哪个节点，主从、哨兵，客户端无法动态增删节点，自行维护分发逻辑
2.代理分区方案 Twemproxy Codis 性能低
3.查询路由方案

顺序分区
哈希分区
1.节点取余分区
2.一致性哈希分区
3.虚拟槽分区 2^14 0 ~ 16383 slot = CRC16（key）& 16383 限制 key批量操作 key事务操作

mkdir -p /home/thomas/server/redis-cluster
cd /home/thomas/server/redis-cluster
mkdir conf data log
mkdir -p data/redis-6379 data/redis-6389 data/redis-6380 data/redis-6390 data/redis-6381 data/redis-6391
cp ~/server/redis-4.0.14/redis.conf ./conf/
cp redis.conf redis-7001.conf

daemonize yes
bind 127.0.0.1
cluster-enabled yes
cluster-node-timeout 10000
appendonly yes
port 6379
cluster-config-file /home/thomas/server/redis-cluster/conf/nodes-6379.conf
dir /home/thomas/server/redis-cluster/data/redis-6379
pidfile /var/run/redis-cluster/redis-6379.pid
logfile /home/thomas/server/redis-cluster/log/redis-6379.log

sudo apt-get install build-essential
make
cp redis-4.0.14/src/redis-trib.rb /home/thomas/server/redis-cluster/
sudo apt-get install ruby
sudo gem install redis

cd /home/thomas/server/redis-4.0.14/src
./redis-server /home/thomas/server/redis-cluster/conf/redis-6379.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-6380.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-6381.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-6389.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-6390.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-6391.conf

#./redis-cli -p 6379 shutdown

./redis-trib.rb create --replicas 1 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6389 127.0.0.1:6390 127.0.0.1:6391

Adding replica 127.0.0.1:6390 to 127.0.0.1:6379
Adding replica 127.0.0.1:6391 to 127.0.0.1:6380
Adding replica 127.0.0.1:6389 to 127.0.0.1:6381

./redis-cli -c -h 127.0.0.1 -p 6379
cluster nodes
cluster info

加节点：
cp redis-6379.conf redis-6385.conf
cp redis-6379.conf redis-6395.conf
mkdir -p data/redis-6385 data/redis-6395

./redis-server /home/thomas/server/redis-cluster/conf/redis-6385.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-6395.conf

./redis-trib.rb add-node 127.0.0.1:6385 127.0.0.1:6379
./redis-trib.rb reshard 127.0.0.1:6379

增加从节点：
./redis-trib.rb add-node --slave --master-id e77bd17d74a84079decfdeee6e75afd3867a44c0 127.0.0.1:6395 127.0.0.1:6379

删除节点：
./redis-trib.rb reshard 127.0.0.1:6379
./redis-trib.rb del-node 127.0.0.1:6385 e77bd17d74a84079decfdeee6e75afd3867a44c0
./redis-trib.rb del-node 127.0.0.1:6395 1411045e449ff84c13ccc548919eda65e2e9dbd1

单点主从、哨兵搭建：
mkdir -p data/redis-7001 data/redis-8001

daemonize yes
bind 127.0.0.1
appendonly no
port 7001
logfile /home/thomas/server/redis-cluster/log/redis-7001.log
pidfile /var/run/redis-cluster/redis-7001.pid
dir /home/thomas/server/redis-cluster/data/redis-7001

slaveof 127.0.0.1 7001

./redis-server /home/thomas/server/redis-cluster/conf/redis-7001.conf
./redis-cli -p 7001
./redis-server /home/thomas/server/redis-cluster/conf/redis-8001.conf
./redis-cli -p 8001

哨兵：
cp ~/server/redis-4.0.14/sentinel.conf ./conf/
cp sentinel.conf sentinel-9001.conf
mkdir -p data/sentinel-9001 data/sentinel-9002 data/sentinel-9003

port 9001
daemonize yes
logfile /home/thomas/server/redis-cluster/log/sentinel-9001.log
dir /home/thomas/server/redis-cluster/data/sentinel-9001
sentinel monitor mymaster 127.0.0.1 7001 2
#sentinel auth-pass mymaster abc
sentinel down-after-milliseconds mymaster 1500
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 30000

cp sentinel-9001.conf sentinel-9002.conf
cp sentinel-9001.conf sentinel-9003.conf

./redis-sentinel /home/thomas/server/redis-cluster/conf/sentinel-9001.conf
./redis-sentinel /home/thomas/server/redis-cluster/conf/sentinel-9002.conf
./redis-sentinel /home/thomas/server/redis-cluster/conf/sentinel-9003.conf

./redis-cli -p 9001
info sentinel
sentinel masters
sentinel slaves mymaster
sentinel get-master-addr-by-name mymaster

./redis-cli -p 8001 shutdown 没什么变化
./redis-cli -p 7001 shutdown

cp ~/server/redis-4.0.14/redis.conf ./conf/
cp redis.conf redis-7001.conf

daemonize yes
bind 0.0.0.0
port 7001
logfile /home/thomas/server/redis-cluster/log/redis-7001.log
pidfile /var/run/redis-cluster/redis-7001.pid
dir /home/thomas/server/redis-cluster/data/redis-7001

mkdir -p data/redis-7001 data/redis-7002 data/redis-7003 data/redis-7004 data/redis-7005

cp redis-7001.conf redis-7002.conf
cp redis-7001.conf redis-7003.conf
cp redis-7001.conf redis-7004.conf
cp redis-7001.conf redis-7005.conf

./redis-server /home/thomas/server/redis-cluster/conf/redis-7001.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-7002.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-7003.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-7004.conf
./redis-server /home/thomas/server/redis-cluster/conf/redis-7005.conf

./redis-cli -p 7001

MongoDB:
sudo apt-get install mongodb

sudo service mongodb start
sudo service mongodb restart
sudo service mongodb stop
systemctl status mongodb.service

sudo vim /etc/mongodb.conf

pgrep mongo -l
sudo apt-get --purge remove mongodb mongodb-clients mongodb-server
sudo apt-get purge mongodb-org*
sudo rm -rf /var/log/mongodb
sudo rm -rf /var/lib/mongodb

show dbs
show collections
show users
use yourDB
db.help()
db.yourCollection.help()
db.createCollection('teacher')
db.student.insert({_id:1, sname: 'zhangsan', sage: 20})
db.student.save({_id:1, sname: 'zhangsan', sage: 22})
db.student.find()
db.student.find({sname: 'lisi'})
db.student.update({sname: 'lisi'}, {$set: {sage: 30}}, false, true)
db.student.remove({sname: 'chenliu'})

mongod --dbpath "D:\mongo\data\db" --logpath "D:\mongo\data\log\mongodb.log" --install --serviceName "mongodb"
NET START mongodb
NET stop mongodb

搭建集群：
sudo apt-get install libcurl3 openssl
tar -xzvf mongodb-linux-x86_64-ubuntu1604-3.6.14.tgz

mkdir -p /home/thomas/server/mongo-cluster/conf
cd /home/thomas/server/mongo-cluster/conf
touch mongod.conf
{
storage:
    engine: wiredTiger
    directoryPerDB: true
    journal:
        enabled: true
systemLog:
    destination: file
    logAppend: true
operationProfiling:
  slowOpThresholdMs: 10000
replication:
    oplogSizeMB: 10240
processManagement:
    fork: true
security:
    authorization: "enabled"
}

touch mongos.conf
{
systemLog:
    destination: file
    logAppend: true
processManagement:
    fork: true
}

mkdir -p /home/thomas/server/mongo-cluster/keyfile
cd /home/thomas/server/mongo-cluster/keyfile
sudo openssl rand -base64 756 > mongo.key
chmod 400 mongo.key

cd /home/thomas/server/mongo-cluster
mkdir -p mongos-10001/data mongos-10002/data mongos-10003/data \
      config-primary-10011/data config-secondary-10012/data config-secondary-10013/data \
      shard1-primary-10021/data shard1-secondary-10022/data shard1-secondary-10023/data \
      shard2-primary-10031/data shard2-secondary-10032/data shard2-secondary-10033/data

cd /home/thomas/server/mongodb-linux-x86_64-ubuntu1604-3.6.14/bin

KEYFILE=/home/thomas/server/mongo-cluster/keyfile/mongo.key
WORK_DIR=/home/thomas/server/mongo-cluster
CONFFILE=/home/thomas/server/mongo-cluster/conf/mongod.conf

Config Server:       --bind_ip 0.0.0.0
./mongod --port 10011 --bind_ip 0.0.0.0 --configsvr --replSet configReplSet --keyFile $KEYFILE --dbpath $WORK_DIR/config-primary-10011/data --pidfilepath $WORK_DIR/config-primary-10011/db.pid --logpath $WORK_DIR/config-primary-10011/db.log --config $CONFFILE

./mongod --port 10012 --bind_ip 0.0.0.0 --configsvr --replSet configReplSet --keyFile $KEYFILE --dbpath $WORK_DIR/config-secondary-10012/data --pidfilepath $WORK_DIR/config-secondary-10012/db.pid --logpath $WORK_DIR/config-secondary-10012/db.log --config $CONFFILE

./mongod --port 10013 --bind_ip 0.0.0.0 --configsvr --replSet configReplSet --keyFile $KEYFILE --dbpath $WORK_DIR/config-secondary-10013/data --pidfilepath $WORK_DIR/config-secondary-10013/db.pid --logpath $WORK_DIR/config-secondary-10013/db.log --config $CONFFILE

./mongo --port 10011 --host 127.0.0.1
cfg = {
    _id:"configReplSet", 
    configsvr: true,
    members:[
        {_id:0, host:'127.0.0.1:10011'},
        {_id:1, host:'127.0.0.1:10012'}, 
        {_id:2, host:'127.0.0.1:10013'}
    ]};
rs.initiate(cfg);
rs.status()

Shard1:
./mongod --port 10021 --bind_ip 0.0.0.0 --shardsvr --replSet shard1 --keyFile $KEYFILE --dbpath $WORK_DIR/shard1-primary-10021/data --pidfilepath $WORK_DIR/shard1-primary-10021/db.pid --logpath $WORK_DIR/shard1-primary-10021/db.log --config $CONFFILE

./mongod --port 10022 --bind_ip 0.0.0.0 --shardsvr --replSet shard1 --keyFile $KEYFILE --dbpath $WORK_DIR/shard1-secondary-10022/data --pidfilepath $WORK_DIR/shard1-secondary-10022/db.pid --logpath $WORK_DIR/shard1-secondary-10022/db.log --config $CONFFILE

./mongod --port 10023 --bind_ip 0.0.0.0 --shardsvr --replSet shard1 --keyFile $KEYFILE --dbpath $WORK_DIR/shard1-secondary-10023/data --pidfilepath $WORK_DIR/shard1-secondary-10023/db.pid --logpath $WORK_DIR/shard1-secondary-10023/db.log --config $CONFFILE

./mongo --port 10021 --host 127.0.0.1
cfg = {
    _id:"shard1", 
    members:[
        {_id:0, host:'127.0.0.1:10021'},
        {_id:1, host:'127.0.0.1:10022'}, 
        {_id:2, host:'127.0.0.1:10023'}
    ]};
rs.initiate(cfg);

Shard2:
./mongod --port 10031 --bind_ip 0.0.0.0 --shardsvr --replSet shard2 --keyFile $KEYFILE --dbpath $WORK_DIR/shard2-primary-10031/data --pidfilepath $WORK_DIR/shard2-primary-10031/db.pid --logpath $WORK_DIR/shard2-primary-10031/db.log --config $CONFFILE

./mongod --port 10032 --bind_ip 0.0.0.0 --shardsvr --replSet shard2 --keyFile $KEYFILE --dbpath $WORK_DIR/shard2-secondary-10032/data --pidfilepath $WORK_DIR/shard2-secondary-10032/db.pid --logpath $WORK_DIR/shard2-secondary-10032/db.log --config $CONFFILE

./mongod --port 10033 --bind_ip 0.0.0.0 --shardsvr --replSet shard2 --keyFile $KEYFILE --dbpath $WORK_DIR/shard2-secondary-10033/data --pidfilepath $WORK_DIR/shard2-secondary-10033/db.pid --logpath $WORK_DIR/shard2-secondary-10033/db.log --config $CONFFILE

./mongo --port 10031 --host 127.0.0.1
cfg = {
    _id:"shard2", 
    members:[
        {_id:0, host:'127.0.0.1:10031'},
        {_id:1, host:'127.0.0.1:10032'}, 
        {_id:2, host:'127.0.0.1:10033'}
    ]};
rs.initiate(cfg);

Mongos:
KEYFILE=/home/thomas/server/mongo-cluster/keyfile/mongo.key
WORK_DIR=/home/thomas/server/mongo-cluster
CONFFILE=/home/thomas/server/mongo-cluster/conf/mongos.conf

./mongos --port=10001 --bind_ip 0.0.0.0 --configdb configReplSet/127.0.0.1:10011,127.0.0.1:10012,127.0.0.1:10013 --keyFile $KEYFILE --pidfilepath $WORK_DIR/mongos-10001/db.pid --logpath $WORK_DIR/mongos-10001/db.log --config $CONFFILE

./mongos --port=10002 --bind_ip 0.0.0.0 --configdb configReplSet/127.0.0.1:10011,127.0.0.1:10012,127.0.0.1:10013 --keyFile $KEYFILE --pidfilepath $WORK_DIR/mongos-10002/db.pid --logpath $WORK_DIR/mongos-10002/db.log --config $CONFFILE

./mongos --port=10003 --bind_ip 0.0.0.0 --configdb configReplSet/127.0.0.1:10011,127.0.0.1:10012,127.0.0.1:10013 --keyFile $KEYFILE --pidfilepath $WORK_DIR/mongos-10003/db.pid --logpath $WORK_DIR/mongos-10003/db.log --config $CONFFILE

./mongo --port 10001 --host 127.0.0.1

sh.addShard("shard1/127.0.0.1:10021")
sh.addShard("shard2/127.0.0.1:10031")

use admin
db.createUser({
    user:'admin',pwd:'123456',
    roles:[
        {role:'clusterAdmin',db:'admin'},
        {role:'userAdminAnyDatabase',db:'admin'},
        {role:'dbAdminAnyDatabase',db:'admin'},
        {role:'readWriteAnyDatabase',db:'admin'}
]})

use admin
db.auth('admin','123456')
sh.status()
rs.add(HOST_NAME:PORT)

use admin
db.system.users.find().pretty()
show users
db.dropUser("admin")
db.changeUserPassword('tank2','test')
db.updateUser("usertest",{roles:[ {role:"read",db:"testDB"} ]})
db.grantRolesToUser("usertest", [{role:"readWrite", db:"testDB"},{role:"read", db:"testDB"}])
db.revokeRolesFromUser("usertest",[{role:"read", db:"testDB"}])
db.shutdownServer()

use appdb
db.createUser({user:'appuser',pwd:'123456',roles:[{role:'dbOwner',db:'appdb'}]})
sh.enableSharding("appdb")

use appdb
db.createCollection("book")
db.device.ensureIndex({createTime:1})
sh.shardCollection("appdb.book", {bookId:"hashed"}, false, { numInitialChunks: 4} )

use appdb
var cnt = 0;
for(var i=0; i<1000; i++){
    var dl = [];
    for(var j=0; j<100; j++){
        dl.push({
                "bookId" : "BBK-" + i + "-" + j,
                "type" : "Revision",
                "version" : "IricSoneVB0001",
                "title" : "Jackson's Life",
                "subCount" : 10,
                "location" : "China CN Shenzhen Futian District",
                "author" : {
                      "name" : 50,
                      "email" : "RichardFoo@yahoo.com",
                      "gender" : "female"
                },
                "createTime" : new Date()
            });
      }
      cnt += dl.length;
      db.book.insertMany(dl);
      print("insert ", cnt);
}

db.book.getShardDistribution()

db.setting.save({"_id":"chunksize","value":1})
db.user.insert({"id":i,"name":"jack"+i})
sh.enableSharding("calon")

show dbs
db
db.dropDatabase()

db.createCollection(name, options)
show tables
show collections
db.collection.drop()

db.collection.find()
db.product.find().pretty()
db.product.find({"test5":"OK"}).pretty()
db.product.find({$or:[{"test5":"OK"},{"test8":"OK"}]}).pretty()
$lt $lte $gt $gte $ne $or $in $nin $all
db.product.find({"test5":{$type:'string'}}).pretty()
db.product.find({"test5":{$type:'string'}}).limit(1).pretty()
db.product.find({"test5":{$type:'string'}}).limit(2).skip(1).pretty()
db.product.find({"test5":{$type:'string'}}).sort({"test9":1}).pretty()
db.users.find({}, {'name' : 1, 'skills' : 1});
db.users.find({name:/hurry/}); select * from users where name like "%hurry%";
db.users.find({name:/^hurry/}); select * from users where name like "hurry%";
db.users.distinct('name');
db.users.count();
db.users.find({'skills' : {'$all' : ['java','python']}})
db.collection.find({ "field" : { $gt: value1, $lt: value2 } } );

db.collection.update() db.product.update({"product_name":"phone"}, {$set:{"brand":"apple"}}, {multi:true})
db.product.update({"price":{$gt:1000}}, {$set:{"test7":"OK"}}, true, true)
db.product.update({"price":{$gt:8000}}, {$inc:{"test9":1}}, true, true)

db.collection.remove() db.product.remove({"test9":1})

db.collection.createIndex(keys, options)
db.product.createIndex({"test9":1})
db.product.createIndex({"test9":1}, {unique:true})
db.posts.ensureIndex({post_text:"text"})
db.posts.find({$text:{$search:"runoob"}})
db.posts.getIndexes()
db.posts.dropIndex("post_text_text")

db.collection.aggregate()
db.product.aggregate([{$group:{_id:"$test9", count:{$sum:1}}}])
$group $sum $avg $min $max $push $addToSet $first $last

Docker:
https://docs.docker.com/install/linux/docker-ce/ubuntu/
sudo apt-get update
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io

#wget -qO- https://get.docker.com/ | sh
#curl -sSL https://get.docker.com/ | sh

sudo mkdir -p /etc/systemd/system/docker.service.d
sudo vim /etc/systemd/system/docker.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=socks5://ip:port" "HTTPS_PROXY=socks5://ip:port"
sudo systemctl daemon-reload
sudo systemctl restart docker
systemctl show --property=Environment docker

sudo groupadd docker
sudo usermod -aG docker thomas
newgrp docker

docker version
docker run hello-world

docker search tutorial
docker pull learn/tutorial
docker run learn/tutorial echo "hello word"

docker rm `docker ps -a -q`
docker rmi -f `docker images -q`

docker run learn/tutorial apt-get install -y ping
docker ps -l
docker commit id learn/ping
docker run lean/ping ping www.baidu.com
docker inspect id/NAME

docker images
docker push learn/ping

sudo service docker start

docker run ubuntu:16.04 /bin/echo "Hello world"
docker run -i -t ubuntu:16.04 /bin/bash
docker run -d ubuntu:16.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"
docker logs id/NAME
docker stop id/NAME
docker start id/NAME
docker restart id/NAME
docker rm id/NAME

docker run -d -P training/webapp python app.py
docker run -d -p 5000:5000 training/webapp python app.py
docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
docker port id/NAME
docker logs -f id/NAME

Dockerfile
docker build -t runoob/centos:6.7 .
docker tag id runoob/centos:dev

docker rmi -f runoob/ubuntu:v4

sudo systemctl stop docker.socket
sudo systemctl stop docker.service
sudo mv /var/lib/docker /data/docker
sudo ln -s /data/docker /var/lib/docker
sudo systemctl start docker.service

LXC:
sudo apt-get install lxc
cd /usr/share/lxc/templates/
sudo lxc-create -t ubuntu -n xiaoming
sudo lxc-start -n xiaoming -d
sudo lxc-ls
sudo lxc-console -n xiaoming
sudo lxc-checkconfig
sudo lxc-info -n xiaoming
sudo lxc-stop -n xiaoming
sudo lxc-destory -n xiaoming

chroot:根切换，从容器内的角度来看，仿佛真的有自己的根树
namespaces:名称空间，负责将资源隔离，pid，net，mnt，user，uts，ipc
CGroups:控制组，负责控制资源的分配 资源限制、优先级分配、资源统计、进程控制

libcontainer:
libnetwork
execdriver
networkdriver
graphdriver

macvlan:
sudo lsmod | grep macvlan
bridge模式
sudo ip link add link ens33 dev mac1 type macvlan mode bridge
sudo ip link add link ens33 dev mac2 type macvlan mode bridge

sudo ip netns add ns1
sudo ip netns add ns2

sudo ip link set mac1 netns ns1
sudo ip link set mac2 netns ns2

sudo ip netns exec ns1 ip a a 192.168.1.122/24 dev mac1
sudo ip netns exec ns1 ip l s mac1 up

sudo ip netns exec ns2 ip a a 192.168.1.123/24 dev mac2
sudo ip netns exec ns2 ip l s mac2 up

sudo ip netns exec ns1 ip a show mac1
sudo ip netns exec ns2 ip a show mac2

sudo ip netns exec ns1 ping 192.168.1.123

veth-pair:
sudo ip netns a ns3
sudo ip netns a ns4

sudo ip l a veth3 type veth peer name veth4
sudo ip l s veth3 netns ns3
sudo ip l s veth4 netns ns4

sudo ip netns exec ns3 ip a a 10.1.1.42/24 dev veth3
sudo ip netns exec ns3 ip l s veth3 up
sudo ip netns exec ns4 ip a a 10.1.1.43/24 dev veth4
sudo ip netns exec ns4 ip l s veth4 up

sudo ip netns exec ns3 ping 10.1.1.43

-------------------------------------
sudo ip l a br0 type bridge
sudo ip l s br0 up

sudo ip l a veth5 type veth peer name br-veth5
sudo ip l a veth6 type veth peer name br-veth6

sudo ip l s veth5 netns ns3
sudo ip l s br-veth5 master br0
sudo ip l s br-veth5 up
sudo ip l s veth6 netns ns4
sudo ip l s br-veth6 master br0
sudo ip l s br-veth6 up

sudo ip netns exec ns3 ip a a 10.1.1.45/24 dev veth5
sudo ip netns exec ns3 ip l s veth5 up
sudo ip netns exec ns4 ip a a 10.1.1.46/24 dev veth6
sudo ip netns exec ns4 ip l s veth6 up

sudo ip netns exec ns3 ping 10.1.1.46

VxLAN:
sudo apt-get install bridge-utils
brctl show

sudo ip link add vxlan1 type vxlan id 1 remote 192.168.1.12 dstport 4789 dev ens33
sudo ip link set vxlan1 up
sudo ip addr add 10.0.0.106/24 dev vxlan1

sudo ip link add vxlan1 type vxlan id 1 remote 192.168.1.11 dstport 4789 dev ens33
sudo ip link set vxlan1 up
sudo ip addr add 10.0.0.107/24 dev vxlan1

ifconfig vxlan1
route -n
ping 10.0.0.107
sudo tcpdump -i ens33 host 192.168.1.12 -s0 -v -w vxlan_vni_1.pcap

存储驱动:
docker info
cat /proc/filesystems

AUFS: 多层
Overlay、Overlay2: 两层、lower只读、upper可写
DeviceMapper: 块级存储
Btrfs: B-tree filesystem、子卷、快照、压缩、copy-on-write
ZFS: 资源池、文件系统、快照、克隆
VFS: 虚拟文件系统
ramfs: 内存文件系统
tmpfs: 内存文件系统、使用swap、固定大小、umount后数据会丢失、
ramdisk: 基于虚拟在内存中的其他文件系统(ex2fs)
XFS FUSE EXT3/EXT4 NFS/CIFS

mkdir apple banana union
echo "apple a.txt" >> apple/a.txt
echo "apple c.txt" >> apple/c.txt
echo "banana b.txt" >> banana/b.txt
echo "banana c.txt" >> banana/c.txt

sudo mount -t aufs -o br=./apple=ro:./banana=ro none ./union
echo "apple d.txt" >> apple/d.txt
sudo umount ./union

rm -rf *
sudo mount -t aufs -o br=./apple=rw:./banana=ro none ./union
echo "apple d.txt" >> apple/d.txt
echo "union e.txt" >> union/e.txt
echo "apple e.txt" >> apple/e.txt
echo "union b.txt" >> union/b.txt
sudo umount ./union

scrapy:
scrapy startproject jdspider
scrapy genspider jd jd.com
scrapy crawl jd -o xxx.json

nginx:
sudo apt-get install nginx

sudo apt-get install build-essential libtool libpcre3 libpcre3-dev zlib1g-dev openssl
tar -zxvf nginx-1.14.0.tar.gz
cd nginx-1.14.0
./configure --prefix=/usr/local/nginx --with-stream
make
sudo make install
sudo ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx
vim /etc/init.d/nginx
sudo update-rc.d nginx defaults
sudo service nginx start

nginx -s stop
nginx -s reload
nginx -s reopen

ps -ax | grep nginx

sudo service nginx start
sudo service nginx stop
sudo service nginx restart
/etc/init.d/nginx start/stop/restart

dpkg --get-selections|grep nginx
sudo apt-get --purge remove nginx
sudo apt-get --purge remove nginx-common
sudo apt-get --purge remove nginx-core
ps -ef |grep nginx

sudo vim /usr/local/nginx/conf/nginx.conf
sudo nginx -s reload

git:
sudo apt-get install git
git config --global user.name "thomas"
git config --global user.email "1272777053@qq.com"
git config --list
git clone https://github.com/harveywangdao/helloworld.git
git add .
git commit -m "changes log"
git status
git pull
git push
git push -u origin master
ssh-keygen -t rsa
git init --bare bigpen.git
git clone thomas@119.23.8.126:/home/thomas/doc/tech/bigpen.git
git submodule add git@ip:a/b/project.git a/b
git clone --recurse-submodules git@ip:a/b/project.git
git clone --recursive git@ip:a/b/project.git
git submodule init
git submodule update
git submodule update --init --recursive
git submodule foreach git pull
git submodule update --remote
git config --global http.proxy http://ip:port
git config --global http.proxy socks5://ip:port
git config --global http.https://github.com.proxy socks5://ip:port
git config --global http.https://github.com.proxy http://ip:port
git config --global --get http.proxy
git config --global --unset http.proxy
git config --global --unset http.https://github.com.proxy

golang:
goland

sudo tar -zxvf go1.6.2.linux-amd64.tar.gz -C /usr/local/
sudo vim /etc/profile
export GOROOT=/usr/local/go
//export GOBIN=$GOROOT/bin
export PATH=$PATH:$GOROOT/bin
export GOPATH=$HOME/goproj
export PATH=$PATH:$GOPATH/bin
source /etc/profile
go version

FTP:
sudo apt-get install vsftpd
sudo vim /etc/vsftpd.conf
sudo service vsftpd restart

ssh:
sudo apt-get install openssh-server openssh-client
sudo ps -e |grep ssh
sudo service ssh start

/etc/init.d/ssh start
sudo /etc/init.d/ssh restart

cat >> /etc/ssh/sshd_config << EOF
ClientAliveInterval 60
ClientAliveCountMax 8888
TCPKeepAlive yes
EOF

sudo service ssh restart

JDK:
http://www.oracle.com/technetwork/java/javase/downloads/index.html
sudo mkdir /usr/local/java
sudo tar -xzvf jdk-8u91-linux-x64.tar.gz -C /usr/local/java/
#sudo vim /etc/profile
#source /etc/profile
sudo vim ~/.bashrc
export JAVA_HOME=/usr/local/java/jdk1.8.0_91
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

gcc/g++:
sudo apt-get install build-essential

vim:
sudo apt-get install vim-gtk
sudo vim /etc/vim/vimrc
set nu                       // ÔÚ×ó²àÐÐºÅ
set tabstop=4                  //tab ³¤¶ÈÉèÖÃÎª 4
set nobackup               //¸²¸ÇÎÄ¼þÊ±²»±¸·Ý
set cursorline               //Í»³öÏÔÊ¾µ±Ç°ÐÐ
set ruler                       //ÔÚÓÒÏÂ½ÇÏÔÊ¾¹â±êÎ»ÖÃµÄ×´Ì¬ÐÐ
set autoindent             //×Ô¶¯Ëõ½ø

vmware tools:
sudo tar zxf VMwareTools-xxx.tar.gz
cd /vmware-tools-distrib
sudo ./vmware-install.pl

Python:
PyCharm
xz -d Python-3.5.1.tar.xz
tar -C /home/harvey/src/ -xvf Python-3.5.1.tar
./configure
sudo make
sudo make install

/usr/local/lib/python3.5/
/usr/local/bin/python3.5

sudo rm /usr/bin/python
sudo ln -s /usr/bin/python3.5 /usr/bin/python
ls -l /usr/bin/python*

Python 2.7.6
Python 3.4.3
Python 3.5.3

sudo apt-get install python-software-properties
sudo apt-get install software-properties-common
sudo add-apt-repository ppa:fkrull/deadsnakes
sudo apt-get update
sudo apt-get install python3.5
sudo ln -s /usr/bin/python3.5 /usr/bin/python

sudo python -m pip install --upgrade pip

Django:
django-admin.py startproject myproject
python manage.py runserver 0.0.0.0:8000

python manage.py startapp myapp

myproject/myproject/settings.py

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'myapp',
]

python manage.py makemigrations
python manage.py migrate

python manage.py createsuperuser

Django 1.9.5 Python2.7
pip 1.5.4
Django 1.11.6

sudo apt-get install python-pip  #python2.x
sudo apt-get install python3-pip #python3.x
sudo pip3 install packagename    #python3.x
sudo pip install Django
sudo pip install --upgrade pip

django-admin.py startproject project-name
python manage.py startapp app-name
python manage.py makemigrations
python manage.py migrate

python manage.py runserver
python manage.py runserver 8001
python manage.py runserver 0.0.0.0:8000
python manage.py flush
python manage.py createsuperuser
python manage.py changepassword username
python manage.py dumpdata appname > appname.json
python manage.py loaddata appname.json
python manage.py shell

python manage.py dbshell

MySQL:
mysql_upgrade -u root -p --force
show global variables like 'port';

mysqldump -u root -p123456 --add-drop-table db1 | mysql -u root -p123456 newdb

wget https://dev.mysql.com/get/mysql-apt-config_0.8.8-1_all.deb
sudo dpkg -i mysql-apt-config_0.8.8-1_all.deb
sudo apt-get update
sudo apt-get install mysql-server
sudo mysql_upgrade -u root -p
sudo service mysql restart
mysqlcheck -u root -p --all-databases

sudo apt-get install mysql-client
sudo apt-get install libmysqlclient-dev

sudo apt-get install libapache2-mod-auth-mysql
sudo apt-get install python-mysqldb    #python2.x

sudo netstat -tap | grep mysql
mysql -u root -p
mysql -h198.168.122.122 -u root -p 123456
exit
mysqladmin -u root -p 123456 password new123456

sudo /etc/init.d/mysql start
sudo /etc/init.d/mysql stop
sudo /etc/init.d/mysql restart

bind-address = 0.0.0.0
create user 'thomas'@'localhost' identified by 'xxx';
GRANT ALL PRIVILEGES ON *.* TO 'thomas'@'localhost';
GRANT ALL PRIVILEGES ON *.* TO'root'@'%' IDENTIFIED BY 'xxx' WITH GRANT OPTION;
SHOW GRANTS FOR thomas;
FLUSH PRIVILEGES;
select user from mysql.user;

show variables like '%max_connections%';
set global max_connections=1000;
show status like 'Threads%';
show processlist;

show databases;
set names utf8;
create database db001;
drop database db001;
drop database if exists db001;
use db001;
show tables;
grant select,insert,update,delete on *.* to [email=test1@¡±%]test1@¡±%[/email]¡± Identified by ¡°abc¡±;
grant select,insert,update,delete on mydb.* to [email=test2@localhost]test2@localhost[/email] identified by ¡°abc¡±;
grant select,insert,update,delete on mydb.* to [email=test2@localhost]test2@localhost[/email] identified by ¡°¡±;

select version();
select now();
SELECT DAYOFMONTH(CURRENT_DATE);
SELECT MONTH(CURRENT_DATE);
SELECT YEAR(CURRENT_DATE);
SELECT "welecome to my blog!";
select ((4 * 4) / 10 ) + 25;

create table MyTable(
  id int(4) not null primary key auto_increment,
  name char(20) not null,
  sex int(4) not null default '0',
  degree double(16,2)
);
drop table MyTable;

insert into MyTable values(1,'Tom',96.45),(2,'Joan',82.99), (2,'Wang', 96.59);
select * from MyTable;
select * from MyTable order by id limit 0,2;
delete from MyTable where id=1;
update MyTable set name='Mary' where id=1;
alter table MyTable add passtest int(4) default '0';
rename table MyTable to MyTable1;

主从搭建:
sudo apt-get install mysql-server
sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf

主:
bind-address = 0.0.0.0
server-id   = 7
log-bin     = mysql-bin
# binlog-do-db = db01
# binlog-ignore-db=mysql
# binlog-ignore-db=information_schema
# binlog-ignore-db=performance_schema
# binlog-ignore-db=performance_schema
# binlog-ignore-db=sys

sudo /etc/init.d/mysql restart
mysql -u root -p
CREATE USER 'slave'@'%' IDENTIFIED BY '123456';
GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%';
show master status\G

从:
bind-address = 0.0.0.0
server-id   = 11
log-bin     = mysql-slave-bin
relay_log   = edu-mysql-relay-bin
# read_only   = 1

change master to master_host='192.168.1.7', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=617, master_connect_retry=30;
show slave status\G
start slave;

create database appledb;
drop database appledb;
show processlist\G
show binlog events\G
stop slave;

mysql -u root -p -Dant_test< ~/server/ant_test.sql 
GRANT ALL PRIVILEGES ON *.* TO'root'@'%' IDENTIFIED BY '180498' WITH GRANT OPTION;

主主:
#sudo apt-get remove --purge mysql-server
#sudo apt-get remove mysql-common
sudo apt-get autoremove mysql* --purge
sudo rm -rf /var/lib/mysql*
sudo rm -rf /etc/mysql

mysql -u root -p
create database appledb;
use appledb;
create table apple_tb (id int, name char(20));
insert into apple_tb (id, name) values (1, 'xiaoming');
insert into apple_tb (id, name) values (2, 'xiaohong');
insert into apple_tb (id, name) values (3, 'xiaowang');
select * from apple_tb\G

sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf

主1:
bind-address = 0.0.0.0
server-id   = 10
log-bin     = mysql-bin
auto_increment_offset = 1
auto_increment_increment = 2

sudo /etc/init.d/mysql restart

主2:
bind-address = 0.0.0.0
server-id   = 11
log-bin     = mysql-bin
auto_increment_offset = 2
auto_increment_increment = 2

sudo /etc/init.d/mysql restart

#grant replication slave on *.* to 'repl'@'192.168.10.12' identified by '123456';
#grant replication slave on *.* to 'repl'@'192.168.10.11' identified by '123456';
grant replication slave,replication client on *.* to 'slave'@'%' identified by '123456';
flush privileges;

mysql -u root -p -Dappledb< ~/server/appledb.sql

show master status;

change master to master_host='192.168.1.11', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=2434, master_connect_retry=30;
start slave;
show slave status\G

change master to master_host='192.168.1.10', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=1424, master_connect_retry=30;
start slave;
show slave status\G

keepalived:
tar -xzvf keepalived-2.0.18.tar.gz
cd keepalived-2.0.18
sudo apt-get install openssl libssl-dev libpopt-dev libnl-3-dev
./configure --prefix=/usr/local/keepalived
make
sudo make install

sudo apt-get install keepalived
cd /usr/local/keepalived/sbin
sudo mv /usr/sbin/keepalived /usr/sbin/keepalived_bk
sudo cp keepalived /usr/sbin/keepalived

sudo service keepalived start
sudo vim /etc/keepalived/keepalived.conf

global_defs {
  router_id db10
}

# state MASTER
vrrp_instance VI_1 {
  state BACKUP
  interface ens33
  virtual_router_id 51
  priority 100
  nopreempt
  advert_int 1

  authentication {
    auth_type PASS
    auth_pass 1111
  }

  virtual_ipaddress {
    192.168.1.100
  }
}

virtual_server 192.168.1.100 3306 {
  delay_loop 6
  persistence_timeout 50
  protocol TCP

  real_server 192.168.1.10 3306 {
    notify_down /etc/keepalived/kill_keepalived.sh
    TCP_CHECK {
      connect_timeout 1
      retry 2
      delay_before_retry 1
      connect_port 3306
    }
  }
}

sudo vim /etc/keepalived/kill_keepalived.sh
#!/bin/bash
service keepalived stop

#kill -9 $(cat /var/run/keepalived.pid)
#pkill keepalived
sudo chmod +x /etc/keepalived/kill_keepalived.sh

ip addr
sudo /etc/init.d/mysql start
sudo /etc/init.d/mysql stop
sudo service keepalived start
sudo service keepalived stop
ps -ef | grep keepalived

tail -f /var/log/syslog | grep Keepalived

haproxy:
sudo apt-get install haproxy
sudo cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy_bk.cfg
sudo vim /etc/haproxy/haproxy.cfg

global
  daemon
  nbproc 1
  pidfile /var/run/haproxy.pid

defaults
  mode tcp
  retries 2
  option redispatch
  option abortonclose
  maxconn 4096
  timeout connect 5000ms
  timeout client 30000ms
  timeout server 30000ms
  log 127.0.0.1 local0 err

listen test1
  bind 0.0.0.0:3306
  mode tcp
  server s1 192.168.1.10:3306
  server s2 192.168.1.11:3306

sudo /etc/init.d/mysql start
sudo /etc/init.d/mysql stop
sudo service haproxy start
sudo service haproxy restart
sudo service haproxy stop
sudo service haproxy status
ps -ef | grep haproxy

lvs:
lsmod | grep ip_vs
sudo ls -alRUv /lib/modules/$(uname -r)/kernel | grep ipvs
find /lib/modules/$(uname -r)/ -iname "**.ko*" | cut -d/ -f5- | grep ipvs

sudo apt-get install ipvsadm
cat /proc/sys/net/ipv4/ip_forward
#echo "1">/proc/sys/net/ipv4/ip_forward

DR模式 TUN模式 NAT模式 Director RealServer

sudo ifconfig eth0:0 192.168.1.200 netmask 255.255.255.0 broadcast 192.168.1.200
sudo route add -host 192.168.1.200 dev eth0:0

sudo ipvsadm -A -t 192.168.1.200:3306 -s rr
sudo ipvsadm -a -t 192.168.1.200:3306 -r 192.168.1.10 -g
sudo ipvsadm -a -t 192.168.1.200:3306 -r 192.168.1.11 -g

#ipvsadm -D -t 192.168.1.200

sudo ifconfig lo:0 192.168.1.200 netmask 255.255.255.255 broadcast 192.168.1.200 up
sudo route add -host 192.168.1.200 dev lo:0

sudo ipvsadm -L -n
sudo ipvsadm -L -c

apache:
sudo apt-get install apache2

sudo apt-get install libapache2-mod-python
sudo apt-get install libapache2-mod-wsgi

http://127.0.0.1

ls /etc/apache2

sudo /etc/init.d/apache2 start
sudo /etc/init.d/apache2 restart
sudo /etc/init.d/apache2 stop

/var/www/html
sudo vim /etc/apache2/apache2.conf
sudo vim /etc/apache2/sites-available/000-default.conf
sudo cp /var/www/html/index.html /var/www/

//sudo apt-get --purge remove apache2-common
sudo apt-get --purge remove apache2
sudo find /etc -name "*apache*" |xargs  rm -rf
sudo rm -rf /var/www
//sudo rm -rf /etc/libapache2-mod-jk
dpkg -l |grep apache2|awk '{print $2}'|xargs dpkg -P

ZooKeeper:
tar -xzvf zookeeper-3.4.10.tar.gz -C /home/thomas/server/
conf/zoo_sample.cfg --> conf/zoo.cfg

vim zoo.cfg
dataDir=/home/thomas/server/zookeeper-3.4.10/data
dataLogDir=/home/thomas/server/zookeeper-3.4.10/logs
server.1=localhost:2888:3888

data/myid -->1

sudo vim /etc/profile
export ZOOKEEPER_HOME=/home/thomas/server/zookeeper-3.4.10
export PATH=$PATH:$ZOOKEEPER_HOME/bin
source /etc/profile

zkServer.sh start
zkCli.sh -server localhost:2181
zkServer.sh status
zkServer.sh stop

create /mynode
get /mynode
rmr /mynode

zkserver
zkserver status
zkserver stop
zkcli -server localhost:2181

集群搭建:
tar -xzvf zookeeper-3.4.14.tar.gz
conf/zoo_sample.cfg --> conf/zoo.cfg

vim zoo.cfg
dataDir=/home/thomas/server/zookeeper-3.4.14/data
dataLogDir=/home/thomas/server/zookeeper-3.4.14/logs
server.1=192.168.1.7:2888:3888
server.2=192.168.1.10:2888:3888
server.3=192.168.1.11:2888:3888

echo 1 >> /home/thomas/server/zookeeper-3.4.14/data/myid
echo 2 >> /home/thomas/server/zookeeper-3.4.14/data/myid
echo 3 >> /home/thomas/server/zookeeper-3.4.14/data/myid

cd /home/thomas/server/zookeeper-3.4.14/bin
./zkServer.sh start
./zkServer.sh status
./zkServer.sh stop

cp conf/zoo_sample.cfg conf/zoo.cfg
vim conf/zoo.cfg
dataDir=/home/thomas/server/zookeeper/data
dataLogDir=/home/thomas/server/zookeeper/logs
server.1=localhost:2888:3888
clientPort=2181
admin.enableServer=false
#admin.serverPort=8499
secureClientPort=2281

mkdir data logs
echo 1 >> data/myid

./bin/zkServer.sh start
./bin/zkServer.sh status
./bin/zkServer.sh stop
./bin/zkCli.sh -server localhost:2181
./bin/zkCli.sh -server localhost:2281

create /dubbo 1
create /dubbo/apple 1
create /dubbo/apple/haha 1

create /dubbo/apple/haha/a 1
delete /dubbo/apple/haha/a

ls /dubbo/apple/haha

world:
setAcl /dubbo/apple/haha world:anyone:cdrwa

super:超级管理员
ip:
setAcl /dubbo/apple/haha ip:192.168.197.128:cdrwa

auth:
addauth digest xiaoming:xiaohong-pw
setAcl /dubbo/apple/haha auth:xiaoming:xiaohong-pw:cdrwa

digest:
addauth digest xiaoming:xiaohong-pw
echo -n xiaoming:xiaohong-pw | openssl dgst -binary -sha1 | openssl base64
setAcl /dubbo/apple/haha digest:xiaoming:kwGy568vH63lusu4sj65Alb13ms=:cdrwa
getAcl /dubbo/apple/haha

vim bin/zkServer.sh
export SERVER_JVMFLAGS="
-Dzookeeper.serverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory
-Dzookeeper.ssl.keyStore.location=/home/thomas/server/zookeeper/ca/testKeyStore.jks 
-Dzookeeper.ssl.keyStore.password=testpass 
-Dzookeeper.ssl.trustStore.location=/home/thomas/server/zookeeper/ca/testTrustStore.jks 
-Dzookeeper.ssl.trustStore.password=testpass"

vim bin/zkCli.sh
export CLIENT_JVMFLAGS="
-Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty 
-Dzookeeper.client.secure=true 
-Dzookeeper.ssl.keyStore.location=/home/thomas/server/zookeeper/ca/testKeyStore.jks 
-Dzookeeper.ssl.keyStore.password=testpass 
-Dzookeeper.ssl.trustStore.location=/home/thomas/server/zookeeper/ca/testTrustStore.jks 
-Dzookeeper.ssl.trustStore.password=testpass"

生成zk证书 testKeyStore.jks 和 testTrustStore.jks:
keytool -genkey -alias test -keysize 2048 -validity 3650 -keyalg RSA -dname "CN=localhost, OU=ZooKeeper, O=Apache, L=Unknown, ST=Unknown, C=Unknown" -keypass testpass -storepass testpass -keystore testKeyStore.jks
keytool -export -alias test -keystore testKeyStore.jks -file testKeyStore.cer -storepass testpass
keytool -import -alias test -keystore testTrustStore.jks -file testKeyStore.cer -keypass testpass -storepass testpass
keytool -list -v -keystore testKeyStore.jks -storepass testpass

通过testKeyStore.jks生成私钥和证书:
keytool -export -alias test -keystore testKeyStore.jks -rfc -file testKeyStore.crt -storepass testpass
keytool -alias test -importkeystore -srckeystore testKeyStore.jks -destkeystore testKeyStore.pfx -deststoretype pkcs12
openssl pkcs12 -in testKeyStore.pfx -nocerts -nodes -out testKeyStore.key
openssl rsa -in testKeyStore.key -check  输出到stdout复制

keytool --list -keystore testKeyStore.jks -storepass testpass
keytool --list -rfc -keystore testKeyStore.jks -storepass testpass
keytool --list -rfc -keystore testTrustStore.jks -storepass testpass

kafka:
tar -xzvf kafka_2.12-1.0.0.tgz -C /home/thomas/server/
#bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties
bin/kafka-server-stop.sh config/server.properties

bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
bin/kafka-topics.sh --list --zookeeper localhost:2181
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 2>/dev/null

bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic

kafka-console-consumer.sh --bootstrap-server kafka:9092 --from-beginning --topic test --group group01

vim server.properties
vim zookeeper.properties
vim producer.properties
vim consumer.properties

cd E:\golang\src\hcxy\iov\iov_server
cd E:\golang\src\hcxy\iov\iov_client

bin\windows\kafka-server-start.bat config\server.properties
bin\windows\kafka-server-stop.bat config\server.properties

bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 10 --topic TspToIov
bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic TspToIov
bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --topic TspToIov --from-beginning 2>/dev/null

集群搭建:
tar -xzvf kafka_2.12-2.2.1.tgz

cd /home/thomas/server/kafka_2.12-2.2.1/config
vim server.properties

broker.id=1
listeners=PLAINTEXT://192.168.1.7:9092
zookeeper.connect=192.168.1.7:2181,192.168.1.10:2181,192.168.1.11:2181
log.dirs=/home/thomas/server/kafka/data

broker.id=2
listeners=PLAINTEXT://192.168.1.10:9092
zookeeper.connect=192.168.1.7:2181,192.168.1.10:2181,192.168.1.11:2181
log.dirs=/home/thomas/server/kafka/data

broker.id=3
listeners=PLAINTEXT://192.168.1.11:9092
zookeeper.connect=192.168.1.7:2181,192.168.1.10:2181,192.168.1.11:2181
log.dirs=/home/thomas/server/kafka/data

mkdir data

cd /home/thomas/server/kafka_2.12-2.2.1/bin
./kafka-server-start.sh -daemon ../config/server.properties
./kafka-topics.sh --create --zookeeper 192.168.1.7:2181 --replication-factor 2 --partitions 3 --topic xiaoming
./kafka-topics.sh --describe --zookeeper 192.168.1.7:2181 --topic xiaoming
./kafka-console-producer.sh --broker-list 192.168.1.7:9092 --topic xiaoming
./kafka-console-consumer.sh --bootstrap-server 192.168.1.10:9092 --from-beginning --topic xiaoming
./kafka-topics.sh --zookeeper 192.168.1.7:2181 --list

./kafka-consumer-groups.sh --new-consumer --bootstrap-server 192.168.1.7:9092 --list
./kafka-consumer-groups.sh --new-consumer --bootstrap-server 192.168.1.7:9092 --group groupid1 --describe
./kafka-consumer-groups.sh --bootstrap-server 192.168.1.7:9092 --describe --group groupid1 --state
./kafka-consumer-groups.sh --bootstrap-server 192.168.1.7:9092 --describe --group groupid1 --members

./bin/kafka-server-start.sh -daemon config/server.properties

./bin/kafka-topics.sh --create --zookeeper 192.168.197.128:2181 --replication-factor 1 --partitions 3 --topic xiaoming
./bin/kafka-console-producer.sh --broker-list 192.168.197.128:9092 --topic xiaoming

./bin/kafka-topics.sh --zookeeper 192.168.197.128:2181 --list
./bin/kafka-topics.sh --zookeeper 192.168.197.128:2181 --topic xiaoming --describe
./bin/kafka-topics.sh --zookeeper 192.168.197.128:2181 --describe --under-replicated-partitions

./bin/kafka-console-consumer.sh --bootstrap-server 192.168.197.128:9092 --from-beginning --topic xiaoming
./bin/kafka-console-consumer.sh --bootstrap-server 192.168.197.128:9092 --topic xiaoming
./bin/kafka-console-consumer.sh --bootstrap-server 192.168.197.128:9092 --from-beginning --topic xiaoming --group groupaa01

./bin/kafka-consumer-groups.sh --bootstrap-server 192.168.197.128:9092 --list
./bin/kafka-consumer-groups.sh --bootstrap-server 192.168.197.128:9092 --group groupaa01 --describe

./bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 192.168.197.128:9092 --topic xiaoming

./bin/kafka-console-consumer.sh --bootstrap-server 192.168.197.128:9092 --from-beginning --topic __consumer_offsets --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter"

./bin/kafka-server-stop.sh

get /kafka/consumers/zoo-consumer-group/offsets/my-topic/0

./bin/kafka-configs.sh --zookeeper localhost:2182 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=alice-secret],SCRAM-SHA-512=[password=alice-secret]' --entity-type users --entity-name alice
./bin/kafka-configs.sh --zookeeper localhost:2182 --describe --entity-type users --entity-name alice
./bin/kafka-configs.sh --zookeeper localhost:2182 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]' --entity-type users --entity-name admin
./bin/kafka-configs.sh --zookeeper localhost:2182 --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name alice


vim config/kafka_server_jaas.conf
KafkaServer {
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="admin"
        password="admin-secret";
    };

vim bin/kafka-server-start.sh
#exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka "$@"
exec $base_dir/kafka-run-class.sh $EXTRA_ARGS -Djava.security.auth.login.config=$base_dir/../config/kafka_server_jaas.conf kafka.Kafka "$@"

vim config/server.properties
listeners=SASL_PLAINTEXT://192.168.197.128:9092
security.inter.broker.protocol=SASL_PLAINTEXT
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256 #(or SCRAM-SHA-512)
sasl.enabled.mechanisms=SCRAM-SHA-256 #(or SCRAM-SHA-512)

allow.everyone.if.no.acl.found=false
super.users=User:admin
authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer

vim config/kafka_client_jaas_admin.conf
KafkaClient {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="admin"
  password="admin-secret";
};

vim config/producer.properties
vim config/consumer.properties
security.protocol=SASL_PLAINTEXT
sasl.mechanism=SCRAM-SHA-256

vim bin/kafka-console-producer.sh
#exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleProducer "$@"
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_admin.conf kafka.tools.ConsoleProducer "$@"

vim bin/kafka-console-consumer.sh
#exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleConsumer "$@"
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_admin.conf kafka.tools.ConsoleConsumer "$@"

./bin/kafka-console-producer.sh --broker-list 192.168.197.128:9092 --topic xiaoming --producer.config config/producer.properties
./bin/kafka-console-consumer.sh --bootstrap-server 192.168.197.128:9092 --from-beginning --topic xiaoming --consumer.config config/consumer.properties

vim config/kafka_client_jaas_alice.conf
KafkaClient {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="alice"
  password="alice-secret";
};

cp bin/kafka-console-producer.sh bin/kafka-console-producer-alice.sh
cp bin/kafka-console-consumer.sh bin/kafka-console-consumer-alice.sh
vim bin/kafka-console-producer-alice.sh
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_alice.conf kafka.tools.ConsoleProducer "$@"

vim bin/kafka-console-consumer-alice.sh
exec $(dirname $0)/kafka-run-class.sh -Djava.security.auth.login.config=$(dirname $0)/../config/kafka_client_jaas_alice.conf kafka.tools.ConsoleConsumer "$@"

./bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:alice --operation All --topic xiaoming
./bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:alice --operation All --group test-consumer-group
./bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --add --allow-principal User:alice --operation All --group apple-group-03

./bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --list
./bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2182 --remove --allow-principal User:alice --operation Read --topic xiaoming --allow-host *

./bin/kafka-console-producer-alice.sh --broker-list 192.168.197.128:9092 --topic xiaoming --producer.config config/producer.properties
./bin/kafka-console-consumer-alice.sh --bootstrap-server 192.168.197.128:9092 --from-beginning --topic xiaoming --consumer.config config/consumer.properties

netstat:
netstat -a
netstat -ap
sudo netstat -apn | grep 3306
ps -aux | grep pid
lsof -i:3306

protobuf:
git clone https://github.com/google/protobuf.git
./autogen.sh
./configure
make
sudo make install
protoc -I=$SRC_DIR --cpp_out=$DST_DIR /path/to/file.proto

google.golang.org/protobuf/cmd/protoc-gen-go
google.golang.org/grpc/cmd/protoc-gen-go-grpc

protoc --go_out=. helloworld.proto
protoc --go_out=. --go-grpc_out=. helloworld.proto

node.js:
curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -
sudo apt-get install -y nodejs

curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -
sudo apt-get install -y nodejs

sudo npm install -g cnpm --registry=https://registry.npm.taobao.org
sudo cnpm install vue
sudo cnpm install --global vue-cli

vue init webpack my-project
cnpm install
cnpm run dev

npm cache clean --force
npm config set registry http://registry.npmjs.org
npm config set registry https://registry.npm.taobao.org
npm config list
npm config get registry
npm config get prefix

sudo npm uninstall -g cnpm
sudo npm install cnpm -g
sudo npm install -g cnpm --registry=https://registry.npm.taobao.org

npm config list
npm config ls
npm config ls -l

npm config set registry "http://registry.npmjs.org"
npm config set registry http://registry.cnpmjs.org
npm config set registry https://registry.npm.taobao.org
npm config set registry https://registry.npmjs.org

npm cache clean -f
npm cache clean --force
npm cache verify

npm info underscore
npm --registry http://registry.cnpmjs.org info underscore

sudo npm install npm -g

nohup:
nohup /root/start.sh >/dev/null 2>&1 &
nohup node app.js >>aaa.log 2>&1 &
exit退出终端才可以

forever start app.js
forever start -l forever.log -o out.log -e err.log app.js 
forever stop app.js
forever list

挂载:
sudo fdisk -l
sudo df -h
ls -l /dev/disk/by-id/
sudo file -s /dev/vdb

sudo fdisk /dev/vdb
n
p
1
wq

sudo fdisk -l
ls -l /dev/disk/by-id/

df -Th
sudo mkfs.ext4 /dev/vdb1

mkdir ~/ethereum
mount /dev/vdb1 /home/ubuntu/ethereum

lsblk -f
blkid /dev/sdb1
dumpe2fs -h /dev/sdb1

sudo cp /etc/fstab /etc/fstab.backup

sudo vim /etc/fstab
/dev/disk/by-id/virtio-disk-o7pgds1a-part1 /home/ubuntu/ethereum ext3 defaults,nofail 0 1
/dev/sdb1 /data ext4 defaults,nofail 0 0

sudo mount -a

扩容
sudo umount -v /home/ubuntu/ethereum
wget -O /tmp/devresize.py http://mirrors.tencentyun.com/install/virts/devresize.py
sudo python /tmp/devresize.py /dev/vdb
sudo mount -a

sudo fdisk -l
lsblk
vgdisplay
lvextend -l +100%FREE /dev/mapper/ubuntu--vg-ubuntu--lv
resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv

MinGW Clang:
gcc main.c -o appc.exe
g++ main.cpp -o appcpp.exe

#clang -std=c11 -Wall --target=i686-pc-mingw32 main.c -o appc.exe
#clang++ -std=c++14 -Wall --target=i686-pc-mingw32 main.cpp -o appcpp.exe

#clang --target=i686-w64-mingw32 main.c -o appc.exe
#clang --target=i686-pc-mingw32 main.c -o appc.exe
#clang++ --target=i686-pc-mingw32 main.cpp -o appcpp.exe
clang --target=x86_64-w64-mingw32 main.c -o appc.exe
clang++ --target=x86_64-w64-mingw32 main.cpp -o appcpp.exe

add_definitions("--target=i686-pc-mingw32")
set(CMAKE_C_COMPILER "D:/Program Files/LLVM/bin/clang.exe")
set(CMAKE_CXX_COMPILER "D:/Program Files/LLVM/bin/clang++.exe")
cmake -DCMAKE_CXX_COMPILER="D:/Program Files/LLVM/bin/clang++.exe"
cmake -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DCMAKE_MAKE_PROGRAM=mingw32-make
message(STATUS "This is BINARY dir " ${CMAKE_BINARY_DIR})

cmake -G"Unix Makefiles" .
cmake -G"MinGW Makefiles" .

cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++  .
cmake -DCMAKE_TOOLCHAIN_FILE=test.toolchain.cmake -G"MinGW Makefiles" .

findstr /s /i "__float128" *.*

Ethereum:
go get -d github.com/ethereum/go-ethereum
git checkout v1.8.12
make geth

geth --datadir data --rpc --rpcaddr 0.0.0.0 --rpccorsdomain "*" --syncmode "fast" --cache=2048 console 2>>eth.log
nohup geth --datadir data --rpc --rpcaddr 0.0.0.0 --rpccorsdomain "*" --syncmode "fast" --cache=2048 >>eth.log 2>&1 &
geth attach ipc:/home/ubuntu/ethereum/node1/data/geth.ipc

net.listening
net.peerCount
admin.peers
admin.nodeInfo
eth.accounts
personal.listAccounts
miner.setEtherbase("0x348b4276c75425016be89a2627bd4797d4c638dc")

personal.unlockAccount(eth.coinbase)
eth.getBalance(eth.coinbase).toNumber()
eth.syncing
eth.mining
eth.blockNumber
miner.setEtherbase(eth.coinbase)
miner.start(4)
miner.stop()

sudo cnpm install solc -g
sudo add-apt-repository ppa:ethereum/ethereum
sudo apt-get update
sudo apt-get install solc
sudo cnpm install web3 -g
sudo cnpm install truffle -g
sudo cnpm install webpack -g
sudo cnpm install webpack-cli -g

vim private.json
geth --datadir data init private.json
geth --rpc --rpcaddr 0.0.0.0 --rpccorsdomain "*" --datadir data --networkid 1448 console
geth attach ipc:/home/thomas/eth/data/geth.ipc

personal.newAccount('13717064390')
eth.getBalance('0xe89d4872b78ab5c5c903583725fe5d485686d6ce')
eth.getBalance("0x044b8ab7c603f0938f53e72b7586ec38f3eff044")
eth.getBalance('0xced5d036328e9b6da0ed9a7ce1a7770b951fc636')
miner.start(1)
miner.stop()

personal.unlockAccount("0xe89d4872b78ab5c5c903583725fe5d485686d6ce",'13717064390')
personal.unlockAccount("0x044b8ab7c603f0938f53e72b7586ec38f3eff044",'13717064390')
personal.unlockAccount("0xced5d036328e9b6da0ed9a7ce1a7770b951fc636",'13717064390')
eth.getBlock(1)
web3.toDecimal(0xffffffff)
web3.toWei('1','ether')
web3.fromWei('22000000000000', 'ether')
wei kwei mwei gwei microether milliether ether
txpool.status
eth.coinbase
eth.sendTransaction({from: '0xe89d4872b78ab5c5c903583725fe5d485686d6ce', to: '0x044b8ab7c603f0938f53e72b7586ec38f3eff044', value: web3.toWei(1, "ether")})  0x10ada6672609311159ae15510801e2c27db91c7e29c2129eb7f74c172a6f58bf
eth.sendTransaction({from: '0x044b8ab7c603f0938f53e72b7586ec38f3eff044', to: '0xced5d036328e9b6da0ed9a7ce1a7770b951fc636', value: web3.toWei(1, "ether")})
eth.getTransaction("0x1b21bba16dd79b659c83594b0c41de42debb2738b447f6b24e133d51149ae2a6")

truffle unbox webpack
npm install
truffle compile
truffle migrate
miner.start(1);admin.sleepBlocks(1);miner.stop();

1.获得地址和私钥
2.交易
3.编写Solidity智能合约
4.编译智能合约
5.安装智能合约
6.升级智能合约
7.交易gas、合约gas
8.solcjs solc --bin --abi -o ./ ./hello.sol
9.truffle
10.web3

1.solidity编码
2.solidity编译
3.移植智能合约ABI
4.用JSON-RPC与智能合约交互

MetaCoin.deployed().then(function(instance){return instance.getBalance(web3.eth.accounts[0]);}).then(function(value){return value.toNumber()});
MetaCoin.deployed().then(function(instance){return instance.sendCoin(web3.eth.accounts[1], 500);});

curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"eth_accounts","params":[],"id":67}' 127.0.0.1:8545
curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"eth_getBalance","params":["0xe89d4872b78ab5c5c903583725fe5d485686d6ce","latest"],"id":67}' 127.0.0.1:8545
curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"eth_blockNumber","params":[],"id":67}' 127.0.0.1:8545

curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0","method":"eth_sendTransaction","params": [{                                                                               
  "from": "0xe89d4872b78ab5c5c903583725fe5d485686d6ce",
  "to": "0x044b8ab7c603f0938f53e72b7586ec38f3eff044",
  "value": "0x1"
}],
"id":67}' 127.0.0.1:8545

{“jsonrpc”:”2.0”,”method”:”eth_call”,”params”:[{“to”:”0xcF9b0ea3D6Cd99C17531eaC74D9B8C845520D688”,”data”:”0xfe50cc72”},”latest”],”id”:67}
"{\"jsonrpc\":\"2.0\",\"method\":\"web3_clientVersion\",\"params\":[{\"to\":\"0x3b37B585D69720a10241b0690310daED9FBD521E\",\"data\":\"0xfe50cc72\"},\"latest\"],\"id\":67}";

curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"net_version","params":[],"id":67}' 127.0.0.1:8545

solc --abi contracts/Greeter.sol
abigen --abi contracts/Greeter.abi --pkg greeter --out greeter.go

solc --abi Store.sol
solc --bin Store.sol
abigen --bin=Store_sol_Store.bin --abi=Store_sol_Store.abi --pkg=store --out=Store.go

privkey:24e37b26f354d123eb4d2675bac002fd802e4db39d6df10fcd2faae8c111b586
address:0xf036eF6F352048b27C291295B6f6DCD237973a0d

contractAddr:0x2b01981B95904CA40B0F390b5D196D4fe56e2f0E

privkey:a166eae0879b0eee88a7a5960cf4f4e0b589c2b2f47121eaddd0d7f3df585059
address:0x23ACbdE81993A95F762Fe858fC75F201636056e5

fabric:
EOS:
mkdir /home/thomas/eos/contracts

docker pull eosio/eos:v1.4.5
docker logs --tail 10 eosio

curl http://127.0.0.1:8888/v1/chain/get_info

cleos --wallet-url http://127.0.0.1:5555 wallet list
curl --request POST --url http://127.0.0.1:5555/v1/wallet/list_wallets --header 'content-type: application/x-www-form-urlencoded; charset=UTF-8'

cleos --url http://127.0.0.1:8888/ --wallet-url http://0.0.0.0:5555/ push action eosio.token transfer '[ "alice", "bob", "25.0000 SYS", "m" ]' -p alice@active

git clone --recursive https://github.com/eosio/eosio.cdt --branch v1.2.1 --single-branch
cd eosio.cdt
./build.sh SYS
sudo ./install.sh

docker run --name eosio \
  --publish 8888:8888 \
  --publish 5555:5555 \
  --volume /home/thomas/eos/contracts:/home/thomas/eos/contracts \
  --detach \
  eosio/eos \
  /bin/bash -c \
  "keosd --http-server-address=0.0.0.0:5555 & exec nodeos -e -p eosio --plugin eosio::producer_plugin --plugin eosio::producer_api_plugin --plugin eosio::chain_api_plugin --plugin eosio::history_plugin --plugin eosio::history_api_plugin --plugin eosio::http_plugin --data-dir /home/thomas/eosio/data --config-dir /home/thomas/eosio/config --http-server-address=0.0.0.0:8888 --access-control-allow-origin=* --contracts-console --http-validate-host=false --filter-on='*'"

docker exec -it eosio bash
cleos wallet create --to-console -n sunlight

wallet password: PW5J5AvkFzVdyy5Z4JoHAZVYV7nv1wgx4dwe48st6LBDELw6Rp7aJ

cleos wallet open -n sunlight
cleos wallet unlock -n sunlight
#cleos wallet lock -n sunlight
#cleos wallet list
cleos wallet create_key -n sunlight

public key: EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA

cleos wallet import -n sunlight

private key: 5KQwrPbwdL6PhXujxW37FSSQZ1JiwsST4cqQzDeyXtP79zkvFD3
public key: EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV

cleos wallet keys

cleos create account eosio bob EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA
cleos create account eosio alice EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA

eosio-cpp -abigen hello.cpp -o hello.wasm
cleos create account eosio hello EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA -p eosio@active
cleos set contract hello /home/thomas/eos/contracts/hello -p hello@active
cleos push action hello hi '["bob"]' -p bob@active
cleos push action hello hi '["alice"]' -p alice@active

eosio-cpp -I include -o eosio.token.wasm src/eosio.token.cpp -abigen
cleos create account eosio eosio.token EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV
cleos set contract eosio.token /home/thomas/eos/contracts/eosio.contracts/eosio.token --abi eosio.token.abi -p eosio.token@active

eosio-cpp -I include -o eosio.bios.wasm src/eosio.bios.cpp -abigen
cleos create account eosio eosio.bios EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV
cleos set contract eosio.bios /home/thomas/eos/contracts/eosio.contracts/eosio.bios -p eosio.bios@active

eosio-cpp -I include -o eosio.system.wasm src/eosio.system.cpp -abigen
cleos create account eosio eosio.system EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV
cleos set contract eosio.system /home/thomas/eos/contracts/eosio.contracts/eosio.system -p eosio.system@active

cleos push action eosio.token create '{"issuer":"eosio", "maximum_supply":"1000000000.0000 SYS"}' -p eosio.token@active
cleos push action eosio.token issue '[ "alice", "10000.0000 SYS", "memo" ]' -p eosio@active
cleos push action eosio.token transfer '[ "alice", "bob", "2.0000 SYS", "m" ]' -p alice@active

cleos get currency balance eosio.token bob SYS
cleos get currency balance eosio.token alice SYS
cleos get currency balance eosio.token eosio SYS
cleos get currency balance eosio.token eosio.token SYS

boost:
sudo apt-get install build-essential g++ python-dev autotools-dev libicu-dev build-essential libbz2-dev libboost-all-dev
tar -xjvf boost.tar.bz2
sudo ./bootstrap.sh --prefix=/usr/local/boost
sudo ./b2 install

bitcoin:
docker pull freewil/bitcoin-testnet-box
docker run -t -i -p 19001:19001 -p 19011:19011 freewil/bitcoin-testnet-box

docker exec -it 85551da51200 /bin/bash

bitcoind -datadir=1 -daemon
bitcoind -datadir=2 -daemon

bitcoin-cli -datadir=1 -getinfo
bitcoin-cli -datadir=2 -getinfo

bitcoin-cli -datadir=1 generate 1
bitcoin-cli -datadir=2 generate 1

bitcoin-cli -datadir=1 getnewaddress
bitcoin-cli -datadir=2 getnewaddress

bitcoin-cli -datadir=1 help

bitcoin-cli -datadir=1 getbalance
bitcoin-cli -datadir=2 getbalance

bitcoin-cli -datadir=1 getwalletinfo
bitcoin-cli -datadir=1 listwallets
bitcoin-cli -datadir=1 listaccounts

bitcoin-cli -datadir=1 getaccount "2NAW7hR3trVq9z3XnoKPdatSGSkus6qT5Gz"
bitcoin-cli -datadir=1 getaccountaddress ""
bitcoin-cli -datadir=1 getaccountaddress thomas
bitcoin-cli -datadir=1 getaddressesbyaccount ""
bitcoin-cli -datadir=1 getaddressesbyaccount thomas

bitcoin-cli -datadir=1 getpeerinfo

bitcoin-cli -datadir=1 gettransaction 441b096b2059eb782a4c60b7bc8538bc6cb7e0f5fb3c0e7c70aa46cfa1d55f01

bitcoin-cli -datadir=1 getblock 
bitcoin-cli -datadir=1 getblockchaininfo 
bitcoin-cli -datadir=1 getblockcount 
bitcoin-cli -datadir=1 getblockhash 107 

1:2N1H1D5pWTJvWpeJoZYL1z4NpuW48LWvGhr
2:2MzxYfzCuVUwaKTaxB48cX4Noh7RqcWByGM

bitcoin-cli -datadir=1 sendtoaddress 2MzxYfzCuVUwaKTaxB48cX4Noh7RqcWByGM 10
036b236789297d837906dd6b8a2b35913372e1a674ef699d9e9e32e5050a6f1a
ebd1454eadd38f7ef7e3bd1988c114911899efba8510bcee5e4c44c276515764

bitcoin-cli -datadir=1 listtransactions
bitcoin-cli -datadir=1 getrawtransaction 036b236789297d837906dd6b8a2b35913372e1a674ef699d9e9e32e5050a6f1a
bitcoin-cli -datadir=1 decoderawtransaction

bitcoin-cli -datadir=1 listreceivedbyaddress

bitcoin-cli -datadir=1 listunspent
bitcoin-cli -datadir=1 createrawtransaction
bitcoin-cli -datadir=1 signrawtransaction
bitcoin-cli -datadir=1 sendrawtransaction
bitcoin-cli -datadir=1 gettransaction

Vue Element-UI:
sudo npm install -g @vue/cli
vue create hello-world
npm run serve
npm run build
vue add element

npm install --save axios
npm install --save qs
npm install --save js-sha256
npm install --save vue-router

git clone https://github.com/lin-xin/vue-manage-system.git (npm install & npm run serve)
git clone https://github.com/PanJiaChen/vue-element-admin.git (npm install & npm run dev)
git clone https://github.com/PanJiaChen/vue-admin-template.git (npm install & npm run dev)


Kubernetes:
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

sudo su
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
exit

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
#sudo apt-get install -y kubelet=1.18.6-00 kubeadm=1.18.6-00 kubectl=1.18.6-00
#kubeadm config images list

echo "source <(kubectl completion bash)" >> ~/.bashrc
source ~/.bashrc

sudo apt-get install ipset ipvsadm
sudo modprobe -- ip_vs
sudo modprobe -- ip_vs_rr
sudo modprobe -- ip_vs_wrr
sudo modprobe -- ip_vs_sh
sudo modprobe -- nf_conntrack
#sudo modprobe -- nf_conntrack_ipv4
lsmod | grep -e ip_vs -e nf_conntrack

sudo vim /etc/containerd/config.toml
disabled_plugins = []
version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    sandbox_image = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8"
sudo systemctl restart containerd

free -hl
sudo swapoff -a
sudo kubeadm init \
    --apiserver-advertise-address=192.168.0.105 \
    --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \
    --kubernetes-version=v1.25.0 \
    --pod-network-cidr=10.244.0.0/16

sudo ctr images ls
sudo ctr images pull docker.io/library/hello-world:latest
sudo ctr run docker.io/library/hello-world:latest my_hello-world
sudo ctr run -t docker.io/library/busybox:latest mybusybox_demo sh
sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock images
sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

sudo kubeadm join 192.168.1.11:6443 --token t5dmmc.z4ko35k3lt710xxs \
  --discovery-token-ca-cert-hash sha256:f512b981b1d1526cfdb65b6d46e366cc6063c6006749843ad965d5174839f79a

kubectl get nodes -o wide
kubectl describe nodes node01
kubectl get pods -o wide --all-namespaces
kubeadm token list
kubeadm token create --print-join-command
kubectl describe pods kube-flannel-ds-8hc75 -n kube-flannel
kubectl create deployment nginx --image=nginx:alpine
kubectl get deployments
kubectl delete deployment ubuntu
kubectl scale deployment nginx --replicas=2
kubectl get pods -l app=nginx -o wide
kubectl expose deployment nginx --port=80 --type=NodePort
kubectl get services nginx
kubectl cluster-info

kubectl run -it curl --image=radial/busyboxplus:curl
nslookup nginx
curl http://nginx/
curl 10.244.1.2

#kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/calico.yaml
#kubectl taint node k8s-master node-role.kubernetes.io/master-
#kubectl taint node k8s-master node-role.kubernetes.io/master=:NoSchedule
#kubectl drain slave01 --delete-local-data --force --ignore-daemonsets
#kubectl delete node slave01

metrics-server:
docker pull bitnami/metrics-server:0.3.7
docker tag bitnami/metrics-server:0.3.7 k8s.gcr.io/metrics-server/metrics-server:v0.3.7
wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml
--kubelet-insecure-tls
--kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
kubectl apply -f components.yaml
kubectl top node
kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes"
kubectl -n kube-system top pods

sudo kubeadm reset --force
sudo rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd /etc/cni $HOME/.kube
sudo ipvsadm --clear
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X
sudo systemctl restart containerd

journalctl -u kubelet.service -e

#sudo swapon -a
#sudo vim /etc/fstab

Helm:
tar -xzvf helm-v2.12.3-linux-amd64.tar.gz
sudo cp linux-amd64/helm /usr/local/bin

helm version #注意tiller版本
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.3  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

#sudo apt-get install socat
#kubectl config view

#create a role and service account name
vim ~/helm/rbac-config.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system

#kubectl create -f rbac-config.yaml
#helm init --service-account tiller

kubectl get pods --all-namespaces -o wide
helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/
helm repo update
helm repo remove stable
helm repo list
helm search
helm search repo stable
helm ls
helm search hub
helm search hub wordpress
helm search repo

helm repo add bitnami https://charts.bitnami.com/bitnami
helm search repo bitnami
helm install my-release bitnami/redis
helm uninstall my-release
helm status my-release

helm show chart stable/mysql
helm show all stable/mysql
helm show values bitnami/redis

helm create deis-workflow
helm lint .
helm package deis-workflow
helm install deis-workflow ./deis-workflow-0.1.0.tgz

kubectl port-forward redis-master-765d459796-258hz 6379:6379 

#helm reset --force
#rm -rf $HOME/.helm

#helm mysql
sudo /etc/init.d/mysql stop

#sudo iptables -P FORWARD ACCEPT
kubectl get pv
kubectl get pvc

helm install test-mysql stable/mysql
helm install test-redis stable/redis

helm install --name my-release \
--set mysqlRootPassword=secretpassword,mysqlUser=my-user,mysqlPassword=my-password,mysqlDatabase=my-database \
  stable/mysql
helm install --name my-release -f values.yaml stable/mysql

helm ls --all
kubectl get pod --all-namespaces -o wide
kubectl describe pod my-release-mysql-86547664d5-z22sl

#helm del --purge my-release
#helm status my-release

MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default my-release-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)

kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il
apt-get update && apt-get install mysql-client -y
mysql -h my-release-mysql -p

MYSQL_HOST=127.0.0.1
MYSQL_PORT=3306
export POD_NAME=$(kubectl get pods --namespace default -l "app=my-release-mysql" -o jsonpath="{.items[0].metadata.name}")
kubectl port-forward $POD_NAME 3306:3306
mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}

kubectl delete pod ubuntu

Mesos&Marathon:

OpenStack:

ipfs:
ipfs init  QmZsv3ZxUJyTUGvzbC7WUWbLGCUMhyFkRy1Cihdtun43KL
ipfs init --profile server

ipfs id

ipfs cat /ipfs/QmS4ustL54uo8FzR9455qaxZwuMiUhyvMcX9Ba8nUH4uVv/readme
ipfs cat /ipfs/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/readme

ipfs cat /ipfs/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/quick-start

ipfs daemon
ipfs swarm peers

ipfs cat /ipfs/QmW2WQi7j6c7UgJTarActp7tDNikE4B2qXtFCfLPdsgaTQ/cat.jpg >cat.jpg

hash=`echo "I <a3 IPFS -$(whoami)" | ipfs add -q`
ipfs cat /ipfs/$hash

curl "https://ipfs.io/ipfs/$hash"
curl "http://127.0.0.1:8080/ipfs/$hash"

curl http://192.168.1.136:5001/webui
curl http://127.0.0.1:5001/webui

ipfs add myfile.txt QmQECo2p8LdVcjtkEWDVNVM7Hrsc7arW52P5vz5BVuvEgR
ipfs name publish　/ipfs/QmQECo2p8LdVcjtkEWDVNVM7Hrsc7arW52P5vz5BVuvEgR
ipfs name resolve QmQQ5t88W44Je5WgvmgpV1xSZTg1y5UXdYFHcQQ7EayxwR
https://ipfs.io/ipfs/QmQECo2p8LdVcjtkEWDVNVM7Hrsc7arW52P5vz5BVuvEgR
https://ipfs.io/ipns/QmQQ5t88W44Je5WgvmgpV1xSZTg1y5UXdYFHcQQ7EayxwR

ipfs key gen --type=rsa --size=2048 keyname
ipfs key list -l
ipfs name publish --key=keyname  QmPoyokqso3BKYCqwiU1rspLE59CPCv5csYhcPkEd6xvtm
ipfs resolve /ipns/QmYTpEqtNYvNFUwFysuRsEufNDJJBBEJfqsqrFXDT93sEE

Network:
#sudo vim /etc/init.d/rc.local
#sudo ifconfig ens33 hw ether 00:0c:29:9e:42:56
#cat /sys/class/net/ens33/address

sudo ifup eth0
sudo ifdown eth0
sudo ifconfig eth0 up
sudo ifconfig eth0 down
sudo ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255

sudo vim /etc/network/interfaces
auto lo
iface lo inet loopback

auto ens33   # 配置了这项systemctl restart networking才会生效
allow-hotplug ens33
# iface ens33 inet dhcp
iface ens33 inet static
address 192.168.1.146
netmask 255.255.255.0
gateway 192.168.1.1
hwaddress ether 00:0c:29:9e:42:56
dns-nameservers 8.8.8.8 8.8.4.4
broadcast 192.168.195.255

sudo vim /etc/resolv.conf
nameserver 192.168.0.1
nameserver 8.8.8.8

sudo vim /etc/resolvconf/resolv.conf.d/base
nameserver 192.168.1.146
nameserver 114.114.114.114

sudo vim /etc/dhcp/dhclient.conf
supersede domain-name-servers 8.8.8.8, 8.8.4.4; # 覆盖DNS服务器地址
prepend domain-name-servers 8.8.8.8, 8.8.4.4;   # 增加,优先级大
sudo systemctl restart networking.service
systemctl status networking.service


sudo vim /etc/netplan/00-installer-config.yaml
network:
  ethernets:
    ens33:
      dhcp4: true
      nameservers:
        addresses: [192.168.45.73]
  version: 2

systemctl status systemd-networkd.service
sudo systemctl restart systemd-networkd
sudo netplan apply
cat /run/systemd/resolve/resolv.conf
sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf

sudo vim /etc/systemd/resolved.conf
DNS=8.8.8.8 114.114.114.114

sudo systemctl restart systemd-resolved
sudo systemctl disable systemd-resolved
systemctl status systemd-resolved
resolvectl status
sudo resolvectl dns ens33 192.168.45.73
sudo resolvectl dns ens33 --reset

networkctl status
nmcli dev show | grep DNS

resolvconf NetworkManager/dhclient/ppp等网络管理工具配合使用,支持多种DNS解析器,包括本地缓存/本地域名服务器/公共DNS服务器
sudo apt install resolvconf
systemctl status resolvconf
sudo systemctl restart resolvconf

sudo vim /etc/hostname
ubuntuthomas1
同步修改/etc/hosts
sudo hostnamectl set-hostname dahai

sudo vim /etc/hosts
127.0.0.1   localhost
127.0.1.1   ubuntuthomas1 delete

192.168.1.145    ubuntuthomas
192.168.1.146    ubuntuthomas1
192.168.1.147    ubuntuthomas2

sudo /etc/init.d/networking restart
sudo reboot

Hadoop:
sudo useradd -m hadoop -s /bin/bash
sudo passwd hadoop
sudo adduser hadoop sudo

sudo chmod u+w /etc/sudoers
sudo vim /etc/sudoers
hadoop  ALL=(ALL:ALL) ALL
sudo chmod u-w /etc/sudoers

su - hadoop

sudo apt-get install openssh-server
ssh localhost
exit
cd ~/.ssh/
ssh-keygen -t rsa
cat ./id_rsa.pub >> ./authorized_keys
ssh localhost

http://mirrors.hust.edu.cn/apache/hadoop/common/
sudo tar -zxvf  hadoop-2.6.0.tar.gz -C /usr/local
cd /usr/local
sudo mv hadoop-2.6.0 hadoop
sudo chown -R hadoop:hadoop hadoop/

vim ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop
export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME

vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_202
export HADOOP_OPTS="-Djava.library.path=${HADOOP_HOME}/lib/native"

vim /usr/local/hadoop/etc/hadoop/core-site.xml
<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>file:/usr/local/hadoop/tmp</value>
    <description>Abase for other temporary directories.</description>
  </property>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>

vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/name</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/data</value>
  </property>
</configuration>

集群部署：
vim etc/hadoop/core-site.xml
<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>file:/usr/local/hadoop/tmp</value>
    <description>Abase for other temporary directories.</description>
  </property>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://ubuntuthomas:9000</value>
  </property>
</configuration>

vim etc/hadoop/hdfs-site.xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/name</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/data</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>ubuntuthomas:9001</value>
  </property>
</configuration>

vim etc/hadoop/mapred-site.xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>

<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>

vim etc/hadoop/yarn-site.xml
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>ubuntuthomas</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
  </property>
</configuration>

vim etc/hadoop/workers
ubuntuthomas1
ubuntuthomas2

scp -r etc/hadoop/* hadoop@ubuntuthomas1:/usr/local/hadoop/etc/hadoop/
scp -r etc/hadoop/* hadoop@ubuntuthomas2:/usr/local/hadoop/etc/hadoop/

rm -rf logs tmp
hdfs namenode -format
start-all.sh
stop-all.sh

echo "My name is Li Thomas. This is a example program called WordCount, run by Li Thomas " >> testWordCount
hadoop fs -mkdir /wordCountInput
hadoop fs -put testWordCount /wordCountInput
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar wordcount /wordCountInput /wordCountOutput
hadoop fs -ls /wordCountOutput
hadoop fs -cat /wordCountOutpart-r-00000

Hive:
mysql -u root -p
create user 'hive'@'%' identified by 'hive';
select Host,User from mysql.user;
grant all privileges on *.* to 'hive'@'%' with grant option;
flush privileges;
create database if not exists hive_metadata;
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive';
flush privileges;
exit
sudo /etc/init.d/mysql restart
mysql -u hive -p hive
show databases;
#create database hive_metadata;

http://www.eu.apache.org/dist/hive/
sudo tar -zxvf apache-hive-2.3.4-bin.tar.gz -C /usr/local
mv apache-hive-2.3.4-bin hive
sudo chown -R hadoop:hadoop hive/
cp hive-default.xml.template hive-site.xml

vim conf/hive-site.xml
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive_metadata?createDatabaseIfNotExist=true</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>

<property>
  <name>system:java.io.tmpdir</name>
  <value>/usr/local/hive/tmpdir</value>
</property>

<property>
  <name>system:user.name</name>
  <value>hive</value>
</property>

<--property>
  <name>datanucleus.schema.autoCreateAll</name>
  <value>true</value>
</property>

<property>
  <name>hive.metastore.schema.verification</name>
  <value>false</value>
</property>

cp hive-env.sh.template hive-env.sh
vim hive-env.sh
export HADOOP_HOME=/usr/local/hadoop
export HIVE_CONF_DIR=/usr/local/hive/conf

vim hive-config.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_91
export HIVE_HOME=/usr/local/hive
export HADOOP_HOME=/usr/local/hadoop

vim ~/.bashrc
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin

cd /usr/local/hive
wget http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz
tar -zxvf mysql-connector-java-5.1.47.tar.gz
cp mysql-connector-java-5.1.47-bin.jar ../lib/

schematool -dbType mysql -initSchema
hive

HBase:
wget http://mirror.bit.edu.cn/apache/hbase/1.4.9/hbase-1.4.9-bin.tar.gz

sudo tar -zxvf hbase-1.4.9-bin.tar.gz -C /usr/local/
sudo mv hbase-1.4.9 hbase
sudo chown -R hadoop:hadoop hbase/

vim ~/.bashrc
export HBASE_HOME=/usr/local/hbase
export PATH=$PATH:$HBASE_HOME/bin

vim conf/hbase-env.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_91
export HBASE_CLASSPATH=/usr/local/hbase/conf
export HBASE_MANAGES_ZK=false

vim conf/hbase-site.xml
<configuration>
  <property>
    <name>hbase.master</name>
    <value>thomas:60000</value>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://thomas:9000/hbase</value>
  </property>

  <property>
    <name>hbase.master.maxclockskew</name>
    <value>180000</value>
  </property>

  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>thomas:2181,slave01:2181,slave02:2181</value>
    <description>The directory shared by RegionServers.
    </description>
  </property>

  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>

  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/usr/local/zookeeper/data</value>
    <description>Property from ZooKeeper config zoo.cfg.
    The directory
    where the snapshot is stored.
    </description>
  </property>

</configuration>

vim conf/regionservers
slave01
slave02

scp -r hbase/* hadoop@slave01:/usr/local/hbase/
scp -r hbase/* hadoop@slave02:/usr/local/hbase/

start-hbase.sh
hbase shell
create 'member', 'm_id', 'address', 'info'
list

Scala:
java -version
sudo tar -zxvf scala-2.12.8.tgz -C /usr/local
sudo mv scala-2.12.8 scala

vim ~/.bashrc
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin

cd /usr/local
sudo mkdir scala
sudo chown -R hadoop:hadoop scala

scp -r scala/* hadoop@slave01:/usr/local/scala/

Spark:
sudo tar -zxvf spark-2.4.0-bin-hadoop2.7.tgz -C /usr/local
sudo mv spark-2.4.0-bin-hadoop2.7 spark
sudo chown -R hadoop:hadoop spark

cp spark-env.sh.template spark-env.sh
vim conf/spark-env.sh
export SCALA_HOME=/usr/local/scala
export JAVA_HOME=/usr/local/java/jdk1.8.0_91
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
SPARK_LOCAL_DIRS=/usr/local/spark
SPARK_DRIVER_MEMORY=1G
export LD_LIBRARY_PATH=/usr/local/hadoop/lib/native/:$LD_LIBRARY_PATH

export SPARK_MASTER_IP=192.168.1.7
export SPARK_MASTER_HOST=192.168.1.7
export SPARK_LOCAL_IP=192.168.1.7        #每个节点不一样
export SPARK_WORKER_MEMORY=1g
export SPARK_WORKER_CORES=2
export SPARK_HOME=/usr/local/spark
export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)

cp slaves.template slaves
vim slaves
slave01
slave02

sudo mkdir spark
sudo chown -R hadoop:hadoop spark

scp -r spark/* hadoop@slave01:/usr/local/spark/
scp -r spark/* hadoop@slave02:/usr/local/spark/

cd /usr/local/spark/sbin
./start-all.sh

cd /usr/local/spark/bin
./run-example SparkPi 10 --slave01 local[2]

go-micro:
protobuf --> protoc
protobuf+golang --> protoc-gen-go
protobuf+golang+grpc --> protoc-gen-go

go + protobuf:
protoc --go_out=. *.proto

go + grpc + protobuf:
protoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld

unzip consul_1.4.4_linux_amd64.zip
cp consul ~/golang/bin/
consul agent -dev
consul members -detailed
curl localhost:8500/v1/catalog/nodes
dig @127.0.0.1 -p 8600 thomas.node.consul

consul kv put redis/config/minconns 1
consul kv get redis/config/minconns
consul kv get -recurse

go run main.go --registry=consul

etcd:
WAL：Write Ahead Log(预写式日志)
Snapshot、Entry、HTTP Server、Store、Raft
静态配置启动、etcd自身服务发现、通过DNS进行服务发现

docker run -d -p 2379:2379 -e "ALLOW_NONE_AUTHENTICATION=yes" bitnami/etcd

tar -xzvf etcd-v3.3.13-linux-amd64.tar.gz -C ~/server/
./etcd
export ETCDCTL_API=3
./etcdctl --endpoints=localhost:2379 put key value
./etcdctl --endpoints=localhost:2379 get key

rm -rf /var/lib/etcd/member/*

scp etcd-v3.3.13-linux-amd64.tar.gz thomas@node01:/home/thomas/server
scp etcd-v3.3.13-linux-amd64.tar.gz thomas@node02:/home/thomas/server
tar -xzvf etcd-v3.3.13-linux-amd64.tar.gz

cd /home/thomas/server/etcd-v3.3.13-linux-amd64
rm -rf data.etcd

nohup ./etcd --data-dir=data.etcd >>etcd.log 2>&1 &
nohup ./src/redis-server redis.conf &

nohup ./etcd --data-dir=data.etcd --name et-wang-1 \
  --initial-advertise-peer-urls http://192.168.0.106:2380 --listen-peer-urls http://192.168.0.106:2380 \
  --advertise-client-urls http://192.168.0.106:2379 --listen-client-urls http://192.168.0.106:2379 \
  --initial-cluster et-wang-1=http://192.168.0.106:2380 \
  --initial-cluster-state new \
  --initial-cluster-token token-wang-group-1 \
  >>etcd.log 2>&1 &

./etcdctl --endpoints=http://192.168.0.106:2379 get / --prefix

nohup ./etcd --data-dir=data.etcd --name et-wang-1 \
  --initial-advertise-peer-urls http://192.168.1.7:2380 --listen-peer-urls http://192.168.1.7:2380 \
  --advertise-client-urls http://192.168.1.7:2379 --listen-client-urls http://192.168.1.7:2379 \
  --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-3=http://192.168.1.11:2380 \
  --initial-cluster-state new \
  --initial-cluster-token token-wang-group-1 \
  >>etcd.log 2>&1 &

nohup ./etcd --data-dir=data.etcd --name et-wang-2 \
  --initial-advertise-peer-urls http://192.168.1.10:2380 --listen-peer-urls http://192.168.1.10:2380 \
  --advertise-client-urls http://192.168.1.10:2379 --listen-client-urls http://192.168.1.10:2379 \
  --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-3=http://192.168.1.11:2380 \
  --initial-cluster-state new \
  --initial-cluster-token token-wang-group-1 \
  >>etcd.log 2>&1 &


nohup ./etcd --data-dir=data.etcd --name et-wang-3 \
  --initial-advertise-peer-urls http://192.168.1.11:2380 --listen-peer-urls http://192.168.1.11:2380 \
  --advertise-client-urls http://192.168.1.11:2379 --listen-client-urls http://192.168.1.11:2379 \
  --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-3=http://192.168.1.11:2380 \
  --initial-cluster-state new \
  --initial-cluster-token token-wang-group-1 \
  >>etcd.log 2>&1 &

export ETCDCTL_API=3
./etcdctl --endpoints=192.168.1.7:2379 member list
./etcdctl --endpoints=192.168.1.7:2379 put wang king
./etcdctl --endpoints=192.168.1.7:2379 get wang
./etcdctl --endpoints=192.168.1.10:2379 get wang

./etcdctl --endpoints=192.168.1.7:2379 put web1 value1
./etcdctl --endpoints=192.168.1.7:2379 put web2 value2
./etcdctl --endpoints=192.168.1.7:2379 put web3 value3
./etcdctl --endpoints=192.168.1.7:2379 get web --prefix

./etcdctl --endpoints=192.168.1.7:2379 put key beer
./etcdctl --endpoints=192.168.1.7:2379 get key
./etcdctl --endpoints=192.168.1.7:2379 del key

./etcdctl --endpoints=192.168.1.7:2379 put k1 value1
./etcdctl --endpoints=192.168.1.7:2379 put k2 value2
./etcdctl --endpoints=192.168.1.7:2379 get k --prefix
./etcdctl --endpoints=192.168.1.7:2379 del k --prefix

./etcdctl --endpoints=192.168.1.7:2379 put user1 bad
./etcdctl --endpoints=192.168.1.7:2379 txn --interactive
compares:
value("user1") = "bad"
success requests (get, put, delete):
del user1
failure requests (get, put, delete):
put user1 good

./etcdctl --endpoints=192.168.1.7:2379 get user1

./etcdctl --endpoints=192.168.1.7:2379 watch stock1
./etcdctl --endpoints=192.168.1.7:2379 put stock1 1000
./etcdctl --endpoints=192.168.1.7:2379 watch stock --prefix
./etcdctl --endpoints=192.168.1.7:2379 put stock1 10
./etcdctl --endpoints=192.168.1.7:2379 put stock2 20

./etcdctl --endpoints=192.168.1.7:2379 lease grant 30
./etcdctl --endpoints=192.168.1.7:2379 put sample value --lease=5e156b838cfa311e
./etcdctl --endpoints=192.168.1.7:2379 get sample
./etcdctl --endpoints=192.168.1.7:2379 lease keep-alive 5e156b838cfa311e
./etcdctl --endpoints=192.168.1.7:2379 lease revoke 5e156b838cfa311e
./etcdctl --endpoints=192.168.1.7:2379 get sample

./etcdctl --endpoints=192.168.1.7:2379 lock mutex1

./etcdctl --endpoints=192.168.1.7:2379,192.168.1.10:2379,192.168.1.11:2379 --write-out=table endpoint status
./etcdctl --endpoints=192.168.1.7:2379 endpoint health

./etcdctl --endpoints=192.168.1.7:2379 elect one p1

./etcdctl --endpoints=192.168.1.7:2379 snapshot save my.db
./etcdctl --endpoints=192.168.1.7:2379 --write-out=table snapshot status my.db

./etcdctl --endpoints=192.168.1.7:2379 member list
./etcdctl --endpoints=192.168.1.7:2379 member remove 1e91165f86b1da62
./etcdctl --endpoints=192.168.1.7:2379 member add et-wang-4 --peer-urls=http://192.168.1.11:2380

rm -rf data.etcd
./etcd --data-dir=data.etcd --name et-wang-4 \
  --initial-advertise-peer-urls http://192.168.1.11:2380 --listen-peer-urls http://192.168.1.11:2380 \
  --advertise-client-urls http://192.168.1.11:2379 --listen-client-urls http://192.168.1.11:2379 \
  --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-4=http://192.168.1.11:2380 \
  --initial-cluster-state existing \
  --initial-cluster-token token-wang-group-1

./etcdctl --endpoints=192.168.1.7:2379 role add root
./etcdctl --endpoints=192.168.1.7:2379 role grant-permission root readwrite fookey
./etcdctl --endpoints=192.168.1.7:2379 role get root
./etcdctl --endpoints=192.168.1.7:2379 role list

./etcdctl --endpoints=192.168.1.7:2379 user add root
./etcdctl --endpoints=192.168.1.7:2379 user grant-role root root
./etcdctl --endpoints=192.168.1.7:2379 user get root
./etcdctl --endpoints=192.168.1.7:2379 user list

./etcdctl --endpoints=192.168.1.7:2379 auth enable

./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 put fookey people
./etcdctl --endpoints=192.168.1.7:2379 get fookey
./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 get fookey
./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 get foo1

./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 auth disable

go build -o app *.go
cd golang/src/
mkdir -p ant/conf

scp app thomas@node01:/home/thomas/golang/src/ant
scp conf/app.yaml thomas@node01:/home/thomas/golang/src/ant/conf
scp app thomas@node02:/home/thomas/golang/src/ant
scp conf/app.yaml thomas@node02:/home/thomas/golang/src/ant/conf

GRPC:
service
method
message

Message
NewMessage
NewMessageWithExtensionRegistry
NewMessageWithMessageFactory
AsDynamicMessage
AsDynamicMessageWithExtensionRegistry
AsDynamicMessageWithMessageFactory

Stub
NewStub
NewStubWithMessageFactory

MessageFactory
NewMessageFactoryWithExtensionRegistry
NewMessageFactoryWithKnownTypeRegistry
NewMessageFactoryWithDefaults
NewMessageFactoryWithRegistries

ExtensionRegistry
NewExtensionRegistryWithDefaults

KnownTypeRegistry
NewKnownTypeRegistryWithDefaults
NewKnownTypeRegistryWithoutWellKnownTypes

MessageRegistry
NewMessageRegistryWithDefaults

ELK:
Elasticsearch:
sudo dpkg -i elasticsearch-7.2.0-amd64.deb

sudo systemctl daemon-reload
sudo systemctl enable elasticsearch.service
sudo systemctl start elasticsearch.service
sudo systemctl stop elasticsearch.service

sudo /etc/init.d/elasticsearch start
/etc/elasticsearch
/usr/share/elasticsearch
sudo journalctl --unit elasticsearch

tar -xzvf elasticsearch-7.6.1-linux-x86_64.tar.gz -C ../server/
mkdir data logs

vim config/elasticsearch.yml
cluster.name: es-cluster01
node.name: es-node01
path.data: /home/thomas/server/elasticsearch-7.6.1/data
path.logs: /home/thomas/server/elasticsearch-7.6.1/logs
node.master: true
node.data: true
http.port: 9200
#network.bind_host: 0.0.0.0
#network.publish_host: 192.168.1.10
network.host: 192.168.1.10
#discovery.zen.ping.unicast.hosts: ["192.168.1.10:9300","192.168.1.11:9300","192.168.1.13:9300"]
#discovery.zen.minimum_master_nodes: 2
#http.cors.enabled: true
#http.cors.allow-origin: "*"
discovery.seed_hosts: ["192.168.1.10", "192.168.1.11", "192.168.1.13"]
cluster.initial_master_nodes: ["es-node01", "es-node02", "es-node04"]

sudo vim /etc/sysctl.conf
vm.max_map_count=655360
sudo sysctl -p

cd /home/thomas/server/elasticsearch-7.6.1
rm -rf data/*
./bin/elasticsearch -d
sudo lsof -i tcp:9200

curl -X GET 192.168.1.10:9200
curl -X GET 192.168.1.10:9200/_cluster/health?pretty
curl -X GET 192.168.1.10:9200/_cluster/stats?pretty
curl -X GET 192.168.1.10:9200/_cat/health?v
curl -X GET 192.168.1.10:9200/_cat/nodes?v
curl -X GET 192.168.1.10:9200/_nodes/process?pretty
curl -X GET 192.168.1.10:9200/_stats?pretty

curl -X GET 192.168.1.10:9200/logstash-2018.10.21/_stats?pretty
curl -X GET 192.168.1.10:9200/logstash-2018.10.21/_mapping?pretty

curl -X GET 192.168.1.10:9200/index_name/_settings?pretty
curl -X GET 192.168.1.10:9200/index_name/_mapping?pretty

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/index_name/index_type/1 -d '{"name":"wes"}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/index_name/_delete_by_query -d '{"query":{"match":{"message":"some message"}}}'

curl -X PUT -H "Content-Type:application/json" 192.168.1.10:9200/index_name/type1/1 -d '{"counter":1,"tags":["red"]}'

创建索引
curl -X PUT 192.168.1.10:9200/customer?pretty

查询所有索引
curl -X GET 192.168.1.10:9200/_cat/indices?v

插入数据
curl -X PUT -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/1?pretty -d '{"name":"Xiaoming"}'

查询数据
curl -X GET 192.168.1.10:9200/customer/external/1?pretty
curl -X GET 192.168.1.10:9200/customer/external/1/_source?pretty

删除索引
curl -X DELETE 192.168.1.10:9200/customer?pretty

修改数据
curl -X PUT -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/1?pretty -d '{"name":"HuangXiaoming"}'

更新数据
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/1/_update?pretty -d '{"doc":{"name":"xiaohong"}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/1/_update?pretty -d '{"doc":{"name":"xiaohong2", "age":33}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/1/_update?pretty -d '{"script":"ctx._source.age += 5"}'

删除数据
curl -X PUT -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/2?pretty -d '{"length":"secret"}'
curl -X DELETE 192.168.1.10:9200/customer/external/2?pretty

批处理
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/_bulk?pretty -d '
{"index":{"_id":"1"}}
{"name":"Johnsdfcsd Doe"}
{"index":{"_id":"2"}}
{"name":"Jane sdfvsdDoe"}
'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/customer/external/_bulk?pretty -d '
{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_bulk?pretty --data-binary "@accounts.json"
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/account/_bulk?pretty --data-binary "@accounts.json"
curl -X POST -H "Content-Type:application/json" "192.168.1.10:9200/bank/_bulk?pretty&refresh" --data-binary "@accounts.json"

curl -X GET "192.168.1.10:9200/bank/_search?q=*&pretty"

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match_all":{}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match_all":{}}, "size":1}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match_all":{}}, "from":10, "size":4}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match_all":{}}, "sort":{"balance":{"order":"desc"}}}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match_all":{}}, "_source":["account_number", "balance"]}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match":{"account_number":20}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match":{"address":"mill"}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match":{"address":"mill lane"}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"match_phrase":{"address":"mill lane"}}}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"bool":{"must":[{"match":{"address":"mill"}}, {"match":{"address":"lane"}}]}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"bool":{"should":[{"match":{"address":"mill"}}, {"match":{"address":"lane"}}]}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query":{"bool":{"must_not":[{"match":{"address":"mill"}}, {"match":{"address":"lane"}}]}}}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query": {"bool": {"must": [{"match": {"age": "40"}}],"must_not": [{"match": {"state": "ID"}}]}}}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"query": {"bool": {"must": { "match_all": {} },"filter": {"range": {"balance": {"gte": 20000,"lte": 30000}}}}}}'

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"size": 0,"aggs": {"group_by_state": {"terms": {"field": "state.keyword"}}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"size": 0,"aggs": {"group_by_state": {"terms": {"field": "state.keyword"},"aggs": {"average_balance": {"avg": {"field": "balance"}}}}}}'
curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/bank/_search?pretty -d '{"size": 0,"aggs": {"group_by_state": {"terms": {"field": "state.keyword","order":{"average_balance":"desc"}},"aggs": {"average_balance": {"avg": {"field": "balance"}}}}}}'

模板 mapping 快照 仓库
curl -X GET 192.168.1.10:9200/_template?pretty
curl -X DELETE 192.168.1.10:9200/_template/no_analyzed

curl -X PUT -H "Content-Type:application/json" 192.168.1.10:9200/_template/t1 -d '{"template" : "*", "settings": {"number_of_replicas": "0"}}'
curl -X PUT -H "Content-Type:application/json" 192.168.1.10:9200/_template/no_analyzed -d '
{"template": "*","mappings": {"_default_": {"dynamic_templates": [{"strings": {"match_mapping_type": "string","mapping": {"type": "keyword"}}}]}}}'
curl -X GET 192.168.1.10:9200/_cat/repositories?v

curl -X POST -H "Content-Type:application/json" 192.168.1.10:9200/logstash-2020.03.14-000001/_search?pretty -d '{"query":{"match_all":{}}}'

Logstash:
sudo -E dpkg -i logstash-7.2.0.deb
#sudo /etc/init.d/logstash start
#sudo systemctl start logstash.service

ps -ef | grep logstash
ps aux | grep pid
netstat -tunlp | grep port

vim /etc/logstash/metrics-pipelines.conf
cd /usr/share/logstash
sudo -E ./bin/logstash -f /etc/logstash/metrics-pipelines.conf

tar -xzvf logstash-7.6.1.tar.gz -C ../server/
cd /home/thomas/server/logstash-7.6.1
bin/logstash -e 'input { stdin { } } output { stdout {} }'

vim config/logstash.conf
input {
    file {
        path => ["/home/thomas/server/logstash-7.6.1/logs/apple.log"]
        type => "file_monitor"
        tags => ["有用的","标识用的"]
        start_position => "beginning"
    }
}
input {
    beats {
        port => "5044"
    }
}
output {
   stdout { codec => rubydebug }
}

output {
  elasticsearch { hosts => ["192.168.1.10:9200"] }
  stdout { codec => rubydebug }
}

bin/logstash -f config/logstash.conf
nohup bin/logstash -f config/logstash.conf >/dev/null 2>&1 &
echo "this is a cat" >> apple.log

Filebeat:
sudo dpkg -i filebeat-7.2.0-amd64.deb

/usr/share/filebeat
sudo vim /etc/filebeat/filebeat.yml

sudo service filebeat start
sudo service filebeat stop
sudo service filebeat restart

sudo filebeat setup -e \
  -E output.logstash.enabled=false \
  -E output.elasticsearch.hosts=['localhost:9200'] \
  -E output.elasticsearch.username=filebeat_internal \
  -E output.elasticsearch.password=YOUR_PASSWORD \
  -E setup.kibana.host=localhost:5601

tar -xzvf filebeat-7.6.1-linux-x86_64.tar.gz -C ../server/
cd /home/thomas/server/filebeat-7.6.1-linux-x86_64
vim filebeat.yml
enabled: true

nohup ./filebeat -e -c filebeat.yml >/dev/null 2>&1 &

Kibana:
sudo dpkg -i kibana-7.2.0-amd64.deb
sudo systemctl daemon-reload
sudo systemctl enable kibana.service
sudo systemctl start kibana.service
sudo systemctl stop kibana.service

/var/log/kibana/
/etc/kibana/kibana.yml
/var/lib/kibana path.data

192.168.1.7:5601

tar -xzvf kibana-7.6.1-linux-x86_64.tar.gz -C ../server/
cd /home/thomas/server/kibana-7.6.1-linux-x86_64
vim config/kibana.yml
server.host: "192.168.1.10"
elasticsearch.hosts: ["http://192.168.1.10:9200"]

bin/kibana
http://192.168.1.10:5601


Metricbeat:
sudo dpkg -i metricbeat-7.2.0-amd64.deb

sudo metricbeat modules enable system
sudo metricbeat setup -e
sudo service metricbeat start
sudo service metricbeat stop
sudo service metricbeat restart

ps -ef | grep metricbeat
sudo vim /etc/metricbeat/metricbeat.yml

http://192.168.1.7:5601/app/kibana#/dashboard/Metricbeat-system-overview-ecs

Prometheus:
tar -xzvf prometheus-2.16.0.linux-amd64.tar.gz -C /home/thomas/server/
cd /home/thomas/server/prometheus-2.16.0.linux-amd64
vim prometheus.yml
./prometheus --config.file=prometheus.yml

192.168.1.10:9090
192.168.1.10:9090/metrics
192.168.1.10:9090/graph

docker run -p 9090:9090 prom/prometheus
docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus
docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus

tar -xzvf node_exporter-0.18.1.linux-amd64.tar.gz -C /home/thomas/server/
cd /home/thomas/server/node_exporter-0.18.1.linux-amd64
./node_exporter
curl http://192.168.1.10:9100/metrics

100-irate(node_cpu_seconds_total{job="linux-node",mode="idle"}[5m])*100
100 - (avg by (instance) (irate(node_cpu_seconds_total{instance="192.168.1.10:9100", mode="idle"}[5m])) * 100)
avg by (instance, mode) (irate(node_cpu_seconds_total{instance="192.168.1.10:9100"}[5m])) * 100
node_load1{instance="192.168.1.10:9100"}

// 入包量
sum by (instance) (rate(node_network_receive_bytes_total{instance="192.168.1.10:9100",device!="lo"}[5m]))
// 出包量
sum by (instance) (rate(node_network_transmit_bytes_total{instance="192.168.1.10:9100",device!="lo"}[5m]))

100-(node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes)/node_memory_MemTotal_bytes*100
100-node_filesystem_free_bytes{mountpoint="/"}/node_filesystem_size_bytes{mountpoint="/"}*100

Grafana:
sudo dpkg -i grafana_6.2.5_amd64.deb

sudo service grafana-server start
admin / admin
sudo update-rc.d grafana-server defaults

sudo systemctl daemon-reload
sudo systemctl start grafana-server
sudo systemctl status grafana-server
sudo systemctl enable grafana-server.service

sudo vim /etc/grafana/grafana.ini

http://192.168.1.7:3000/

docker run -d -p 3000:3000 --name grafana grafana/grafana

wget https://dl.grafana.com/oss/release/grafana-6.6.2.linux-amd64.tar.gz
tar -xzvf grafana-6.6.2.linux-amd64.tar.gz -C ../server/
cd /home/thomas/server/grafana-6.6.2
./bin/grafana-server web

InfluxDB:

Supervisor:
//sudo pip install --upgrade pip
//sudo npm uninstall -g supervisor

sudo pip install supervisor
echo_supervisord_conf

mkdir ~/supervisor
cd ~/supervisor
echo_supervisord_conf > supervisord.conf

supervisord -c supervisord.conf
ps -ef | grep supervisor
supervisorctl -c supervisord.conf

MicroService:
#docker run -d -p 8080:8080 ants/app01:v1.0
#docker run -i -t ants/app01:v1.0 /bin/bash
#docker exec -it d92a084a8d89 /bin/sh 
#docker exec -it ants/app01:v1.0 /bin/bash
#docker save -o app01.tar ants/app01:v1.0
#scp app01.tar thomas@node01:/home/thomas/golang/src/app01/
#scp app01.tar thomas@node02:/home/thomas/golang/src/app01/
#docker load -i app01.tar
#dig -t A draveness.me +trace
#dig +trace +additional draveness.me

CGO_ENABLED=0 go build -o app01 main.go
docker build -t ants/app01:v1.0 .

scp * thomas@node01:/home/thomas/golang/src/app01/
kubectl apply -f app01-deployment.yaml
kubectl apply -f app01-service.yaml

hostname:foo
subdomain:bar
namespace:my-namespace
foo.bar.my-namespace.svc.cluster-domain.example

kubectl get nodes
kubectl get pods --all-namespaces -o wide
kubectl get deployments --all-namespaces -o wide
kubectl get services --all-namespaces -o wide
kubectl get configmaps --all-namespaces -o wide
kubectl get ingress --all-namespaces -o wide
kubectl describe pod app01-deployment-d68448f69-484pp
kubectl describe ingress ants-ingress
kubectl -n kube-system get configmap coredns -oyaml
kubectl delete --namespace=kube-system deployment coredns

kubectl exec -ti app02-deployment-5989c68c56-zw6ff -- /bin/sh
kubectl exec -ti nginx-ingress-controller-7995bd9c47-2n9v4 -n ingress-nginx -- /bin/bash

curl 10.110.76.133:8080/ping
curl 10.99.236.149:8081/ping
curl 192.168.1.10:30001/ping
curl 192.168.1.10:30002/ping
curl nginx.default.svc.cluster.local
curl http://app01-service.default.svc.cluster.local:8080/ping
curl app01.app.com:30367/ping
curl app02.app.com:30367/ping

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

POD_NAME=$(kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $POD_NAME -n ingress-nginx -- /nginx-ingress-controller --version

Istio:
control plane,data plane,Envoy,sidecar,Mixer,Pilot,Galley,Citadel
iostat -k -x 1 3

Envoy:
docker pull envoyproxy/envoy:v1.11.1
docker run --rm -d -p 10000:10000 envoyproxy/envoy:v1.11.1
curl -v localhost:10000

Istio集群:
mkdir -p /home/thomas/server/servicemesh/istio
cd /home/thomas/server/servicemesh/istio
#curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.3.3 sh -
curl -L https://istio.io/downloadIstio | sh -
cd istio-1.3.3
sudo cp bin/istioctl /usr/bin/
sudo chmod 755 /usr/bin/istioctl

#for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
#kubectl apply -f install/kubernetes/istio-demo.yaml

istioctl manifest apply --set values.grafana.enabled=true \
                        --set values.kiali.enabled=true \
                        --set values.prometheus.enabled=true \
                        --set values.tracing.enabled=true

istioctl manifest generate --set values.grafana.enabled=true \
                           --set values.kiali.enabled=true \
                           --set values.prometheus.enabled=true \
                           --set values.tracing.enabled=true \
                           > ./generated-manifest.yaml

istioctl verify-install -f ./generated-manifest.yaml

#docker pull docker.io/istio/citadel:1.4.0
#docker pull docker.io/istio/galley:1.4.0
#docker pull docker.io/istio/proxyv2:1.4.0
#docker pull docker.io/istio/pilot:1.4.0
#docker pull docker.io/istio/mixer:1.4.0
#docker pull docker.io/istio/sidecar_injector:1.4.0
#docker pull grafana/grafana:6.4.3
#docker pull docker.io/jaegertracing/all-in-one:1.14
#docker pull quay.io/kiali/kiali:v1.9
#docker pull docker.io/prom/prometheus:v2.12.0

#docker pull docker.io/istio/examples-bookinfo-details-v1:1.15.0
#docker pull docker.io/istio/examples-bookinfo-ratings-v1:1.15.0
#docker pull docker.io/istio/examples-bookinfo-reviews-v1:1.15.0
#docker pull docker.io/istio/examples-bookinfo-reviews-v2:1.15.0
#docker pull docker.io/istio/examples-bookinfo-reviews-v3:1.15.0
#docker pull docker.io/istio/examples-bookinfo-productpage-v1:1.15.0

kubectl label namespace default istio-injection=enabled
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o "<title>.*</title>"
kubectl get pod --all-namespaces -o wide

kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
kubectl get gateway
kubectl get virtualservices
kubectl get destinationrules

#kubectl apply -f samples/httpbin/httpbin.yaml
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}')

export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
curl -s http://${GATEWAY_URL}/productpage | grep -o "<title>.*</title>"

#samples/bookinfo/platform/kube/cleanup.sh
#kubectl delete -f install/kubernetes/istio-demo.yaml
#for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl delete -f $i; done

流量治理:
kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml
kubectl get destinationrules -o yaml

流量切到v1
kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml
kubectl get virtualservices -o yaml

流量切到v2
kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml

故障注入
kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml
kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml

按比例切流量
kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml
kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml

Circuit Breaking
kubectl apply -f samples/httpbin/httpbin.yaml

kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: httpbin
spec:
  host: httpbin
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1
      http:
        http1MaxPendingRequests: 1
        maxRequestsPerConnection: 1
    outlierDetection:
      consecutiveErrors: 1
      interval: 1s
      baseEjectionTime: 3m
      maxEjectionPercent: 100
EOF

kubectl get destinationrule httpbin -o yaml

kubectl apply -f samples/httpbin/sample-client/fortio-deploy.yaml
FORTIO_POD=$(kubectl get pod | grep fortio | awk '{ print $1 }')
kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -curl  http://httpbin:8000/get
kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://httpbin:8000/get
kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -c 3 -qps 0 -n 30 -loglevel Warning http://httpbin:8000/get
kubectl exec $FORTIO_POD -c istio-proxy -- pilot-agent request GET stats | grep httpbin | grep pending

#kubectl delete destinationrule httpbin
#kubectl delete deploy httpbin fortio-deploy
#kubectl delete svc httpbin
#kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml

Gateway
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
export INGRESS_HOST=$(kubectl get po -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].status.hostIP}')

kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway # use Istio default gateway implementation
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "httpbin.example.com"
EOF

kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - "httpbin.example.com"
  gateways:
  - httpbin-gateway
  http:
  - match:
    - uri:
        prefix: /status
    - uri:
        prefix: /delay
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF

curl -I -HHost:httpbin.example.com http://$INGRESS_HOST:$INGRESS_PORT/status/200
curl -I -HHost:httpbin.example.com http://$INGRESS_HOST:$INGRESS_PORT/headers

kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway # use Istio default gateway implementation
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - "*"
  gateways:
  - httpbin-gateway
  http:
  - match:
    - uri:
        prefix: /headers
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF

curl http://$INGRESS_HOST:$INGRESS_PORT/headers

Jaeger:
#http://<IP ADDRESS OF CLUSTER INGRESS>:15032/
#istioctl dashboard jaeger

实现了OpenTracing
trace_id，span_id，parentId
span:单次调用
trace:一条完整的调用,多个span组成

Span:
An operation name，操作名称
A start timestamp，起始时间
A finish timestamp，结束时间
Span Tag，一组键值对构成的Span标签集合。键值对中，键必须为string，值可以是字符串，布尔，或者数字类型
Span Log，一组span的日志集合

jaeger-client(send spans to agent in UDP)
jarger-agent(recv from client push to collector)
jaeger-collector(recv agent to DB or kafka)
jaeger-ingester(from kafka to DB)
jaeger-query
web ui
DB(Cassandra/Elasticsearch)

kubectl -n istio-system port-forward --address 0.0.0.0 $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 15032:16686
for i in `seq 1 100`; do curl -s -o /dev/null http://$GATEWAY_URL/productpage; done
kubectl exec -it -n istio-system istio-ingressgateway-76c6d6485c-kbrq7 -- bash

Kiali:
#http://<IP ADDRESS OF CLUSTER INGRESS>:15029/

KIALI_USERNAME=$(read -p 'Kiali Username: ' uval && echo -n $uval | base64)
KIALI_PASSPHRASE=$(read -sp 'Kiali Passphrase: ' pval && echo -n $pval | base64)

kubectl -n istio-system port-forward --address 0.0.0.0 $(kubectl -n istio-system get pod -l app=kiali -o jsonpath='{.items[0].metadata.name}') 20001:20001

Prometheus:
#http://<IP ADDRESS OF CLUSTER INGRESS>:15030/
#istioctl dashboard prometheus

kubectl -n istio-system port-forward --address 0.0.0.0 $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090

istio_requests_total
istio_requests_total{destination_service="productpage.default.svc.cluster.local"}
istio_requests_total{destination_service="reviews.default.svc.cluster.local", destination_version="v3"}
rate(istio_requests_total{destination_service=~"productpage.*", response_code="200"}[5m])

Grafana:
#http://<IP ADDRESS OF CLUSTER INGRESS>:15031/

kubectl -n istio-system port-forward --address 0.0.0.0 $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000

http://192.168.1.11:3000/dashboard/db/istio-mesh-dashboard
http://192.168.1.11:3000/dashboard/db/istio-service-dashboard
http://192.168.1.11:3000/dashboard/db/istio-workload-dashboard

kubectl -n istio-system delete gateway grafana-gateway kiali-gateway prometheus-gateway tracing-gateway
kubectl -n istio-system delete virtualservice grafana-vs kiali-vs prometheus-vs tracing-vs

Pod
Replica Set/Replication Controller
Deployment
Service
Horizontal Pod Autoscaler
StatefulSet
DaemonSet
Job
Cronjob
Volume
Persistent Volume
Persistent Volume Claim
StorageClass
VolumeSnapshotClass
Namespace
CustomResourceDefinition
Ingress
NetworkPolicy
Secret
LimitRange
ConfigMap
Node
Role
ClusterRole
RoleBinding
ClusterRoleBinding
ServiceAccount
ResourceQuota

CSI

VirtualService
DestinationRule
Gateway
ServiceEntry
Sidecar
EnvoyFilter

kubernetes CRD
istio ServiceEntry
用istio部署自己的应用

61.Go Module
export GO111MODULE=on
export GOPROXY=https://goproxy.io

go mod init packagename
go mod download
go mod tidy
go get packagename@v1.2.3
go get -u go.etcd.io/etcd/clientv3@v0.0.0-20190917205325-a14579fbfb1a
go mod vendor

go get go.etcd.io/etcd@v0.0.0-20210226220824-aa7126864d82
go.etcd.io/etcd v0.0.0-20210226220824-aa7126864d82 // indirect

go get go.etcd.io/etcd@aa7126864d82e88c477594b8a53f55f2e2408aa3
go.etcd.io/etcd v0.5.0-alpha.5.0.20210226220824-aa7126864d82 // indirect

go get go.etcd.io/etcd@latest

GO111MODULE=on go list -m -json -versions go.etcd.io/etcd@latest

Ceph:
object storage
block storage
file system
ceph-deploy
rados(Reliable Autonomic Distributed Object Store)
Monitor
OSD(Object Storage Device)
MDS(Ceph Metadata Server)
CRUSH(Controlled Replication Under Scalable Hashing)
RBD(RADOS block device)
RGW(RADOS gateway)
CephFS

Ceph Monitor, Ceph Manager, Ceph OSD (Object Storage Daemon), Ceph Metadata Server

集群部署:
mkdir -p /home/thomas/server/ceph-cluster/data
cd /home/thomas/server/ceph-cluster

sudo rm /usr/bin/python
sudo ln -s /usr/bin/python3.5 /usr/bin/python
ls -l /usr/bin/python*

sudo apt-get install python3-dev python3-pip
sudo pip3 install --upgrade pip
sudo pip install ceph-deploy

sudo chmod u+w /etc/sudoers
sudo vim /etc/sudoers
thomas  ALL=(root) NOPASSWD:ALL
sudo chmod u-w /etc/sudoers

ssh-keygen -t rsa
cat ./id_rsa.pub >> ./authorized_keys

scp /home/thomas/.ssh/id_rsa.pub thomas@node02:/home/thomas/.ssh/id_rsa.pub
scp /home/thomas/.ssh/authorized_keys thomas@node02:/home/thomas/.ssh/authorized_keys
scp /home/thomas/.ssh/id_rsa.pub thomas@node04:/home/thomas/.ssh/id_rsa.pub
scp /home/thomas/.ssh/authorized_keys thomas@node04:/home/thomas/.ssh/authorized_keys

ssh node01
ssh node02
ssh node04

wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
#echo deb https://download.ceph.com/debian-mimic/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
echo deb https://eu.ceph.com//debian-mimic/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt update
sudo apt-get install ceph ceph-osd ceph-mds ceph-mon radosgw
ll /usr/bin/ceph*

ceph-deploy --username thomas new node01
ceph-deploy --username thomas install node01
ceph-deploy --username thomas install node02
ceph-deploy --username thomas install node04

ceph-deploy mon create-initial
ceph-deploy admin node01 node02 node04
ceph-deploy mgr create node01

增加磁盘
sudo df -hl
sudo fdisk -l
sudo lsblk -a
sudo lsblk -f

sudo fdisk /dev/sdb
n
p
3
+10G
t
8e
p
w

sudo pvcreate /dev/sdb3
sudo pvdisplay
sudo pvs

sudo vgcreate vg-ceph /dev/sdb3
#sudo vgchange -a y vg-ceph
#sudo vgextend vg-ceph /dev/sdc
#sudo vgreduce vg-ceph /dev/sdc
sudo vgdisplay
sudo vgs

sudo lvcreate -L 8G -n lv-osd vg-ceph
sudo lvdisplay
sudo lvs
#sudo lvremove /dev/vg-ceph/lv-osd
#sudo lvextend -L+500M /dev/vg-ceph/lv-osd
#sudo resize4fs /dev/vg-ceph/lv-osd

sudo mkfs.ext4 /dev/vg-ceph/lv-osd
#sudo mkdir /lvmdata
#sudo mount /dev/vg-ceph/lv-osd /lvmdata

ceph-deploy osd create --data /dev/vg-ceph/lv-osd node01
ceph-deploy osd create --data /dev/vg-ceph/lv-osd node02
ceph-deploy osd create --data /dev/vg-ceph/lv-osd node04

ssh node02 sudo ceph health
ssh node02 sudo ceph -s
ps -ef | grep ceph
ceph-deploy mgr create node02 node04

增加mon节点:
vim /home/thomas/server/ceph-cluster/ceph.conf
public network = 192.168.1.0/24
ceph-deploy --overwrite-conf config push node01 node02 node04
#ceph-deploy --overwrite-conf admin node01 node02 node04
ceph-deploy mon add node02
ceph-deploy mon add node04
sudo ceph quorum_status --format json-pretty

RGW:
ceph-deploy rgw create node01
echo "hello ceph haha" > testfile.txt
sudo ceph osd pool create pool01 100
sudo rados put object01 testfile.txt --pool=pool01
sudo rados -p pool01 ls
sudo ceph osd map pool01 object01
sudo rados rm object01 --pool=pool01
sudo ceph osd pool rm pool01 pool01 --yes-i-really-really-mean-it

常用命令:
sudo rbd ls -p pool01
sudo rbd rm <image-name> -p pool01
sudo rbd create one.img -p pool01 -s <image-size>
sudo rbd create pool01/one.img --size <image-size>
sudo rbd map pool01/one.img[@<snap-name>]
sudo rbd unmap pool01/one.img[@<snap-name>]
sudo rbd resize pool01/one.img --size <image-size> --allow-shrink

sudo rados lspools
sudo rados rmpool pool01 pool01 --yes-i-really-really-mean-it
sudo rados purge pool01 --yes-i-really-really-mean-it
sudo rados df -p pool01
sudo rados ls -p pool01

sudo ceph osd pool ls detail
sudo ceph osd stat
sudo ceph osd dump
sudo ceph osd tree
sudo ceph osd down 0
sudo ceph osd up 0
sudo ceph osd out 0
sudo ceph osd in 0
sudo ceph osd rm 0
sudo ceph stop osd.0
sudo ceph osd crush rm osd.0
sudo ceph osd crush rm node01
sudo ceph osd getmaxosd
sudo ceph osd setmaxosd 60
sudo ceph osd crush reweight osd.3 1.0
sudo ceph osd reweight 3 0.5
sudo ceph osd pause
sudo ceph osd unpause

Block Device Storage:
sudo ceph osd crush show-tunables
sudo ceph osd crush tunables hammer
#sudo ceph osd crush tunables jewel
#sudo ceph osd crush tunables optimal
#sudo ceph osd crush tunables legacy
#sudo lsmod | grep ceph
#firefly giant hammer infernalis jewel kraken luminous mimic nautilus testing

Create Pool:
sudo ceph osd pool create pool02 100
sudo rbd pool init pool02
sudo rbd create foo --size 8 --image-feature layering -m 192.168.1.10 -k /etc/ceph/ceph.client.admin.keyring -p pool02
sudo rbd map pool02/foo --name client.admin
sudo mkfs.ext4 -m0 /dev/rbd/pool02/foo
sudo mkdir /mnt/ceph-block-device
sudo mount /dev/rbd/pool02/foo /mnt/ceph-block-device
cd /mnt/ceph-block-device

sudo rbd create pool02/xiaoming --size 32
sudo rbd -p pool02 ls
sudo rbd info pool02/xiaoming
sudo rbd resize pool02/xiaoming --size 16 --allow-shrink
sudo rbd feature disable pool02/xiaoming exclusive-lock object-map fast-diff deep-flatten
sudo rbd map pool02/xiaoming
sudo mkfs.ext4 /dev/rbd1
sudo mount /dev/rbd1 /home/thomas/server/ceph-cluster/data
sudo dd if=/dev/zero of=//home/thomas/server/ceph-cluster/data/testfile01.txt bs=1M count=1
sudo rbd snap create --snap mysnap pool02/xiaoming
sudo rbd snap ls pool02/xiaoming
sudo rm testfile01.txt
sudo umount /dev/rbd1
sudo rbd snap rollback pool02/xiaoming@mysnap
sudo mount /dev/rbd1 /home/thomas/server/ceph-cluster/data

Delete Pool:
vim /home/thomas/server/ceph-cluster/ceph.conf
mon_allow_pool_delete = true

ceph-deploy --overwrite-conf admin node01 node02 node04
cat /etc/ceph/ceph.conf

systemctl list-units --type=service | grep ceph
sudo systemctl restart ceph-mon@node01.service
sudo systemctl restart ceph-mon@node02.service
sudo systemctl restart ceph-mon@node04.service
sudo ceph osd pool rm pool02 pool02 --yes-i-really-really-mean-it

mon常用命令:
cd /var/log/ceph/
tail -f ceph-mon.node02.log
sudo ceph -s
sudo ceph mon stat
sudo ceph quorum_status
sudo ceph mon dump
sudo ceph mon remove mon.node01
sudo ceph mon add mon.node02 192.168.1.11:6789
sudo ceph mon add mon.node04 192.168.1.13:6789
sudo ceph mon getmap -o 1.txt
sudo monmaptool --print 1.txt
#sudo ceph-mon -i node02 -inject-monmap 1.txt
sudo ceph daemon mon.node01 mon_status
sudo ceph-conf --name mon.node01 --show-config-value admin_socket

CephFS:
sudo ceph-deploy mds create node01
sudo ceph osd pool create cephfs_data 32
sudo ceph osd pool create cephfs_meta 32
sudo ceph fs new mycephfs cephfs_meta cephfs_data
sudo ceph fs ls
sudo ceph mds stat

sudo mkdir /mnt/mycephfs
sudo ceph auth get-key client.admin
sudo mount -t ceph 192.168.1.10:6789:/ /mnt/mycephfs -o name=admin,secret=AQAoTrZdJkIYARAA4q9zXwfwCD3Ifv57NWAdaQ==
sudo df -hT
sudo umount /mnt/mycephfs

sudo mkdir /mnt/myfusecephfs
sudo ceph-fuse /mnt/myfusecephfs

#ssh user@ceph-server
#sudo useradd -d /home/{username} -m {username}
#sudo passwd {username}
#echo "{username} ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/{username}
#sudo chmod 0440 /etc/sudoers.d/{username}

#sudo pip uninstall ceph-deploy
#ceph-deploy purge {ceph-node} [{ceph-node}]
#ceph-deploy purgedata {ceph-node} [{ceph-node}]
#ceph-deploy forgetkeys
#rm ceph.*

#sudo df -hl
#sudo fdisk -l
#sudo lsblk -a
#sudo lsblk -f
#dd if=/dev/zero of=/home/thomas/server/ceph-cluster/data/loop0.img bs=1M count=10240
#sudo mkfs.ext4 /home/thomas/server/ceph-cluster/data/loop0.img
#file /home/thomas/server/ceph-cluster/data/loop0.img
#sudo losetup -f
#sudo losetup /dev/loop0 /home/thomas/server/ceph-cluster/data/loop0.img
#sudo losetup -a
#sudo mount /dev/loop0 /mnt/loopback
#sudo umount /mnt/loopback
#sudo losetup -d /dev/loop0


GlusterFS:
sudo add-apt-repository ppa:gluster/glusterfs-6
sudo apt-get update
sudo apt-get install glusterfs-server

ps -ef | grep gluster
systemctl list-units --type=service | grep gluster
sudo service glusterd start
sudo service glusterd stop

sudo gluster peer probe node01
sudo gluster peer probe node02
sudo gluster peer probe node04
sudo gluster peer status

sudo fdisk -l
sudo fdisk /dev/sdb
n
p
1
+5G
t
8e
p
w

sudo mkfs.xfs -i size=512 /dev/sdb1
mkdir -p /home/thomas/server/glusterfs/data/brick1
cd /home/thomas/server/glusterfs/data
sudo vim /etc/fstab
/dev/sdb1 /home/thomas/server/glusterfs/data/brick1 xfs defaults 1 2
sudo mount -a && sudo mount
sudo mkdir -p /home/thomas/server/glusterfs/data/brick1/gv0

sudo gluster volume create gv0 replica 3 node01:/home/thomas/server/glusterfs/data/brick1/gv0 node02:/home/thomas/server/glusterfs/data/brick1/gv0 node04:/home/thomas/server/glusterfs/data/brick1/gv0
sudo gluster volume start gv0
sudo gluster volume info

sudo mkdir /mnt/myglusterfs
sudo mount -t glusterfs node01:/gv0 /mnt/myglusterfs

sudo gluster pool list
sudo gluster peer detach server4

sudo gluster volume start mamm-volume
sudo gluster volume stop mamm-volume
sudo gluster volume delete mamm-volume
sudo gluster volume add-brick mamm-volume [strip|repli <count>] brick1...
sudo gluster volume remove-brick mamm-volume [repl <count>] brick1...

#迁移需要完成一系列的事务，假如我们准备将mamm卷中的brick3替换为brick5  
#启动迁移过程  
sudo gluster volume replace-brick mamm-volume node3:/exp3 node5:/exp5 start
#暂停迁移过程  
sudo gluster volume replace-brick mamm-volume node3:/exp3 node5:/exp5 pause
#中止迁移过程  
sudo gluster volume replace-brick mamm-volume node3:/exp3 node5:/exp5 abort
#查看迁移状态  
sudo gluster volume replace-brick mamm-volume node3:/exp3 node5:/exp5 status
#迁移完成后提交完成  
sudo gluster volume replace-brick mamm-volume node3:/exp3 node5:/exp5 commit

sudo gluster volume rebalane mamm-volume start|stop|status
sudo gluster volume heal mamm-volume #只修复有问题的文件  
sudo gluster volume heal mamm-volume full #修复所有文件  
sudo gluster volume heal mamm-volume info#查看自愈详情  
sudo gluster volume heal mamm-volume info healed|heal-failed|split-brain

sudo gluster volume statedump mamm-vol
sudo gluster volume set server.statedump-path /var/log/
sudo gluster volume info dumpfile
sudo gluster volume status [all|volname] [detail|clients|mem|fd|inode|callpoll]

sudo gluster volume top mamm-vol {open|read|write|opendir|readdir} brick node1:/exp1 list-cnt 1
sudo gluster volume top mamm-vol read-perf|write-perf bs 256 count 10 brick node1:/exp1 list-cnt 1

sudo gluster volume profile mamm-vol start
sudo gluster volume profile info
sudo gluster volume profile mamm-vol stop

#sudo apt-get remove glusterfs-server
#sudo apt-get autoremove glusterfs*
#sudo add-apt-repository -r ppa:gluster/glusterfs-6

Block卷 FUSE
卷管理器管理多个Brick Server
Volume:一组bricks的逻辑集合
BrickServer、TCP/IP 和 InfiniBand RDMA
1.DHT
2.AFR 创建volume时带replica
gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2
3.Striped 创建volume时带stripe数量 将文件切割成数据块
gluster volume create test-volume stripe 2 transport tcp server1:/exp1 server2:/exp2
4.分布式条带模式 是DHT与Striped的组合型
5.分布式复制模式 是DHT与AFR的组合型
6.条带复制卷模式 是Striped与AFR的组合型
7.三种模式混合

RDMA: Remote Direct Memory Access、Infiniband、RoCE、iWARP
iSCSI: Internet小型计算机系统接口，基于TCP/IP，用于通过IP网络仿真SCSI，为远程块存储设备提供数据传输和管理
NAS: Network Attached Storage 网络附属存储 网络存储器 openfiler FreeNAS
SAN: Storage Area Network 存储区域网络
RAID
IDE/SATA/M.2 FC CE SCSI SAS microSATA
AHCI/NVMe

Minio: object storage, storing unstructured data such as photos, videos, log files, backups and container/VM images, Sets Drives Buckets
Swift
S3
OpenZFS
MogileFS
FastDFS
SeaweedFS
HDFS
Lustre
MooseFS

NSQ:
tar -xzvf nsq-1.2.0.linux-amd64.go1.12.9.tar.gz
mkdir /home/thomas/server/nsq-cluster
cd /home/thomas/server/nsq-cluster
mkdir nsqd1 nsqd2 nsqd3
cd /home/thomas/server/nsq-1.2.0.linux-amd64.go1.12.9/bin

nohup ./nsqlookupd >>nsqlookupd.log 2>&1 &

nohup ./nsqd -lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4601 -http-address=0.0.0.0:4701 -data-path=/home/thomas/server/nsq-cluster/nsqd1 >>/home/thomas/server/nsq-cluster/nsqd1.log 2>&1 &
nohup ./nsqd -lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4602 -http-address=0.0.0.0:4702 -data-path=/home/thomas/server/nsq-cluster/nsqd2 >>/home/thomas/server/nsq-cluster/nsqd2.log 2>&1 &
nohup ./nsqd -lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4603 -http-address=0.0.0.0:4703 -data-path=/home/thomas/server/nsq-cluster/nsqd3 >>/home/thomas/server/nsq-cluster/nsqd3.log 2>&1 &

nohup ./nsqadmin --lookupd-http-address=127.0.0.1:4161 >>nsqadmin.log 2>&1 &
nohup ./nsq_to_file --topic=test --output-dir=/tmp --lookupd-http-address=127.0.0.1:4161 >>nsq_to_file.log 2>&1 &

curl -d 'hello world 1' 'http://127.0.0.1:4151/pub?topic=test'
http://192.168.1.7:4171/

curl -X POST http://127.0.0.1:4151/channel/create?topic=name&channel=name
curl http://127.0.0.1:4151/stats
curl http://127.0.0.1:4161/lookup
curl http://127.0.0.1:4161/topics
curl http://127.0.0.1:4161/channels
curl http://127.0.0.1:4161/nodes

TiDB:
git clone https://github.com/pingcap/tidb-docker-compose.git
cd tidb-docker-compose && docker-compose pull && docker-compose up -d
mysql -h 127.0.0.1 -P 4000 -u root
http://localhost:3000 admin admin
http://localhost:8010

docker-compose down

Ansible:
sudo apt update
sudo apt install software-properties-common
sudo apt-add-repository --yes --update ppa:ansible/ansible
sudo apt install ansible
sudo apt remove ansible
sudo rm -rf /etc/ansible

sudo pip install --upgrade pip
sudo pip install ansible
#sudo pip install paramiko

sudo mkdir /etc/ansible
cd /etc/ansible/
sudo vim /etc/ansible/ansible.cfg

sudo vim /etc/ansible/hosts
[antservers]
192.168.1.7
192.168.1.10
192.168.1.11

[remote]
192.168.1.11 ansible_ssh_user=root ansible_ssh_pass=180498 ansible_python_interpreter=/usr/bin/python2.7

sudo apt-get install openssh-server
ssh 192.168.1.10   #need password
exit
cd ~/.ssh/
ssh-keygen -t rsa
cat ./id_rsa.pub >> ./authorized_keys
拷贝authorized_keys到其它主机的~/.ssh/
ssh 192.168.1.10   #no need password

ansible all -m ping
ansible remote -m ping
ansible all -m ping -u thomas --ask-pass
ansible all -m ping -u thomas --become --ask-become-pass
ansible all -a "/bin/echo hello"
#ansible localhost -m ping -e 'ansible_python_interpreter="/usr/bin/env python"'

ansible -i ./hosts remote -b --become-user=root -m apt -a 'name=nginx state=installed update_cache=true'
ansible remote --become --become-user=root --ask-become-pass -m apt -a 'name=nginx state=installed update_cache=true'
ansible remote --become --become-user=root --ask-become-pass -m apt -a 'name=nginx state=absent'

Playbooks:
mkdir -p /home/thomas/server/ansible
cd /home/thomas/server/ansible
vim nginx.yml
- hosts: remote
  become: yes
  become_user: root
  tasks:
    - name: install nginx
      apt:
        name: nginx
        state: present
        update_cache: true

ansible-playbook nginx.yml --ask-become-pass

roles:
mkdir apple
cd apple/
ansible-galaxy init apple
ansible-playbook server.yml --ask-become-pass

sudo add-apt-repository ppa:nginx/stable
sudo add-apt-repository -r ppa:nginx/stable
cd /etc/apt/sources.list.d
sudo rm nginx*

sudo apt install nginx
sudo apt remove nginx
sudo apt autoremove nginx
ps -ef | grep nginx
sudo service nginx start
sudo service nginx stop

/usr/lib/systemd/system
/etc/systemd/system
/lib/systemd/system
/etc/init.d

sudo systemctl start nginx.service
sudo systemctl enable nginx.service
sudo systemctl disable nginx.service
sudo systemctl status nginx.service
sudo systemctl restart nginx.service
sudo systemctl stop nginx.service
sudo systemctl unmask nginx.service
sudo systemctl list-units --type=service

66.Serverless
fn:
mkdir -p /home/thomas/server/serverless
cd /home/thomas/server/serverless
#curl -LSs https://raw.githubusercontent.com/fnproject/cli/master/install | sh
下载https://github.com/fnproject/cli/releases/download/0.5.91/fn_linux
chmod +x fn

fn start
#export FN_API_URL=http://127.0.0.1:8081
#fn start -p 8081
fn version

fn list contexts
#fn update context api-url http://localhost:8080
fn use context default
fn update context registry fndemouser
#fn update context registry your-docker-hub-user-name

fn init --runtime go gofn
cd gofn
fn create app goapp
fn --verbose deploy --app goapp --local
fn invoke goapp gofn
echo -n '{"name":"Bob"}' | fn invoke goapp gofn --content-type application/json

fn list apps
fn list functions goapp
fn inspect function goapp gofn
curl -X "POST" -H "Content-Type: application/json" http://localhost:8080/invoke/01DRG45NKQNG8G00GZJ0000002
curl -X "POST" -H "Content-Type: application/json" -d '{"name":"Bob"}' http://localhost:8080/invoke/01DRG45NKQNG8G00GZJ0000002

# update
fn --verbose deploy --app goapp --local
fn stop

OpenFaaS:
mkdir -p /home/thomas/server/serverless/openfaas
cd /home/thomas/server/serverless/openfaas
#curl -sL https://cli.openfaas.com | sudo sh
手动下载https://github.com/openfaas/faas-cli/releases/download/0.10.3/faas-cli
chmod +x ./faas-cli
sudo mv ./faas-cli /usr/local/bin
sudo ln -sf /usr/local/bin/faas-cli /usr/local/bin/faas
echo 'source <(faas-cli completion --shell bash)' >>~/.bashrc

queue-worker,gateway,nats-streaming,alertmanager,prometheus,faas-swarm,basic-auth-plugin

git clone https://github.com/openfaas/faas
cd /home/thomas/golang/src/github.com/openfaas/faas
./deploy_stack.sh
username: admin 
password: 2bd4f14cb4cb80cd968e6aa4cc2cb853a6881054a23fa3192223a20cdcb5ee13
echo -n 2bd4f14cb4cb80cd968e6aa4cc2cb853a6881054a23fa3192223a20cdcb5ee13 | faas-cli login --username=admin --password-stdin

http://192.168.1.11:8080/

#faas-cli store list
#faas-cli store deploy figlet
#faas-cli list --verbose
#echo "OpenFaaS!" | faas-cli invoke figlet
#faas-cli template pull
#faas-cli template store list
#faas-cli new --list

mkdir -p /home/thomas/golang/src/serverless
cd /home/thomas/golang/src/serverless
faas-cli new gohash --lang go

#cd gohash
#go get -u github.com/golang/dep/cmd/dep
#dep init
#dep ensure -add github.com/cnf/structhash

vim gohash.yml
image: 192.168.1.13:5000/gohash:latest

faas-cli build -f ./gohash.yml
faas-cli push -f ./gohash.yml
faas-cli deploy -f ./gohash.yml
faas-cli list --verbose
faas-cli describe gohash

docker service scale gohash=2

echo -n "test" | faas-cli invoke gohash
docker exec -it go-fn01 sh

faas-cli remove -f ./gohash.yml
docker stack rm func
docker network rm func_functions
docker rmi gohash
docker secret rm basic-auth-password basic-auth-user

watchdog:
监视器提供了一个外部世界和函数之间的非托管的通用接口,它的工作是收集从API网关来的HTTP请求,然后调用程序

gateway:
Gateway作为一个入口,当CLI或者web页面发来要部署或者调用一个函数的时候,Gateway会将请求转发给Provider,同时会将监控指标发给Prometheus,AlterManager会根据需求,调用API自动伸缩函数

queue-worker:
异步函数,docker service logs -f func_queue-worker

NATS Streaming:
消息队列

prometheus && alertmanager && faas-swarm:
监控指标,自动扩容

67.Docker Swarm && Docker Machine && Docker Registry
stacks, services, and tasks

docker swarm init
docker node ls
#docker swarm join-token worker
docker swarm join --token SWMTKN-1-2162pmm7pee9rhqkuzff4v23bo03lnrtqqhrlsxsovqg7sv5sg-ehe51t3b371y7cxmnvdqogxog 192.168.1.7:2377
docker info

mkdir -p /home/thomas/server/swarm
cd /home/thomas/server/swarm

docker service create --replicas 1 --name helloworld alpine ping docker.com
docker service ls
docker service inspect --pretty helloworld
docker service ps helloworld
docker service scale helloworld=3
docker service rm helloworld

docker service create \
  --replicas 3 \
  --name redis \
  --update-delay 10s \
  redis:3.0.6
docker service inspect --pretty redis
docker service update --image redis:3.0.7 redis
docker service ps redis

docker node update --availability drain node01
docker node inspect --pretty node01
docker service ps redis
docker node update --availability active node01

docker stack deploy -c stack.yaml samle-stack
docker stack ls
docker stack services samle-stack
docker service ls
docker service ps samle-stack_whoami
docker service logs samle-stack_whoami
docker container rm -f samle-stack_whoami.2.f9l4aktganb8pku5k8iypaso3
docker stack rm samle-stack
docker service rm samle-stack_whoami

docker swarm leave --force

Docker Registry:
docker run -d -p 5000:5000 --restart=always --name registry registry:2
docker pull ubuntu:16.04
docker tag ubuntu:16.04 localhost:5000/my-ubuntu
docker push localhost:5000/my-ubuntu

docker image remove ubuntu:16.04
docker image remove localhost:5000/my-ubuntu

docker pull localhost:5000/my-ubuntu

docker container stop registry
docker container stop registry && docker container rm -v registry

#docker run -d \
  -p 5000:5000 \
  --restart=always \
  --name registry \
  -v /mnt/registry:/var/lib/registry \
  registry:2

curl -XGET http://127.0.0.1:5000/v2/_catalog
sudo vim /etc/docker/daemon.json
{
  "insecure-registries": [
    "192.168.1.13:5000",
  ],
  "registry-mirrors": [
    "https://docker.mirrors.ustc.edu.cn",
    "http://ovfftd6p.mirror.aliyuncs.com",
    "https://registry.docker-cn.com",
    "http://hub-mirror.c.163.com",
    "https://cr.console.aliyun.com"
  ]
}
sudo service docker restart

docker pull hub.c.163.com/library/ubuntu:latest

68.Docker网络
bridge,overlay,host,macvlan,none

ip link show docker0
ifconfig -a
sudo ifconfig br0 down
sudo brctl delbr br0
ip netns show

bridge:
同一主机上的docker容器相互通信
#docker network create \
  --driver=bridge \
  --subnet=172.28.0.0/16 \
  --ip-range=172.28.5.0/24 \
  --gateway=172.28.5.254 \
  br0

docker network create -d bridge my-net
docker network ls
brctl show
docker network inspect my-net

docker run -itd --name centos01 \
  --network my-net \
  centos:latest
docker run -itd --name centos02 \
  --network my-net \
  centos:latest

docker exec -it centos01 /bin/bash
docker exec -it centos02 /bin/bash
ping centos01
ping centos02

docker rm -f centos01 centos02
docker network rm my-net

host:
容器与主机在相同的网络命名空间下面
docker run -itd --name centos01 \
  --network host \
  centos:latest
docker run -itd --name centos02 \
  --network host \
  centos:latest

docker exec -it centos01 /bin/bash
docker exec -it centos02 /bin/bash
docker rm -f centos01 centos02

none:
docker run -itd --name centos01 \
  --network none \
  centos:latest
docker exec -it centos01 /bin/bash
docker run -itd --name centos02 \
  --network none \
  centos:latest
docker exec -it centos02 /bin/bash
ip addr
docker rm -f centos01 centos02

bridge default: 不能用名字访问
docker run -it --rm --name busybox01 busybox:latest sh
docker run -it --rm --name busybox02 busybox:latest sh

bridge private: 可以用名字访问
docker network create --driver bridge bridge02
docker run -it --rm --name busybox03 --network bridge02 busybox:latest sh
docker run -it --rm --name busybox04 --network bridge02 busybox:latest sh
sudo brctl show

host:
docker run -it --rm --name busybox05 --net=host busybox:latest sh
none:
docker run -it --rm --name busybox06 --net=none busybox:latest sh
container:
docker run -it --rm --name busybox07 --net=container:busybox03  busybox:latest sh

--link:
docker run -it --rm --name busybox08 --link busybox01 busybox:latest sh
docker run -it --rm --name busybox09 --network bridge02 --link busybox03 busybox:latest sh

docker network connect bridge02 busybox02
docker network inspect bridge02

-p 指定端口
-P 随机端口 要有EXPOSE

无EXPOSE:
docker build -t ants/pear:v2.0 .
docker run -d --name pear01 --net=host ants/pear:v2.0    能自动监听主机端口6666
docker run -d --name pear02 ants/pear:v2.0               不能
docker run -d --name pear03 -p 7777:6666 ants/pear:v2.0  能自动监听主机端口7777
docker run -d --name pear04 -P ants/pear:v2.0            不能

有EXPOSE:
docker run -d --name pear05 --net=host ants/pear:v1.0    能自动监听主机端口6666
docker run -d --name pear06 ants/pear:v1.0               不能
docker run -d --name pear07 -p 8888:6666 ants/pear:v1.0  能自动监听主机端口8888
docker run -d --name pear08 -P ants/pear:v1.0            监听主机随机端口

docker run -d --name pear09 -p 6666 ants/pear:v1.0       监听主机随机端口
docker run -d --name pear10 -p 6666 ants/pear:v2.0       监听主机随机端口

macvlan:
自定义容器的mac地址,将容器作为硬件设备,最佳实践是想将容器直接连接到物理网络时,可以考虑使用macvlan的方案
macvlan的缺点是需要将主机网卡(NIC)设置为混杂模式(Promiscuous Mode),这在大部分公有云平台上是不允许的

四种模式:
Private: 所有子设备不能通信
VEPA: 子设备虚拟局网内通信,hairpin
Bridge: 所有子设备可以通信,也可以与外部通信(docker默认)
Passthru: 只有一个设备

相似的方案:
ipvlan: 一个物理网卡所有子设备的mac地址与主机相同,ip地址与主机不同,L2,L3
macvtap: MACVTAP是对MACVLAN的改进,MACVLAN与TAP设备的特点综合一下,使用MACVLAN的方式收发数据包,但是收到的包不交给network stack处理,而是生成一个/dev/tapX文件,交给这个文件

开启混杂模式: sudo ifconfig eth0 promisc
关闭混杂模式: sudo ifconfig eth0 -promisc
sudo ip -d link show ens33 | grep promisc
sudo ip link set ens33 promisc on

Bridge模式:(公用一块物理网卡)
docker network create -d macvlan \
  --subnet=172.16.86.0/24 \
  --gateway=172.16.86.1 \
  -o parent=ens33 \
  my-macvlan-net

docker run --rm -dit \
  --network my-macvlan-net \
  --name alpine01 \
  --ip 172.16.86.5 \
  alpine:latest \
  ash

docker run --rm -dit \
  --network my-macvlan-net \
  --name alpine02 \
  --ip 172.16.86.6 \
  alpine:latest \
  ash

docker network ls
docker network inspect my-macvlan-net

docker exec alpine01 ping -c 5 172.16.86.6    #通
docker exec alpine02 ping -c 5 172.16.86.5    #通
docker exec alpine01 ping -c 5 192.168.1.12   #不通
docker exec alpine01 ping -c 5 192.168.1.11   #不通

docker exec alpine01 ip addr
docker exec alpine02 ip addr
docker exec alpine01 ip route
docker exec alpine02 ip route

docker run --rm -dit \
  --network my-macvlan-net \
  --name alpine03 \
  --ip 172.16.86.7 \
  alpine:latest \
  ash
docker exec alpine01 ping -c 5 172.16.86.7    #通
docker exec alpine02 ping -c 5 172.16.86.7    #通

docker container stop alpine01 alpine03
docker container stop alpine02
docker container stop alpine03
docker network rm my-macvlan-net

Bridge模式:(虚拟多块物理网卡)
sudo vconfig add ens33 100
sudo vconfig add ens33 200
sudo vconfig set_flag ens33.100 1 1
sudo vconfig set_flag ens33.200 1 1
sudo ifconfig ens33.100 up
sudo ifconfig ens33.200 up
docker network create -d macvlan --subnet=172.16.30.0/24 --gateway=172.16.30.1 -o parent=ens33.100 mac10
docker network create -d macvlan --subnet=172.16.40.0/24 --gateway=172.16.40.1 -o parent=ens33.200 mac20

node1:
docker run -itd --name d1 --ip=172.16.30.10 --network mac10 busybox
docker run -itd --name d2 --ip=172.16.40.10 --network mac20 busybox

node2:
docker run -itd --name d3 --ip=172.16.30.11 --network mac10 busybox
docker run -itd --name d4 --ip=172.16.40.11 --network mac20 busybox

docker exec d1 ping 172.16.30.10
docker exec d1 ping 172.16.40.10
docker exec d1 ping 172.16.30.11
docker exec d1 ping 172.16.40.11

node3:
#sysctl -w net.ipv4.ip_forward=1
cat /proc/sys/net/ipv4/ip_forward
sudo vconfig add ens33 100
sudo vconfig add ens33 200
sudo vconfig set_flag ens33.100 1 1
sudo vconfig set_flag ens33.200 1 1
sudo ifconfig ens33.100 172.16.30.1 netmask 255.255.255.0 up
sudo ifconfig ens33.200 172.16.40.1 netmask 255.255.255.0 up

sudo iptables -t nat -A POSTROUTING -o ens33.100 -j MASQUERADE
sudo iptables -t nat -A POSTROUTING -o ens33.200 -j MASQUERADE
sudo iptables -A FORWARD -i ens33.100 -o ens33.200 -m state --state RELATED,ESTABLISHED -j ACCEPT
sudo iptables -A FORWARD -i ens33.200 -o ens33.100 -m state --state RELATED,ESTABLISHED -j ACCEPT
sudo iptables -A FORWARD -i ens33.100 -o ens33.200 -j ACCEPT
sudo iptables -A FORWARD -i ens33.200 -o ens33.100 -j ACCEPT

docker exec d1 ping 172.16.40.10
docker exec d1 ping 172.16.40.11

docker exec d1 ip route
ip route

VEPA模式:(同主机不同容器不能通信,增加hairpin才能通讯)
docker network create -d macvlan \
  --subnet=172.16.86.0/24 \
  --gateway=172.16.86.1 \
  -o parent=ens33 \
  -o macvlan_mode=vepa \
  my-macvlan-vepa-net

docker run --rm -dit \
  --network my-macvlan-vepa-net \
  --name alpine01 \
  --ip 172.16.86.5 \
  alpine:latest \
  ash

docker run --rm -dit \
  --network my-macvlan-vepa-net \
  --name alpine02 \
  --ip 172.16.86.6 \
  alpine:latest \
  ash

docker network ls
docker network inspect my-macvlan-vepa-net

docker exec alpine01 ping -c 5 172.16.86.6   #通
docker exec alpine02 ping -c 5 172.16.86.5   #通
docker exec alpine01 ip addr
docker exec alpine02 ip addr
docker exec alpine01 ip route
docker exec alpine02 ip route

docker run --rm -dit \
  --network my-macvlan-vepa-net \
  --name alpine03 \
  --ip 172.16.86.7 \
  alpine:latest \
  ash
docker exec alpine01 ping -c 5 172.16.86.7    #不通
docker exec alpine02 ping -c 5 172.16.86.7    #通

docker container stop alpine01 alpine03
docker container stop alpine02
docker container stop alpine03
docker network rm my-macvlan-vepa-net

Private模式:(同主机不同容器不能通信,增加hairpin也不能通讯)
docker network create -d macvlan \
  --subnet=172.16.86.0/24 \
  --gateway=172.16.86.1 \
  -o parent=ens33 \
  -o macvlan_mode=private \
  my-macvlan-private-net

docker run --rm -dit \
  --network my-macvlan-private-net \
  --name alpine01 \
  --ip 172.16.86.5 \
  alpine:latest \
  ash

docker run --rm -dit \
  --network my-macvlan-private-net \
  --name alpine02 \
  --ip 172.16.86.6 \
  alpine:latest \
  ash

docker network ls
docker network inspect my-macvlan-private-net

docker exec alpine01 ping -c 5 172.16.86.6   #通
docker exec alpine02 ping -c 5 172.16.86.5   #通

docker run --rm -dit \
  --network my-macvlan-private-net \
  --name alpine03 \
  --ip 172.16.86.7 \
  alpine:latest \
  ash
docker exec alpine01 ping -c 5 172.16.86.7    #不通
docker exec alpine02 ping -c 5 172.16.86.7    #通

docker container stop alpine01 alpine03
docker container stop alpine02
docker network rm my-macvlan-private-net

Passthru模式:(虚拟mac只能有一个,虚拟mac地址和主机mac地址一样)
docker network create -d macvlan \
  --subnet=172.16.86.0/24 \
  --gateway=172.16.86.1 \
  -o parent=ens33 \
  -o macvlan_mode=passthru \
  my-macvlan-passthru-net

docker run --rm -dit \
  --network my-macvlan-passthru-net \
  --name alpine01 \
  --ip 172.16.86.5 \
  alpine:latest \
  ash

docker run --rm -dit \
  --network my-macvlan-passthru-net \
  --name alpine02 \
  --ip 172.16.86.6 \
  alpine:latest \
  ash

docker network ls
docker network inspect my-macvlan-passthru-net

docker exec alpine01 ping -c 5 172.16.86.6   #通
docker exec alpine02 ping -c 5 172.16.86.5   #通

docker run --rm -dit \
  --network my-macvlan-passthru-net \
  --name alpine03 \
  --ip 172.16.86.7 \
  alpine:latest \
  ash       #创建失败

docker container stop alpine01
docker container stop alpine02
docker network rm my-macvlan-passthru-net

overlay:
使用VXLAN协议,VXLAN可以封装L2/L3协议数据,是一种隧道协议,用于不同容器在多个host上进行通信

不使用swarm:
docker network create -d bridge --subnet 172.18.0.0/16 my-overlay-net
#docker network create -d overlay --subnet 172.18.20.0/16 my-overlay-net2  #只能swarm用

docker run --rm -dit \
  --network my-overlay-net \
  --name centos01 \
  --ip 172.18.0.2 \
  centos:latest

docker run --rm -dit \
  --network my-overlay-net \
  --name centos02 \
  --ip 172.18.0.3 \
  centos:latest

docker exec centos01 ip addr
docker exec centos02 ip addr

docker exec centos01 ping 172.18.0.3     #不通
docker exec centos01 ping 192.168.1.11   #通
docker exec centos01 ping 192.168.1.12   #通
docker exec -it centos01 /bin/bash

brctl show
sudo ip link add vxlan_docker type vxlan id 200 remote 192.168.1.12 dstport 4789 dev ens33
sudo ip link set vxlan_docker up
ifconfig
sudo brctl addif br-07492132d7b3 vxlan_docker

sudo ip link add vxlan_docker type vxlan id 200 remote 192.168.1.11 dstport 4789 dev ens33
sudo ip link set vxlan_docker up
ifconfig
sudo brctl addif br-372b985ef56a vxlan_docker

docker exec centos01 ping 172.18.0.3    #通

docker stop centos01
docker stop centos02
docker network rm my-overlay-net
sudo ip link delete vxlan_docker

#yum whatprovides ifconfig
#net-tools-2.0-0.51.20160912git.el8.x86_64
#yum install net-tools -y
#ifconfig eth0

使用swarm:
docker swarm init
docker swarm join --token SWMTKN-1-2162pmm7pee9rhqkuzff4v23bo03lnrtqqhrlsxsovqg7sv5sg-ehe51t3b371y7cxmnvdqogxog 192.168.1.7:2377
docker node ls

docker network create -d overlay --subnet 172.18.23.0/16 my-overlay-net2

docker service create --name ubuntu-srv \
--network my-overlay-net2 \
--replicas 2 \
ubuntu sleep infinity

docker service ls
docker service ps ubuntu-srv
docker network inspect my-overlay-net2

docker exec -it ubuntu-srv.1.tmcljalyz3a7r2x79nu6vb27m /bin/bash
docker exec -it ubuntu-srv.2.q2aaudoef8f6iegp6t24g0sd2 /bin/bash

apt-get update
apt-get install net-tools iputils-ping iproute2
ifconfig
ping 172.18.0.3
ping 172.18.0.4
ping 192.168.1.11
ping 192.168.1.12

docker service rm ubuntu-srv
docker network rm my-overlay-net2

unshare:
sudo unshare --fork --pid --mount-proc bash

ipvlan:
l3:
sudo ip link add link ens33 ipvlan1 type ipvlan mode l3
sudo ip link add link ens33 ipvlan2 type ipvlan mode l3
sudo ip netns add ns1
sudo ip netns add ns2
sudo ip link set ipvlan1 netns ns1
sudo ip link set ipvlan2 netns ns2
sudo ip netns exec ns1 ip addr add 10.0.2.18/24 dev ipvlan1
sudo ip netns exec ns2 ip addr add 10.0.3.19/24 dev ipvlan2
sudo ip netns exec ns1 ip link set ipvlan1 up
sudo ip netns exec ns2 ip link set ipvlan2 up
sudo ip netns exec ns1 route add default dev ipvlan1
sudo ip netns exec ns2 route add default dev ipvlan2

sudo ip netns exec ns1 ping 10.0.3.19      #通
sudo ip netns exec ns2 ping 10.0.2.18      #通
sudo ip netns exec ns1 ping 192.168.1.11   #不通
sudo ip netns exec ns1 ping 192.168.1.12   #不通

sudo ip netns exec ns1 tcpdump -ni ipvlan1 -p arp  #无数据

l2:
sudo ip link add link ens33 ipvlan3 type ipvlan mode l2
sudo ip link add link ens33 ipvlan4 type ipvlan mode l2
sudo ip netns add ns3
sudo ip netns add ns4
sudo ip link set ipvlan3 netns ns3
sudo ip link set ipvlan4 netns ns4
sudo ip netns exec ns3 ip addr add 10.0.4.20/24 dev ipvlan3
sudo ip netns exec ns4 ip addr add 10.0.5.21/24 dev ipvlan4
sudo ip netns exec ns3 ip link set ipvlan3 up
sudo ip netns exec ns4 ip link set ipvlan4 up
sudo ip netns exec ns3 route add default dev ipvlan3
sudo ip netns exec ns4 route add default dev ipvlan4

sudo ip netns exec ns3 ping 10.0.5.21      #通
sudo ip netns exec ns4 ping 10.0.4.20      #通
sudo ip netns exec ns3 ping 192.168.1.11   #不通
sudo ip netns exec ns3 ping 192.168.1.12   #不通

sudo ip netns exec ns3 tcpdump -ni ipvlan3 -p arp  #有数据

macvtap:
"private", "vepa", "bridge" or "passthru"
sudo ip link add link ens33 macvtap1 type macvtap mode bridge
sudo ip link add link ens33 macvtap2 type macvtap mode bridge
sudo ip netns add ns5
sudo ip netns add ns6
sudo ip link set macvtap1 netns ns5
sudo ip link set macvtap2 netns ns6
sudo ip netns exec ns5 ip addr add 10.0.6.22/24 dev macvtap1
sudo ip netns exec ns6 ip addr add 10.0.7.23/24 dev macvtap2
sudo ip netns exec ns5 ip link set macvtap1 up
sudo ip netns exec ns6 ip link set macvtap2 up
sudo ip netns exec ns5 route add default dev macvtap1
sudo ip netns exec ns6 route add default dev macvtap2

sudo ip netns exec ns5 ping 10.0.7.23      #通
sudo ip netns exec ns6 ping 10.0.6.22      #通
sudo ip netns exec ns5 ping 192.168.1.11   #不通
sudo ip netns exec ns5 ping 192.168.1.12   #不通

Kubernetes网络 && Linux网络:
Etcd:
sudo su
cd /home/thomas/server/etcd-v3.3.13-linux-amd64
export ETCDCTL_API=3
./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key get / --prefix --keys-only | grep -Ev "^$" | grep "flannel"
./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key get /registry/configmaps/kube-flannel/kube-flannel-cfg -w=json | jq .
./etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key member list

Open vSwitch: 虚拟交换机,支持OpenFlow协议,支持gre/vxlan/IPsec等隧道技术,ovs-vswitchd,ovsdb-server,kernel datapath,OpenFlow控制器
iptables: 
GRE: 

tap/tun: 虚拟网卡,字符设备驱动和网卡驱动,/dev/tap*,/dev/net/tun,用户空间通过tap/tun与内核交互,用于隧道通信VPN/tunnel/IPsec/openvpn/VTun,tap是二层设备处理以太网帧,tun是三层设备处理IP数据包

sudo ip tunnel help
sudo modinfo tun
sudo lsmod | grep tun
sudo lsmod | grep ip
sudo modprobe tun
sudo modprobe ipip
sudo mknod /dev/net/tun c 10 200
sudo ip addr show

CNM(Container Network Model):
network/sandbox/endpoint
CNI(Container Networking Interface):
ADD/DELETE

kubernetes网络: 
Flannel,Calico,Weave,Canal,Contiv,Cilium,Romana,kube-router,cni-genie,Pipework; Kuryr,Open vSwitch
Ingress: ingress-nginx,Traefik,Kong,kubernetes-ingress,Gloo,Ambassador

云原生: 
Jaeger,Fluentd,Containerd,CoreDNS,Envoy,Prometheus,OpenTracing,CNI,vitess,rook,NATS,Linkerd,Notary,Harbor,Helm,Etcd,OpenPolicyAgent,CRI-O,TiKV,rkt

Flannel:
L3 Overlay模式设计,etcd用于存储整个集群的网络配置,flanneld,flannel0网桥,docker0,UDP,subnet
kubectl run nginx --image=nginx --replicas=2
kubectl get configmaps -n kube-flannel -o yaml kube-flannel-cfg

UDP:
Type: udp
Port: 8285
用于调试或不支持vxlan的内核环境下
两次用户态内核态切换,flannel0是TUN设备
容器A -- cni0 -- flannel0 -- flanneld:8285 -- eth0
eth0 -- flanneld:8285 -- flannel0 -- cni0 -- 容器B

VxLan:
Type: vxlan
VNI: 1
Port: 8472
GBP: false
DirectRouting: false #直接路由,当host在同一个subnet下,windows不支持
一次用户态内核态切换,udp 8472
sudo route -n
sudo arp -n
sudo bridge fdb show dev flannel.1
sudo netstat -anp | grep 8472
ip -d link show flannel.1
ip neigh show dev flannel.1
容器A -- cni0 -- flannel.1 -- ens33
ens33 -- flannel.1 -- cni0 -- 容器B
sudo tcpdump -i veth198a61d0 -p icmp -nn
sudo tcpdump -i cni0 -p icmp -nn
sudo tcpdump -i flannel.1 -p icmp -nn
sudo tcpdump -i ens33 -p udp -nn

host-gw:
Type: host-gw
性能最好,但是不能用于多云环境下
只能局域网内部同一个网段,flanneld负责主机上路由表的动态更新,路由表更新压力大
sudo brctl show
sudo ip route show
sudo route -n
容器A -- cni0 -- ens33
ens33 -- cni0 -- 容器B
sudo tcpdump -i veth8da8c1c0 -p icmp -nn
sudo tcpdump -i cni0 -p icmp -nn
sudo tcpdump -i ens33 -p icmp -nn

DirectRouting:
DirectRouting: true
容器A -- cni0 -- ens33
ens33 -- cni0 -- 容器B
sudo tcpdump -i veth198a61d0 -p icmp -nn
sudo tcpdump -i cni0 -p icmp -nn
#sudo tcpdump -i flannel.1 -p icmp -nn
sudo tcpdump -i ens33 -p icmp -nn

Calico:
纯三层,数据包都是通过路由的形式找到对应的主机和容器的,通过BGP 协议来将所有路由同步到所有的机器或数据中心,从而完成整个网络的互联。
veth pair,其中一端在主机上,另一端在容器的网络命名空间里
使用虚拟路由代替虚拟交换
BGP(Border Gateway Protocol),边界网关协议,在大规模网络中实现节点路由信息共享的一种协议,每一台虚拟路由通过BGP协议传播可达信息,BGP协议,就是大规模网络中,共享节点路由信息的协议
没有使用CNI的网桥模式,calico的CNI插件还需要在host机器上为每个容器的veth pair配置一条路由规则,cni插件是calico与kubernetes对接部分

Felix,BGP Client(BIRD),BGP Route Reflector
容器A -- cali0000 -- routes -- eth0 
eth0 -- routes -- cali1111 -- 容器B

网段不一样时:
IPIP:
容器A -- cali0000 -- tunl0 -- eth0 -- Router1
Router2 -- eth0 -- tunl0 -- cali1111 -- 容器B

将宿主机网关也加入到BGP Mesh里:
第一种方案就是所有宿主机都跟宿主机网关建立BGP Peer关系,Dynamic Neighbors的BGP配置方式
第二种方案是使用一个或多个独立组件负责搜集整个集群里的所有路由信息，然后通过BGP协议同步给网关

sudo ifconfig lxcbr0 down
sudo brctl delbr lxcbr0

kubectl delete -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

mkdir -p /home/thomas/server/calico
cd /home/thomas/server/calico
curl https://docs.projectcalico.org/v3.10/manifests/calico.yaml -O
curl https://docs.projectcalico.org/manifests/calico.yaml -O
POD_CIDR="10.244.0.0/16"
sed -i -e "s?192.168.0.0/16?$POD_CIDR?g" calico.yaml
kubectl apply -f calico.yaml

kubectl get pods --all-namespaces -o wide
kubectl get deployments.apps --all-namespaces -o wide
kubectl delete deployments.apps nginx

sudo ip -d link show tunl0
kubectl run nginx-deploy --image=nginx --replicas=2
kubectl exec -it nginx-deploy-dc9ff4b8c-cxp6c -- bash
kubectl exec -it nginx-deploy-dc9ff4b8c-zwcbl -- bash
apt-get update && apt-get install net-tools iputils-ping iproute2

ip route
ip neigh
当ARP请求目标跨网段时,网关设备收到此ARP请求,会用自己的MAC地址返回给请求者,这便是代理ARP(Proxy ARP)
cat /proc/sys/net/ipv4/conf/calib00075ae01c/proxy_arp
sudo tcpdump -i calib00075ae01c -e -nn

kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/calicoctl.yaml
kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide
kubectl exec -ti -n kube-system calicoctl -- /calicoctl get nodes

systemctl:
sudo /etc/init.d/mysql status
sudo /etc/init.d/mysql start
sudo /etc/init.d/mysql stop

sudo systemctl status mysql.service
sudo systemctl start mysql.service
sudo systemctl stop mysql.service

sudo service mysql status
sudo service mysql start
sudo service mysql stop

sudo vim /etc/rc.local
+/etc/init.d/mysql start

sudo mv startBlog /etc/init.d/
sudo update-rc.d startBlog defaults 99
sudo update-rc.d -f startBlog remove

cd /etc/systemd/system
cd /lib/systemd/system
cd /usr/lib/systemd/system

# 开机启动 && 开机不启动
sudo systemctl enable mysql.service
sudo systemctl disable mysql.service

systemd-analyze blame

sudo systemctl show mysql.service
multi-user.target.wants

sudo systemctl list-units -all
sudo systemctl --failed

grpcurl:
grpcurl -plaintext -d '{"phoneNumber":"12465465461", "name":"xiaoming"}' 127.0.0.1:6068 user.UserService/AddUser
grpcurl -plaintext -d '{"userID":"fsadfgsdfsadf"}' 127.0.0.1:6068 user.UserService/DelUser
grpcurl -plaintext -d '{"userID":"3258a18a000147adb3c47d88fce9115a"}' 127.0.0.1:6068 user.UserService/GetUser
grpcurl -plaintext -d '{"userID":"18931f8fd0134a35bf7fcd2137254fca", "userInfo":{"name":"xiaowang"}}' 127.0.0.1:6068 user.UserService/ModifyUserInfo
grpcurl -plaintext -d '{"phoneNumber":"12465465461"}' 127.0.0.1:6068 user.UserService/GetUserIdByPhoneNumber
grpcurl -plaintext -d '{"name":"xiaoming"}' 127.0.0.1:6068 user.UserService/GetUsersByName

memcached:
tar -xzvf memcached-1.6.6.tar.gz
cd memcached-1.6.6
sudo apt-get install libevent libevent-dev
./configure && make && make test && sudo make install
#sudo apt-get install memcached

./memcached -p 11211 -m 64m -vvv
./memcached -p 11211 -m 64m -d

telnet 192.168.197.128 11211
set key01 0 900 7
value01
get key01

NFS:
sudo apt-get install nfs-kernel-server
sudo apt-get install nfs-common
sudo vim /etc/exports
/home/thomas/nfsroot *(rw,sync,no_root_squash)

sudo /etc/init.d/nfs-kernel-server start
sudo /etc/init.d/nfs-kernel-server restart

sudo mount -t nfs 192.168.1.12:/home/thomas/nfsroot /mnt -o nolock
sudo umount /mnt

sudo exportfs -arv
sudo showmount -e
nfsstat -s

strace:
strace -o aaa.txt -T -tt -e trace=all -p 1111

Debian:
su
cat << EOF >/etc/apt/sources.list
deb http://mirrors.aliyun.com/debian/ bullseye main non-free contrib
deb-src http://mirrors.aliyun.com/debian/ bullseye main non-free contrib
deb http://mirrors.aliyun.com/debian-security/ bullseye-security main
deb-src http://mirrors.aliyun.com/debian-security/ bullseye-security main
deb http://mirrors.aliyun.com/debian/ bullseye-updates main non-free contrib
deb-src http://mirrors.aliyun.com/debian/ bullseye-updates main non-free contrib
deb http://mirrors.aliyun.com/debian/ bullseye-backports main non-free contrib
deb-src http://mirrors.aliyun.com/debian/ bullseye-backports main non-free contrib
EOF
apt update

sudo vim /etc/apt/sources.list
deb https://mirrors.163.com/debian/ bullseye main non-free contrib
deb-src https://mirrors.163.com/debian/ bullseye main non-free contrib
deb https://mirrors.163.com/debian-security/ bullseye-security main
deb-src https://mirrors.163.com/debian-security/ bullseye-security main
deb https://mirrors.163.com/debian/ bullseye-updates main non-free contrib
deb-src https://mirrors.163.com/debian/ bullseye-updates main non-free contrib
deb https://mirrors.163.com/debian/ bullseye-backports main non-free contrib
deb-src https://mirrors.163.com/debian/ bullseye-backports main non-free contrib

sudo vim /etc/apt/sources.list
deb http://mirrors.ustc.edu.cn/debian/ bullseye main non-free contrib
deb-src http://mirrors.ustc.edu.cn/debian/ bullseye main non-free contrib
deb http://mirrors.ustc.edu.cn/debian-security/ bullseye-security main
deb-src http://mirrors.ustc.edu.cn/debian-security/ bullseye-security main
deb http://mirrors.ustc.edu.cn/debian/ bullseye-updates main non-free contrib
deb-src http://mirrors.ustc.edu.cn/debian/ bullseye-updates main non-free contrib
deb http://mirrors.ustc.edu.cn/debian/ bullseye-backports main non-free contrib
deb-src http://mirrors.ustc.edu.cn/debian/ bullseye-backports main non-free contrib

apt install vim sudo
vim ~/.bashrc
export PATH=$PATH:/sbin
exit
su
visudo
thomas  ALL=(root) NOPASSWD:ALL
vim /etc/network/interfaces
auto ens33
exit

sudo ldconfig
ldd --version
strings /lib/x86_64-linux-gnu/libm.so.6 | grep GLIBC_
strings /lib/x86_64-linux-gnu/libc.so.6 | grep GLIBC_

sudo vim /etc/apt/sources.list
deb http://mirrors.163.com/debian/ testing contrib main non-free
deb-src http://mirrors.163.com/debian/ testing contrib main non-free
sudo apt-get -t testing install libc6-dev

deb http://ftp.us.debian.org/debian testing main
sudo apt-get install libc6

deb http://deb.debian.org/debian testing main
deb-src http://deb.debian.org/debian testing main

deb http://ftp.debian.org/debian sid main
sudo apt-get -t sid install libc6 libc6-dev libc6-dbg

自己的编译容易造成系统无法启动
wget http://ftp.gnu.org/pub/gnu/glibc/glibc-2.34.tar.bz2
tar -jxvf glibc-2.34.tar.bz2
mkdir build
cd build
sudo apt install gawk bison
#../configure --prefix=/usr/local --disable-sanity-checks
../configure --prefix=/usr
make -j4
sudo make install
sudo cp /usr/local/lib/libm.so.6 /lib/x86_64-linux-gnu/libm-2.34.so
sudo ln -sf libm-2.34.so libm.so.6
ldd --version
#sudo ln -sf libm-2.31.so libm.so.6
sudo cp /usr/local/lib/libc.so.6 /lib/x86_64-linux-gnu/libc-2.34.so
sudo ln -sf libc-2.34.so libc.so.6
#sudo ln -sf libc-2.31.so libc.so.6

NTP:
本地时间带时区/系统时间
UTC时间/系统时间
RTC硬件时间

timesyncd:
timesyncd替代了ntpd的客户端的部分,不同的是timesyncd一检查到时间改变就会同步
/lib/systemd/system/systemd-timesyncd.service
sudo apt install systemd-timesyncd
sudo systemctl restart systemd-timesyncd.service
sudo systemctl stop systemd-timesyncd.service
systemctl status systemd-timesyncd.service
sudo vim /etc/systemd/timesyncd.conf
[Time]
NTP=paper.com

timedatectl:
timedatectl由systemd-timedated服务实现,和timesyncd冲突,不和ntpd冲突,可以设置RTC时间
sudo timedatectl set-ntp true
sudo timedatectl set-ntp false
timedatectl timesync-status
timedatectl show-timesync
timedatectl status
timedatectl show
sudo timedatectl set-time "2020-04-29 11:30:00"
timedatectl list-timezones
sudo timedatectl set-timezone "Asia/Shanghai"
sudo timedatectl set-local-rtc true
sudo timedatectl set-local-rtc 0

date "+%Y-%m-%d %H:%M:%S"
sudo date -s "20150101 8:30:00"  # 不会设置RTC

RTC:
实时时钟 real-time clock
hwclock --localtime
hwclock --utc
hwclock -r
sudo hwclock -s # set the system time from the RTC
sudo hwclock -w # set the RTC from the system time
sudo hwclock --set --date "20140225 20:23:00"

SNTP/Simple NTP
ntpdate:不设置RTC时间,ntpdate和ntpd是互斥的,两者不能同时使用,RFC5905
ntpd:port UDP 123,RFC2030,会设置RTC时间
sudo apt-get install ntp ntpdate ntpstat
service --status-all
/etc/systemd/system/ntp.service
/usr/lib/systemd/system/ntp.service
systemctl status ntp.service
sudo systemctl unmask ntp.service
sudo systemctl mask ntp.service
sudo systemctl enable ntp.service
sudo systemctl restart ntp.service
sudo systemctl stop ntp.service
sudo netstat -lunp | grep 123
ntpq -p 127.0.0.1
ntpstat
ntpdate -q 127.0.0.1
ntpdate -q 192.168.45.146
ntpdate -q paper.com
ntpq -p 192.168.0.115
ntpdate -q 192.168.0.115
ntpdate -q 192.168.44.126
sudo ntpdate 192.168.0.115
sudo ntpdate paper.com
sudo ntpdate ntp.aliyun.com

sntp --version

sudo vim /etc/ntp.conf
driftfile /var/lib/ntp/ntp.drift   # 该选项指定了NTP服务的时钟漂移文件的路径
leapfile /usr/share/zoneinfo/leap-seconds.list # 闰秒信息

statistics loopstats peerstats clockstats # 统计信息
filegen loopstats file loopstats type day enable
filegen peerstats file peerstats type day enable
filegen clockstats file clockstats type day enable

server ntp.your-provider.example

pool 0.debian.pool.ntp.org iburst
pool 1.debian.pool.ntp.org iburst
pool 2.debian.pool.ntp.org iburst
pool 3.debian.pool.ntp.org iburst

restrict -4 default kod notrap nomodify nopeer noquery limited
restrict -6 default kod notrap nomodify nopeer noquery limited
restrict 127.0.0.1   # 本地主机可以访问
restrict ::1         # 本地主机可以访问
restrict 0.0.0.0 mask 255.255.255.0 nomodify
restrict source notrap nomodify noquery

broadcast 192.168.123.255

server 127.127.1.0 # 把本地当成时间来源
fudge 127.127.1.0 stratum 10  # 0-15层

telnet 192.168.45.146 8080
nc -vz 192.168.45.146 8080
nc -vuz 192.168.45.146 123

sqlite:
sqlite3 test.db
.open test.db
.databases
.tables
.schema tablename

sqlite3 testDB.db .dump > testDB.sql
sqlite3 testDB.db < testDB.sql

.header on
.mode column

SELECT tbl_name FROM sqlite_master WHERE type = 'table';
SELECT sql FROM sqlite_master WHERE type = 'table' AND tbl_name = 'COMPANY';

DROP INDEX index_name;

CREATE INDEX key_create_time ON tablename(create_time);
CREATE UNIQUE INDEX uni_phone ON tablename(phone);
CREATE INDEX key_create_time ON tablename(column1, column2);

SELECT * FROM sqlite_master;
SELECT * FROM sqlite_master WHERE type = 'index';

文件系统:
setfattr -n user.desc -v "test only file" a.txt
setfattr -n user.blue -v "test only file" a.txt
attr -lq a.txt
getfattr -n user.desc a.txt
getfattr -d -m ".*" a.txt
lsattr
chattr
setfacl

DBUS:
https://github.com/godbus/dbus
系统总线:由操作系统和后台进程使用
会话总线
匹配器
对象路径
Object
接口:Interface,多个方法和信号的集合
信号:signal,广播,一对多通信
DBUS_SESSION_BUS_ADDRESS
DBUS_SYSTEM_BUS_ADDRESS
unix socket:
listen会自动生成socket文件,如果在listen之前就存在socket文件,listen会失败
unix:abstract=/tmp/dbus-a1DonZq4BQ,guid=a655273d9fb07ec7a3b36c6264006361
unix:path=/run/user/1000/bus

sudo apt install dbus dbus-x11

destination
path object
iface
method
dbus-daemon
dbus-send
dbus-monitor
dbus-launch: 启动新的dbus-daemon
dbus-cleanup-sockets
dbus-uuidgen
dbus-run-session
dbus-daemon --syslog --fork --print-pid 4 --print-address 6 --session
DBUS_VERBOSE=1 dbus-daemon --session --print-address --address=unix:path=/tmp/xiaoming
unix:path=/run/user/1000/bus
dbus编译安装
启动dbus-daemon
dbus性能瓶颈
进程与内核的通信

rsyslog:
rsyslogd
数据来源:imfile/imtcp/imudp/imuxsock
数据发送:udp/tcp/omuxsock/omfile/ommysql/ompgsql
facility
proirity
module(load="imuxsock" SysSock.Name="/var/sock")
$ModLoad omuxsock
$OMUxSockSocket /tmp/socksample
*.* :omuxsock:

sudo service rsyslog restart
sudo systemctl restart rsyslog.service

logger -u /var/sock ysdahdahjs

bash:
/bin/bash -c
如果存在-c选项,则命令将从第一个非选项参数命令字符串中读取,如果在命令字符串后有参数,则第一个参数被分配给$0,任何剩余的参数被分配给位置参数,对$0的赋值设置了shell的名称,该名称用于警告和错误消息中
/bin/bash -c 后面接命令
/bin/bash 后面接执行的脚本
/bin/bash -c ls
/bin/bash ls
sudo echo "kettle" >> nohup.log  #sudo不能给>>权限
sudo /bin/bash -c 'echo "kettle" >> nohup.log'

netbios:
NetBIOS Name Service,NetBIOS名称服务
它是用于Windows网络中计算机名解析的服务,它允许计算机通过名称而不是IP地址进行通信,当计算机需要与其他计算机通信时,它将查询NetBIOS名称服务以获取目标计算机的IP地址
UDP 137/138端口


Kerberos:
sudo apt install krb5-kdc krb5-admin-server

sudo vim /etc/hosts
192.168.45.20 paper.com

sudo mkdir /var/log/kerberos
sudo touch /var/log/kerberos/{krb5kdc,kadmin,krb5lib}.log
sudo chmod -R 750 /var/log/kerberos

sudo cp /etc/krb5.conf /etc/krb5.conf.bk
sudo vim /etc/krb5.conf
[logging]
    kdc = FILE:/var/log/kerberos/krb5kdc.log
    admin_server = FILE:/var/log/kerberos/kadmin.log
    default = FILE:/var/log/kerberos/krb5lib.log

[libdefaults]
    default_realm = PAPER.COM

[realms]
    PAPER.COM = {
        kdc = paper.com
        admin_server = paper.com
    }

[domain_realm]
    .paper.com = PAPER.COM
    paper.com = PAPER.COM

sudo cp /etc/krb5kdc/kdc.conf /etc/krb5kdc/kdc.conf.bk
sudo vim /etc/krb5kdc/kdc.conf
[kdcdefaults]
    kdc_ports = 750,88

[realms]
    PAPER.COM = {
        database_name = /var/lib/krb5kdc/principal
        admin_keytab = FILE:/etc/krb5kdc/kadm5.keytab
        acl_file = /etc/krb5kdc/kadm5.acl
        key_stash_file = /etc/krb5kdc/stash
        kdc_ports = 750,88
        max_life = 10h 0m 0s
        max_renewable_life = 7d 0h 0m 0s
        master_key_type = des3-hmac-sha1
        #supported_enctypes = aes256-cts:normal aes128-cts:normal
        default_principal_flags = +preauth
    }

sudo cp /etc/krb5kdc/kadm5.acl /etc/krb5kdc/kadm5.acl.bk
sudo vim /etc/krb5kdc/kadm5.acl
*/admin@PAPER.COM *

#sudo rm -rf /var/lib/krb5kdc/*

sudo krb5_newrealm
#sudo kdb5_util create -r PAPER.COM -s

sudo systemctl restart krb5-kdc
sudo systemctl restart krb5-admin-server
sudo systemctl status krb5-kdc
sudo systemctl status krb5-admin-server

#sudo kadmin.local -q "addprinc admin/admin"
#sudo kadmin.local -q "addprinc root/admin"

#sudo vim /etc/ssh/ssh_config
sudo vim /etc/ssh/sshd_config
GSSAPIAuthentication yes
GSSAPICleanupCredentials yes
GSSAPIKeyExchange yes
UsePAM yes

sudo systemctl restart ssh
sudo systemctl status ssh

sudo kadmin.local
#add_policy -minlength -minclasses admin
#add_policy -minlength -minclasses host
#add_policy -minlength -minclasses service
#add_policy -minlength -minclasses user
#add_policy admin
#add_policy host
#add_policy service
#add_policy user
#addprinc -policy user thomas
#addprinc -policy service -randkey host/paper.com

addprinc thomas
addprinc -randkey host/paper.com
ktadd -k /etc/krb5.keytab -norandkey host/paper.com

kinit thomas
ssh thomas@paper.com

klist
klist -t -e -k /etc/krb5.keytab
kinit -R
setprinc thomas
kdestroy

TCP端口88: Kerberos认证服务
TCP/UDP端口749: Kerberos管理服务

sudo apt install krb5-user krb5-config
sudo vim /etc/krb5.conf
[libdefaults]
    default_realm = PAPER.COM
[realms]
    PAPER.COM = {
        kdc = paper.com
        admin_server = paper.com
    }
[domain_realm]
    .paper.com = PAPER.COM
    paper.com = PAPER.COM

kinit thomas
ssh thomas@paper.com
不用输入密码

sssd:
设置dns地址
sudo apt install packagekit
sudo apt install sssd-ad sssd-tools realmd adcli
sudo realm -v discover paper.com
sudo realm join -v -U administrator paper.com
sudo pam-auth-update --enable mkhomedir
tail -f /var/log/sssd/sssd.log

getent passwd xiaoming@paper.com
getent passwd zhanglu@paper.com
groups xiaoming@paper.com
groups zhanglu@paper.com
id xiaoming@paper.com
id zhanglu@paper.com

sudo cat /etc/krb5.conf
[libdefaults]
udp_preference_limit = 0
default_realm = PAPER.COM

sudo cat /etc/sssd/sssd.conf
[sssd]
domains = paper.com
config_file_version = 2
services = nss, pam

[domain/paper.com]
default_shell = /bin/bash
krb5_store_password_if_offline = True
cache_credentials = True
krb5_realm = PAPER.COM
realmd_tags = manages-system joined-with-adcli 
id_provider = ad
fallback_homedir = /home/%u@%d
ad_domain = paper.com
use_fully_qualified_names = True
ldap_id_mapping = True
access_provider = ad

sudo cat /etc/nsswitch.conf
passwd:         files systemd sss
group:          files systemd sss
shadow:         files sss
gshadow:        files

hosts:          files dns
networks:       files

protocols:      db files
services:       db files sss
ethers:         db files
rpc:            db files

netgroup:       nis sss
automount:      sss

systemctl status sssd
klist -k /etc/krb5.keytab
#realm permit --realm paper.com --all

ssh test1@paper.com@192.168.45.249
sudo login
sudo apt install smbclient
smbclient --use-kerberos=required -L winserver1.paper.com
smbclient --use-kerberos=required -L winserver1.paper.com -U test1@paper.com
smbclient //winserver1.paper.com/hahdir -U test1@paper.com
smbclient //winserver1.paper.com/hahdir -U administrator@paper.com

winbind:
sudo vim /etc/resolv.conf
nameserver 192.168.0.113

sudo hostnamectl set-hostname harveynode.book.com
sudo vim /etc/hosts
192.168.0.117 harveynode.book.com

sudo apt update
sudo apt install packagekit
sudo apt install realmd samba
sudo apt install libnss-winbind

sudo realm discover book.com
sudo realm join -v --membership-software=samba --client-software=winbind book.com

sudo vim /etc/nsswitch.conf
passwd:         files systemd winbind
group:          files systemd winbind

sudo pam-auth-update --enable mkhomedir
cat /etc/samba/smb.conf

getent passwd xiaoming@book.com
groups xiaoming@book.com
id xiaoming@book.com

sssctl

nss pam
samba
Kerberos
LDAP

autofs
sudo
su

KRB5_TRACE=/dev/stdout kinit -V aduser@AD.EXAMPLE.COM.
sudo cat /etc/samba/smb.conf

kinit Administrator
net ads join -k
net ads join -U Administrator

/etc/pam.d/common-auth
auth sufficient pam_sss.so use_first_pass
/etc/pam.d/common-account
account [default=bad success=ok user_unknown=ignore] pam_sss.so
/etc/pam.d/common-password
password sufficient pam_sss.so use_authtok
/etc/pam.d/common-session
session optional pam_mkhomedir.so
session optional pam_sss.so

sudo apt-get install -y cifs-utils
sudo apt install slapd ldap-utils
