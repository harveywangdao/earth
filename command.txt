1.groupadd
  sudo groupadd groupname
  sudo groupdel groupname
  sudo useradd ¨Cd /usr/sam -m sam
  sudo userdel sam
  passwd
  
2.samba
  sudo apt-get update
  sudo apt-get install samba samba-common
  sudo mkdir /home/share
  sudo chmod 777 /home/share
  sudo vim /etc/samba/smb.conf
  security = user
  [myshare]
    comment = my share directory
    path = /home/share
    browseable = yes
    writable = yes
  sudo useradd smbuser
  sudo smbpasswd -a smbuser
  sudo service smbd restart

  sudo smbpasswd -x username
  
3.sudo gedit /etc/default/apport
  sudo rm -rf /var/crash
  sudo sed -i 's/enabled=1/enabled=0/g' /etc/default/apport
  sudo df -hl
  sudo du -sh dir
  sudo du -ah --max-depth=1
  sudo fdisk -l
  
4.apt-get update
  apt-get install packagename
  apt-get install packagename --reinstall
  apt-get -f install
  apt-get remove packagename
  apt-get remove --purge packagename
  apt-get autoremove packagename
  apt-get autoremove --purge packagname
  dpkg --force-all --purge packagename
  apt-get autoclean
  apt-get clean 
  apt-get upgrade
  apt-get dist-upgrade
  apt-cache search package
  apt-cache show package
  apt-cache depends package
  apt-cache rdepends package
  apt-get build-dep package
  apt-get source package
  apt-get clean && sudo apt-get autoclean
  apt-get install --reinstall ca-certificates

  lsb_release -a
  do-release-upgrade
  
5.zip -r xxx.zip dir
  unzip xxx.zip
  tar -cvf xxx.tar dir
  tar -xvf xxx.tar
  tar -jcvf xxx.tar.bz2 dir
  tar -jxvf xxx.tar.bz2 -C des_dir
  tar -zcvf xxx.tar.gz dir
  tar -zxvf xxx.tar.gz -C des_dir
  xz -z xxx.tar.xz
  xz -d xxx.tar.xz
  tar -xvJf
  
6.ifconfig eth0 up
  ifconfig eth0 down
  
  ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255
  
  
7.sudo su
  vim /etc/network/interfaces
  # interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopback
  # The primary network interface
  auto eth0
  iface eth0 inet static
  address 192.168.195.129
  netmask 255.255.255.0
  gateway 192.168.195.1
  network 192.168.195.0
  broadcast 192.168.195.255
  
  vim /etc/resolv.conf
  nameserver 192.168.0.1
  nameserver 8.8.8.8
  
  vim /etc/resolvconf/resolv.conf.d/base
  nameserver 192.168.0.1
  nameserver 8.8.8.8
  
  /etc/init.d/networking restart
    
  ifdown eth0
  ifup eth0
  
  
8.golang-beego
	go get github.com/beego/bee
	go get github.com/astaxie/beego
  //golang.org/x/net/context
  mkdir -p golang.org/x
  git clone https://github.com/golang/net.git
  go get github.com/mattn/go-sqlite3
  go get github.com/Unknwon/com
	go get -u github.com/beego/bee
	go get -u github.com/astaxie/beego
	bee fix
	
	bee new projectname
	bee run
	localhost:8080

9.SSL/TLS
  go run $GOROOT/src/crypto/tls/generate_cert.go --host 127.0.0.1


10.PostgreSQL
  sudo apt-get update
  sudo apt-get install postgresql postgresql-client
  sudo -i -u postgres

  sudo /etc/init.d/postgresql start
  sudo /etc/init.d/postgresql stop
  sudo /etc/init.d/postgresql restart

  sudo su - postgres
  psql
  \password postgres

  CREATE USER dbuser WITH PASSWORD 'password';
  CREATE DATABASE exampledb OWNER dbuser;
  GRANT ALL PRIVILEGES ON DATABASE exampledb to dbuser;

  \q

  psql -U dbuser -d exampledb -h 127.0.0.1 -p 5432


  CREATE TABLE user_tbl(name VARCHAR(20), signup_date DATE);
  INSERT INTO user_tbl(name, signup_date) VALUES('ÕÅÈý', '2013-12-22');
  SELECT * FROM user_tbl;
  UPDATE user_tbl set name = 'ÀîËÄ' WHERE name = 'ÕÅÈý';
  DELETE FROM user_tbl WHERE name = 'ÀîËÄ' ;
  ALTER TABLE user_tbl ADD email VARCHAR(40);
  ALTER TABLE user_tbl ALTER COLUMN signup_date SET NOT NULL;
  ALTER TABLE user_tbl RENAME COLUMN signup_date TO signup;
  ALTER TABLE user_tbl DROP COLUMN email;
  ALTER TABLE user_tbl RENAME TO backup_tbl;
  DROP TABLE IF EXISTS backup_tbl;

  sudo apt-get --purge remove postgresql-*
  sudo rm -r /etc/postgresql/
  sudo rm -r /etc/postgresql-common/
  sudo rm -r /var/lib/postgresql/
  sudo userdel -r postgres
  sudo groupdel postgres
  cat /etc/passwd | cut -f 1 -d:
  grep bash /etc/passwd



11.Redis
  sudo apt-get install redis-server
  ps -aux|grep redis
  netstat -nlt|grep 6379
  sudo /etc/init.d/redis-server status
  sudo /etc/init.d/redis-server restart
  sudo /etc/init.d/redis-server start
  sudo /etc/init.d/redis-server stop

  sudo vi /etc/redis/redis.conf
  requirepass redisredis
  #bind 127.0.0.1
  sudo /etc/init.d/redis-server restart

  redis-cli -a 180498
  redis-cli -h host -p port -a password

  set key1 "hello"
  get key1

  set key2 1
  INCR key2

  LPUSH key3 a
  LPUSH key3 b
  LRANGE key3 0 3

  HSET key4 name "John Smith"
  HSET key4 email "abc@gmail.com"
  HGET key4 name
  HGETALL key4

  del key1

  主从、哨兵、集群
  string -- int，raw或者embstr
  hash   -- ziplist 或者 hashtable
  list   -- ziplist(压缩列表) 和 linkedlist(双端链表)
  set    -- intset 或者 hashtable
  sorted set  --  ziplist 或者 skiplist
  事务
  hyperloglog
  pub/sub
  lua
  pipelining

  FIFO：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。
  LRU：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。
  LFU：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰。
  redis缓存击穿(热点key不失效或加锁)、缓存穿透(不存在的key设置空value和过期时间)、缓存雪崩(key失效时间不同)

  分布式锁
  redlock
  持久化RDB AOF


  普通redis分布式锁：
  上锁 set key value px 30000 nx # value是唯一值
  解锁 先判断value是否对再删key，可用lua脚本保证原子性
  if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
  else
    return 0
  end

  存在单点问题

  redlock：
  上锁 多个master节点，相同key和唯一value，响应超时时间50ms，30s-获取锁的时间，只要在一个节点上获取锁失败就要在其它节点释放锁
  所有节点key的超时时间都是30s，但由于设置key本身也需要时间，所以实际超时时间是30s-设置key时间

  1.未上锁          获取锁
  2.已上锁          锁失败
  3.redis节点不够   锁失败，删除所有节点key
  4.响应时间太长    锁失败，删除所有节点key

  1.正常用完        释放锁，删除所有节点key
  2.服务挂了        所有节点key等待超时
  3.服务没挂但超时了 所有节点key，但要判断value，防止删除别人的锁

  1.客户端分区方案 Redis Sharding 根据key的hash映射到哪个节点，主从、哨兵，客户端无法动态增删节点，自行维护分发逻辑
  2.代理分区方案 Twemproxy Codis 性能低
  3.查询路由方案

  顺序分区
  哈希分区
  1.节点取余分区
  2.一致性哈希分区
  3.虚拟槽分区 2^14 0 ~ 16383 slot = CRC16（key）& 16383 限制 key批量操作 key事务操作

  mkdir -p /home/thomas/server/redis-cluster
  cd /home/thomas/server/redis-cluster
  mkdir conf data log
  mkdir -p data/redis-6379 data/redis-6389 data/redis-6380 data/redis-6390 data/redis-6381 data/redis-6391
  cp ~/server/redis-4.0.14/redis.conf ./conf/
  cp redis.conf redis-7001.conf

  daemonize yes
  bind 127.0.0.1
  cluster-enabled yes
  cluster-node-timeout 10000
  appendonly yes
  port 6379
  cluster-config-file /home/thomas/server/redis-cluster/conf/nodes-6379.conf
  dir /home/thomas/server/redis-cluster/data/redis-6379
  pidfile /var/run/redis-cluster/redis-6379.pid
  logfile /home/thomas/server/redis-cluster/log/redis-6379.log

  sudo apt-get install build-essential
  make
  cp redis-4.0.14/src/redis-trib.rb /home/thomas/server/redis-cluster/
  sudo apt-get install ruby
  sudo gem install redis

  cd /home/thomas/server/redis-4.0.14/src
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6379.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6380.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6381.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6389.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6390.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6391.conf

  #./redis-cli -p 6379 shutdown

  ./redis-trib.rb create --replicas 1 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6389 127.0.0.1:6390 127.0.0.1:6391

  Adding replica 127.0.0.1:6390 to 127.0.0.1:6379
  Adding replica 127.0.0.1:6391 to 127.0.0.1:6380
  Adding replica 127.0.0.1:6389 to 127.0.0.1:6381

  ./redis-cli -c -h 127.0.0.1 -p 6379
  cluster nodes
  cluster info

  加节点：
  cp redis-6379.conf redis-6385.conf
  cp redis-6379.conf redis-6395.conf
  mkdir -p data/redis-6385 data/redis-6395

  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6385.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-6395.conf

  ./redis-trib.rb add-node 127.0.0.1:6385 127.0.0.1:6379
  ./redis-trib.rb reshard 127.0.0.1:6379

  增加从节点：
  ./redis-trib.rb add-node --slave --master-id e77bd17d74a84079decfdeee6e75afd3867a44c0 127.0.0.1:6395 127.0.0.1:6379

  删除节点：
  ./redis-trib.rb reshard 127.0.0.1:6379
  ./redis-trib.rb del-node 127.0.0.1:6385 e77bd17d74a84079decfdeee6e75afd3867a44c0
  ./redis-trib.rb del-node 127.0.0.1:6395 1411045e449ff84c13ccc548919eda65e2e9dbd1


  单点主从、哨兵搭建：
  mkdir -p data/redis-7001 data/redis-8001

  daemonize yes
  bind 127.0.0.1
  appendonly no
  port 7001
  logfile /home/thomas/server/redis-cluster/log/redis-7001.log
  pidfile /var/run/redis-cluster/redis-7001.pid
  dir /home/thomas/server/redis-cluster/data/redis-7001

  slaveof 127.0.0.1 7001

  ./redis-server /home/thomas/server/redis-cluster/conf/redis-7001.conf
  ./redis-cli -p 7001
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-8001.conf
  ./redis-cli -p 8001

  哨兵：
  cp ~/server/redis-4.0.14/sentinel.conf ./conf/
  cp sentinel.conf sentinel-9001.conf
  mkdir -p data/sentinel-9001 data/sentinel-9002 data/sentinel-9003

  port 9001
  daemonize yes
  logfile /home/thomas/server/redis-cluster/log/sentinel-9001.log
  dir /home/thomas/server/redis-cluster/data/sentinel-9001
  sentinel monitor mymaster 127.0.0.1 7001 2
  #sentinel auth-pass mymaster abc
  sentinel down-after-milliseconds mymaster 1500
  sentinel parallel-syncs mymaster 1
  sentinel failover-timeout mymaster 30000

  cp sentinel-9001.conf sentinel-9002.conf
  cp sentinel-9001.conf sentinel-9003.conf

  ./redis-sentinel /home/thomas/server/redis-cluster/conf/sentinel-9001.conf
  ./redis-sentinel /home/thomas/server/redis-cluster/conf/sentinel-9002.conf
  ./redis-sentinel /home/thomas/server/redis-cluster/conf/sentinel-9003.conf

  ./redis-cli -p 9001
  info sentinel
  sentinel masters
  sentinel slaves mymaster
  sentinel get-master-addr-by-name mymaster

  ./redis-cli -p 8001 shutdown 没什么变化
  ./redis-cli -p 7001 shutdown

  cp ~/server/redis-4.0.14/redis.conf ./conf/
  cp redis.conf redis-7001.conf

  daemonize yes
  bind 0.0.0.0
  port 7001
  logfile /home/thomas/server/redis-cluster/log/redis-7001.log
  pidfile /var/run/redis-cluster/redis-7001.pid
  dir /home/thomas/server/redis-cluster/data/redis-7001

  mkdir -p data/redis-7001 data/redis-7002 data/redis-7003 data/redis-7004 data/redis-7005

  cp redis-7001.conf redis-7002.conf
  cp redis-7001.conf redis-7003.conf
  cp redis-7001.conf redis-7004.conf
  cp redis-7001.conf redis-7005.conf

  ./redis-server /home/thomas/server/redis-cluster/conf/redis-7001.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-7002.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-7003.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-7004.conf
  ./redis-server /home/thomas/server/redis-cluster/conf/redis-7005.conf
  
  ./redis-cli -p 7001



12.MongoDB
  sudo apt-get install mongodb

  sudo service mongodb start
  sudo service mongodb restart
  sudo service mongodb stop
  systemctl status mongodb.service

  sudo vim /etc/mongodb.conf

  pgrep mongo -l
  sudo apt-get --purge remove mongodb mongodb-clients mongodb-server
  sudo apt-get purge mongodb-org*
  sudo rm -rf /var/log/mongodb
  sudo rm -rf /var/lib/mongodb

  show dbs
  show collections
  show users
  use yourDB
  db.help()
  db.yourCollection.help()
  db.createCollection('teacher')
  db.student.insert({_id:1, sname: 'zhangsan', sage: 20})
  db.student.save({_id:1, sname: 'zhangsan', sage: 22})
  db.student.find()
  db.student.find({sname: 'lisi'})
  db.student.update({sname: 'lisi'}, {$set: {sage: 30}}, false, true)
  db.student.remove({sname: 'chenliu'})

  mongod --dbpath "D:\mongo\data\db" --logpath "D:\mongo\data\log\mongodb.log" --install --serviceName "mongodb"
  NET START mongodb
  NET stop mongodb

  搭建集群：
  sudo apt-get install libcurl3 openssl
  tar -xzvf mongodb-linux-x86_64-ubuntu1604-3.6.14.tgz

  mkdir -p /home/thomas/server/mongo-cluster/conf
  cd /home/thomas/server/mongo-cluster/conf
  touch mongod.conf
  {
  storage:
      engine: wiredTiger
      directoryPerDB: true
      journal:
          enabled: true
  systemLog:
      destination: file
      logAppend: true
  operationProfiling:
    slowOpThresholdMs: 10000
  replication:
      oplogSizeMB: 10240
  processManagement:
      fork: true
  security:
      authorization: "enabled"
  }

  touch mongos.conf
  {
  systemLog:
      destination: file
      logAppend: true
  processManagement:
      fork: true
  }

  mkdir -p /home/thomas/server/mongo-cluster/keyfile
  cd /home/thomas/server/mongo-cluster/keyfile
  sudo openssl rand -base64 756 > mongo.key
  chmod 400 mongo.key

  cd /home/thomas/server/mongo-cluster
  mkdir -p mongos-10001/data mongos-10002/data mongos-10003/data \
        config-primary-10011/data config-secondary-10012/data config-secondary-10013/data \
        shard1-primary-10021/data shard1-secondary-10022/data shard1-secondary-10023/data \
        shard2-primary-10031/data shard2-secondary-10032/data shard2-secondary-10033/data

  cd /home/thomas/server/mongodb-linux-x86_64-ubuntu1604-3.6.14/bin

  KEYFILE=/home/thomas/server/mongo-cluster/keyfile/mongo.key
  WORK_DIR=/home/thomas/server/mongo-cluster
  CONFFILE=/home/thomas/server/mongo-cluster/conf/mongod.conf

  Config Server:       --bind_ip 0.0.0.0
  ./mongod --port 10011 --bind_ip 0.0.0.0 --configsvr --replSet configReplSet --keyFile $KEYFILE --dbpath $WORK_DIR/config-primary-10011/data --pidfilepath $WORK_DIR/config-primary-10011/db.pid --logpath $WORK_DIR/config-primary-10011/db.log --config $CONFFILE

  ./mongod --port 10012 --bind_ip 0.0.0.0 --configsvr --replSet configReplSet --keyFile $KEYFILE --dbpath $WORK_DIR/config-secondary-10012/data --pidfilepath $WORK_DIR/config-secondary-10012/db.pid --logpath $WORK_DIR/config-secondary-10012/db.log --config $CONFFILE

  ./mongod --port 10013 --bind_ip 0.0.0.0 --configsvr --replSet configReplSet --keyFile $KEYFILE --dbpath $WORK_DIR/config-secondary-10013/data --pidfilepath $WORK_DIR/config-secondary-10013/db.pid --logpath $WORK_DIR/config-secondary-10013/db.log --config $CONFFILE

  ./mongo --port 10011 --host 127.0.0.1
  cfg = {
      _id:"configReplSet", 
      configsvr: true,
      members:[
          {_id:0, host:'127.0.0.1:10011'},
          {_id:1, host:'127.0.0.1:10012'}, 
          {_id:2, host:'127.0.0.1:10013'}
      ]};
  rs.initiate(cfg);
  rs.status()

  Shard1:
  ./mongod --port 10021 --bind_ip 0.0.0.0 --shardsvr --replSet shard1 --keyFile $KEYFILE --dbpath $WORK_DIR/shard1-primary-10021/data --pidfilepath $WORK_DIR/shard1-primary-10021/db.pid --logpath $WORK_DIR/shard1-primary-10021/db.log --config $CONFFILE

  ./mongod --port 10022 --bind_ip 0.0.0.0 --shardsvr --replSet shard1 --keyFile $KEYFILE --dbpath $WORK_DIR/shard1-secondary-10022/data --pidfilepath $WORK_DIR/shard1-secondary-10022/db.pid --logpath $WORK_DIR/shard1-secondary-10022/db.log --config $CONFFILE

  ./mongod --port 10023 --bind_ip 0.0.0.0 --shardsvr --replSet shard1 --keyFile $KEYFILE --dbpath $WORK_DIR/shard1-secondary-10023/data --pidfilepath $WORK_DIR/shard1-secondary-10023/db.pid --logpath $WORK_DIR/shard1-secondary-10023/db.log --config $CONFFILE

  ./mongo --port 10021 --host 127.0.0.1
  cfg = {
      _id:"shard1", 
      members:[
          {_id:0, host:'127.0.0.1:10021'},
          {_id:1, host:'127.0.0.1:10022'}, 
          {_id:2, host:'127.0.0.1:10023'}
      ]};
  rs.initiate(cfg);

  Shard2:
  ./mongod --port 10031 --bind_ip 0.0.0.0 --shardsvr --replSet shard2 --keyFile $KEYFILE --dbpath $WORK_DIR/shard2-primary-10031/data --pidfilepath $WORK_DIR/shard2-primary-10031/db.pid --logpath $WORK_DIR/shard2-primary-10031/db.log --config $CONFFILE

  ./mongod --port 10032 --bind_ip 0.0.0.0 --shardsvr --replSet shard2 --keyFile $KEYFILE --dbpath $WORK_DIR/shard2-secondary-10032/data --pidfilepath $WORK_DIR/shard2-secondary-10032/db.pid --logpath $WORK_DIR/shard2-secondary-10032/db.log --config $CONFFILE

  ./mongod --port 10033 --bind_ip 0.0.0.0 --shardsvr --replSet shard2 --keyFile $KEYFILE --dbpath $WORK_DIR/shard2-secondary-10033/data --pidfilepath $WORK_DIR/shard2-secondary-10033/db.pid --logpath $WORK_DIR/shard2-secondary-10033/db.log --config $CONFFILE

  ./mongo --port 10031 --host 127.0.0.1
  cfg = {
      _id:"shard2", 
      members:[
          {_id:0, host:'127.0.0.1:10031'},
          {_id:1, host:'127.0.0.1:10032'}, 
          {_id:2, host:'127.0.0.1:10033'}
      ]};
  rs.initiate(cfg);

  Mongos:
  KEYFILE=/home/thomas/server/mongo-cluster/keyfile/mongo.key
  WORK_DIR=/home/thomas/server/mongo-cluster
  CONFFILE=/home/thomas/server/mongo-cluster/conf/mongos.conf

  ./mongos --port=10001 --bind_ip 0.0.0.0 --configdb configReplSet/127.0.0.1:10011,127.0.0.1:10012,127.0.0.1:10013 --keyFile $KEYFILE --pidfilepath $WORK_DIR/mongos-10001/db.pid --logpath $WORK_DIR/mongos-10001/db.log --config $CONFFILE
  
  ./mongos --port=10002 --bind_ip 0.0.0.0 --configdb configReplSet/127.0.0.1:10011,127.0.0.1:10012,127.0.0.1:10013 --keyFile $KEYFILE --pidfilepath $WORK_DIR/mongos-10002/db.pid --logpath $WORK_DIR/mongos-10002/db.log --config $CONFFILE
  
  ./mongos --port=10003 --bind_ip 0.0.0.0 --configdb configReplSet/127.0.0.1:10011,127.0.0.1:10012,127.0.0.1:10013 --keyFile $KEYFILE --pidfilepath $WORK_DIR/mongos-10003/db.pid --logpath $WORK_DIR/mongos-10003/db.log --config $CONFFILE

  ./mongo --port 10001 --host 127.0.0.1

  sh.addShard("shard1/127.0.0.1:10021")
  sh.addShard("shard2/127.0.0.1:10031")

  use admin
  db.createUser({
      user:'admin',pwd:'123456',
      roles:[
          {role:'clusterAdmin',db:'admin'},
          {role:'userAdminAnyDatabase',db:'admin'},
          {role:'dbAdminAnyDatabase',db:'admin'},
          {role:'readWriteAnyDatabase',db:'admin'}
  ]})

  use admin
  db.auth('admin','123456')
  sh.status()
  rs.add(HOST_NAME:PORT)

  use admin
  db.system.users.find().pretty()
  show users
  db.dropUser("admin")
  db.changeUserPassword('tank2','test')
  db.updateUser("usertest",{roles:[ {role:"read",db:"testDB"} ]})
  db.grantRolesToUser("usertest", [{role:"readWrite", db:"testDB"},{role:"read", db:"testDB"}])
  db.revokeRolesFromUser("usertest",[{role:"read", db:"testDB"}])
  db.shutdownServer()
  
  use appdb
  db.createUser({user:'appuser',pwd:'123456',roles:[{role:'dbOwner',db:'appdb'}]})
  sh.enableSharding("appdb")

  use appdb
  db.createCollection("book")
  db.device.ensureIndex({createTime:1})
  sh.shardCollection("appdb.book", {bookId:"hashed"}, false, { numInitialChunks: 4} )

  use appdb
  var cnt = 0;
  for(var i=0; i<1000; i++){
      var dl = [];
      for(var j=0; j<100; j++){
          dl.push({
                  "bookId" : "BBK-" + i + "-" + j,
                  "type" : "Revision",
                  "version" : "IricSoneVB0001",
                  "title" : "Jackson's Life",
                  "subCount" : 10,
                  "location" : "China CN Shenzhen Futian District",
                  "author" : {
                        "name" : 50,
                        "email" : "RichardFoo@yahoo.com",
                        "gender" : "female"
                  },
                  "createTime" : new Date()
              });
        }
        cnt += dl.length;
        db.book.insertMany(dl);
        print("insert ", cnt);
  }

  db.book.getShardDistribution()

  db.setting.save({"_id":"chunksize","value":1})
  db.user.insert({"id":i,"name":"jack"+i})
  sh.enableSharding("calon")


  show dbs
  db
  db.dropDatabase()

  db.createCollection(name, options)
  show tables
  show collections
  db.collection.drop()

  db.collection.find()
  db.product.find().pretty()
  db.product.find({"test5":"OK"}).pretty()
  db.product.find({$or:[{"test5":"OK"},{"test8":"OK"}]}).pretty()
  $lt $lte $gt $gte $ne $or $in $nin $all
  db.product.find({"test5":{$type:'string'}}).pretty()
  db.product.find({"test5":{$type:'string'}}).limit(1).pretty()
  db.product.find({"test5":{$type:'string'}}).limit(2).skip(1).pretty()
  db.product.find({"test5":{$type:'string'}}).sort({"test9":1}).pretty()
  db.users.find({}, {'name' : 1, 'skills' : 1});
  db.users.find({name:/hurry/}); select * from users where name like "%hurry%";
  db.users.find({name:/^hurry/}); select * from users where name like "hurry%";
  db.users.distinct('name');
  db.users.count();
  db.users.find({'skills' : {'$all' : ['java','python']}})
  db.collection.find({ "field" : { $gt: value1, $lt: value2 } } );

  db.collection.update() db.product.update({"product_name":"phone"}, {$set:{"brand":"apple"}}, {multi:true})
  db.product.update({"price":{$gt:1000}}, {$set:{"test7":"OK"}}, true, true)
  db.product.update({"price":{$gt:8000}}, {$inc:{"test9":1}}, true, true)

  db.collection.remove() db.product.remove({"test9":1})

  db.collection.createIndex(keys, options)
  db.product.createIndex({"test9":1})
  db.product.createIndex({"test9":1}, {unique:true})
  db.posts.ensureIndex({post_text:"text"})
  db.posts.find({$text:{$search:"runoob"}})
  db.posts.getIndexes()
  db.posts.dropIndex("post_text_text")

  db.collection.aggregate()
  db.product.aggregate([{$group:{_id:"$test9", count:{$sum:1}}}])
  $group $sum $avg $min $max $push $addToSet $first $last



  
13.Docker
  https://docs.docker.com/install/linux/docker-ce/ubuntu/
  sudo apt-get update
  sudo apt-get install \
      apt-transport-https \
      ca-certificates \
      curl \
      gnupg-agent \
      software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
  sudo add-apt-repository \
     "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
     $(lsb_release -cs) \
     stable"
  sudo apt-get update
  sudo apt-get install docker-ce docker-ce-cli containerd.io

  #wget -qO- https://get.docker.com/ | sh
  #curl -sSL https://get.docker.com/ | sh

  sudo usermod -aG docker thomas

  docker version
  docker run hello-world

  docker search tutorial
  docker pull learn/tutorial
  docker run learn/tutorial echo "hello word"

  docker run learn/tutorial apt-get install -y ping
  docker ps -l
  docker commit id learn/ping
  docker run lean/ping ping www.baidu.com
  docker inspect id/NAME

  docker images
  docker push learn/ping

  sudo service docker start

  docker run ubuntu:16.04 /bin/echo "Hello world"
  docker run -i -t ubuntu:16.04 /bin/bash
  docker run -d ubuntu:16.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"
  docker logs id/NAME
  docker stop id/NAME
  docker start id/NAME
  docker restart id/NAME
  docker rm id/NAME

  docker run -d -P training/webapp python app.py
  docker run -d -p 5000:5000 training/webapp python app.py
  docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py
  docker port id/NAME
  docker logs -f id/NAME

  Dockerfile
  docker build -t runoob/centos:6.7 .
  docker tag id runoob/centos:dev

  docker rmi -f runoob/ubuntu:v4

14.scrapy
  scrapy startproject jdspider
  scrapy genspider jd jd.com
  scrapy crawl jd -o xxx.json


15.nginx
  sudo apt-get install nginx

  sudo apt-get install build-essential libtool libpcre3 libpcre3-dev zlib1g-dev openssl
  tar -zxvf nginx-1.14.0.tar.gz
  cd nginx-1.14.0
  ./configure --prefix=/usr/local/nginx --with-stream
  make
  sudo make install
  sudo ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx
  vim /etc/init.d/nginx
  sudo update-rc.d nginx defaults
  sudo service nginx start

  nginx -s stop
  nginx -s reload
  nginx -s reopen

  ps -ax | grep nginx

  sudo service nginx start
  sudo service nginx stop
  sudo service nginx restart
  /etc/init.d/nginx start/stop/restart

  dpkg --get-selections|grep nginx
  sudo apt-get --purge remove nginx
  sudo apt-get --purge remove nginx-common
  sudo apt-get --purge remove nginx-core
  ps -ef |grep nginx

  sudo vim /usr/local/nginx/conf/nginx.conf
  sudo nginx -s reload

16.git
  sudo apt-get install git
  
  git config --global user.name "thomas"
  git config --global user.email "1272777053@qq.com"
  
  git config --list
  
  git clone https://github.com/harveywangdao/helloworld.git
  
  git add .
  
  git commit -m "changes log"
  
  git status
  
  git pull
  git push
    
  git push -u origin master

  ssh-keygen -t rsa

  git init --bare bigpen.git
  git clone thomas@119.23.8.126:/home/thomas/doc/tech/bigpen.git

  git submodule update --init --recursive

17.golang
  goland

  sudo tar -zxvf go1.6.2.linux-amd64.tar.gz -C /usr/local/
  sudo vim  /etc/profile
  export GOROOT=/usr/local/go
  //export GOBIN=$GOROOT/bin
  export PATH=$PATH:$GOROOT/bin
  export GOPATH=$HOME/goproj
  export PATH=$PATH:$GOPATH/bin
  source /etc/profile
  go version
  
18.FTP:
  sudo apt-get install vsftpd
  sudo vim /etc/vsftpd.conf
  sudo service vsftpd restart

19.ssh
  sudo apt-get install openssh-server openssh-client
  sudo ps -e |grep ssh
  sudo service ssh start
  
  /etc/init.d/ssh start
  sudo/etc/init.d/ssh restart
  
  /etc/ssh/sshd_config

20.JDK:
  http://www.oracle.com/technetwork/java/javase/downloads/index.html
  sudo mkdir /usr/local/java
  sudo tar -xzvf jdk-8u91-linux-x64.tar.gz -C /usr/local/java/
  #sudo vim /etc/profile
  #source /etc/profile
  #sudo vim /etc/sudoers
  sudo vim ~/.bashrc
  export JAVA_HOME=/usr/local/java/jdk1.8.0_91
  export JRE_HOME=$JAVA_HOME/jre
  export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib
  export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
  
21.gcc/g++:
  sudo apt-get install build-essential

22.vim:
  sudo apt-get install vim-gtk
  sudo vim /etc/vim/vimrc
  set nu                       // ÔÚ×ó²àÐÐºÅ
  set tabstop=4                  //tab ³¤¶ÈÉèÖÃÎª 4
  set nobackup               //¸²¸ÇÎÄ¼þÊ±²»±¸·Ý
  set cursorline               //Í»³öÏÔÊ¾µ±Ç°ÐÐ
  set ruler                       //ÔÚÓÒÏÂ½ÇÏÔÊ¾¹â±êÎ»ÖÃµÄ×´Ì¬ÐÐ
  set autoindent             //×Ô¶¯Ëõ½ø

23.vmware tools:
  sudo tar zxf VMwareTools-xxx.tar.gz
  cd /vmware-tools-distrib
  sudo ./vmware-install.pl

24.python:
  PyCharm
  xz -d Python-3.5.1.tar.xz
  tar -C /home/harvey/src/ -xvf Python-3.5.1.tar
  ./configure
  sudo make
  sudo make install
  
  /usr/local/lib/python3.5/
  /usr/local/bin/python3.5
  
  sudo rm /usr/bin/python
  sudo ln -s /usr/bin/python3.5 /usr/bin/python

  Python 2.7.6
  Python 3.4.3
  Python 3.5.3

  sudo apt-get install python-software-properties
  sudo apt-get install software-properties-common
  sudo add-apt-repository ppa:fkrull/deadsnakes
  sudo apt-get update
  sudo apt-get install python3.5
  sudo ln -s /usr/bin/python3.5 /usr/bin/python

  sudo python -m pip install --upgrade pip
  
25.Django:
  django-admin.py startproject myproject
  python manage.py runserver 0.0.0.0:8000
  
  python manage.py startapp myapp
  
  myproject/myproject/settings.py

  INSTALLED_APPS = [
      'django.contrib.admin',
      'django.contrib.auth',
      'django.contrib.contenttypes',
      'django.contrib.sessions',
      'django.contrib.messages',
      'django.contrib.staticfiles',
      'myapp',
  ]

  python manage.py makemigrations
  python manage.py migrate

  python manage.py createsuperuser

  Django 1.9.5 Python2.7
  pip 1.5.4
  Django 1.11.6

  sudo apt-get install python-pip  #python2.x
  sudo apt-get install python3-pip #python3.x
  sudo pip3 install packagename    #python3.x
  sudo pip install Django
  sudo pip install --upgrade pip
  
  django-admin.py startproject project-name
  python manage.py startapp app-name
  python manage.py makemigrations
  python manage.py migrate
  
  python manage.py runserver
  python manage.py runserver 8001
  python manage.py runserver 0.0.0.0:8000
  python manage.py flush
  python manage.py createsuperuser
  python manage.py changepassword username
  python manage.py dumpdata appname > appname.json
  python manage.py loaddata appname.json
  python manage.py shell
  
  python manage.py dbshell

26.MySQL:
  mysql_upgrade -u root -p --force
  show global variables like 'port';
  
  wget https://dev.mysql.com/get/mysql-apt-config_0.8.8-1_all.deb
  sudo dpkg -i mysql-apt-config_0.8.8-1_all.deb
  sudo apt-get update
  sudo apt-get install mysql-server
  sudo mysql_upgrade -u root -p
  sudo service mysql restart
  mysqlcheck -u root -p --all-databases

  sudo apt-get install mysql-client
  sudo apt-get install libmysqlclient-dev
  
  sudo apt-get install libapache2-mod-auth-mysql
  sudo apt-get install python-mysqldb    #python2.x
  
  sudo netstat -tap | grep mysql
  mysql -u root -p
  mysql -h198.168.122.122 -u root -p 123456
  exit
  mysqladmin -u root -p 123456 password new123456
  
  sudo /etc/init.d/mysql start
  sudo /etc/init.d/mysql stop
  sudo /etc/init.d/mysql restart

  bind-address = 0.0.0.0
  GRANT ALL PRIVILEGES ON *.* TO'root'@'%' IDENTIFIED BY '180498' WITH GRANT OPTION;
  flush privileges;

  show variables like '%max_connections%';
  set global max_connections=1000;
  show status like 'Threads%';
  show processlist;
  
  show databases;
  set names utf8;
  create database db001;
  drop database db001;
  drop database if exists db001;
  use db001;
  show tables;
  grant select,insert,update,delete on *.* to [email=test1@¡±%]test1@¡±%[/email]¡± Identified by ¡°abc¡±;
  grant select,insert,update,delete on mydb.* to [email=test2@localhost]test2@localhost[/email] identified by ¡°abc¡±;
  grant select,insert,update,delete on mydb.* to [email=test2@localhost]test2@localhost[/email] identified by ¡°¡±;
  
  select version();
  select now();
  SELECT DAYOFMONTH(CURRENT_DATE);
  SELECT MONTH(CURRENT_DATE);
  SELECT YEAR(CURRENT_DATE);
  SELECT "welecome to my blog!";
  select ((4 * 4) / 10 ) + 25;
  
  create table MyTable(
    id int(4) not null primary key auto_increment,
    name char(20) not null,
    sex int(4) not null default '0',
    degree double(16,2)
  );
  drop table MyTable;
  
  insert into MyTable values(1,'Tom',96.45),(2,'Joan',82.99), (2,'Wang', 96.59);
  select * from MyTable;
  select * from MyTable order by id limit 0,2;
  delete from MyTable where id=1;
  update MyTable set name='Mary' where id=1;
  alter table MyTable add passtest int(4) default '0';
  rename table MyTable to MyTable1;

  主从搭建:
  sudo apt-get install mysql-server
  sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf

  主:
  bind-address = 0.0.0.0
  server-id   = 7
  log-bin     = mysql-bin
  # binlog-do-db = db01
  # binlog-ignore-db=mysql
  # binlog-ignore-db=information_schema
  # binlog-ignore-db=performance_schema
  # binlog-ignore-db=performance_schema
  # binlog-ignore-db=sys

  sudo /etc/init.d/mysql restart
  mysql -u root -p
  CREATE USER 'slave'@'%' IDENTIFIED BY '123456';
  GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%';
  show master status\G

  从:
  bind-address = 0.0.0.0
  server-id   = 11
  log-bin     = mysql-slave-bin
  relay_log   = edu-mysql-relay-bin
  # read_only   = 1

  change master to master_host='192.168.1.7', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=617, master_connect_retry=30;
  show slave status\G
  start slave;

  create database appledb;
  drop database appledb;
  show processlist\G
  show binlog events\G
  stop slave;

  mysql -u root -p -Dant_test< ~/server/ant_test.sql 
  GRANT ALL PRIVILEGES ON *.* TO'root'@'%' IDENTIFIED BY '180498' WITH GRANT OPTION;

  主主:
  #sudo apt-get remove --purge mysql-server
  #sudo apt-get remove mysql-common
  sudo apt-get autoremove mysql* --purge
  sudo rm -rf /var/lib/mysql*
  sudo rm -rf /etc/mysql

  mysql -u root -p
  create database appledb;
  use appledb;
  create table apple_tb (id int, name char(20));
  insert into apple_tb (id, name) values (1, 'xiaoming');
  insert into apple_tb (id, name) values (2, 'xiaohong');
  insert into apple_tb (id, name) values (3, 'xiaowang');
  select * from apple_tb\G

  sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf

  主1:
  bind-address = 0.0.0.0
  server-id   = 10
  log-bin     = mysql-bin
  auto_increment_offset = 1
  auto_increment_increment = 2

  sudo /etc/init.d/mysql restart

  主2:
  bind-address = 0.0.0.0
  server-id   = 11
  log-bin     = mysql-bin
  auto_increment_offset = 2
  auto_increment_increment = 2

  sudo /etc/init.d/mysql restart

  #grant replication slave on *.* to 'repl'@'192.168.10.12' identified by '123456';
  #grant replication slave on *.* to 'repl'@'192.168.10.11' identified by '123456';
  grant replication slave,replication client on *.* to 'slave'@'%' identified by '123456';
  flush privileges;

  mysql -u root -p -Dappledb< ~/server/appledb.sql

  show master status;

  change master to master_host='192.168.1.11', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=2434, master_connect_retry=30;
  start slave;
  show slave status\G

  change master to master_host='192.168.1.10', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=1424, master_connect_retry=30;
  start slave;
  show slave status\G

  keepalived:
  tar -xzvf keepalived-2.0.18.tar.gz
  cd keepalived-2.0.18
  sudo apt-get install openssl libssl-dev libpopt-dev libnl-3-dev
  ./configure --prefix=/usr/local/keepalived
  make
  sudo make install

  sudo apt-get install keepalived
  cd /usr/local/keepalived/sbin
  sudo mv /usr/sbin/keepalived /usr/sbin/keepalived_bk
  sudo cp keepalived /usr/sbin/keepalived

  sudo service keepalived start
  sudo vim /etc/keepalived/keepalived.conf

  global_defs {
    router_id db10
  }

  # state MASTER
  vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 51
    priority 100
    nopreempt
    advert_int 1

    authentication {
      auth_type PASS
      auth_pass 1111
    }

    virtual_ipaddress {
      192.168.1.100
    }
  }

  virtual_server 192.168.1.100 3306 {
    delay_loop 6
    persistence_timeout 50
    protocol TCP

    real_server 192.168.1.10 3306 {
      notify_down /etc/keepalived/kill_keepalived.sh
      TCP_CHECK {
        connect_timeout 1
        retry 2
        delay_before_retry 1
        connect_port 3306
      }
    }
  }

  sudo vim /etc/keepalived/kill_keepalived.sh
  #!/bin/bash
  service keepalived stop

  #kill -9 $(cat /var/run/keepalived.pid)
  #pkill keepalived
  sudo chmod +x /etc/keepalived/kill_keepalived.sh

  ip addr
  sudo /etc/init.d/mysql start
  sudo /etc/init.d/mysql stop
  sudo service keepalived start
  sudo service keepalived stop
  ps -ef | grep keepalived

  tail -f /var/log/syslog | grep Keepalived

  haproxy:
  sudo apt-get install haproxy
  sudo cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy_bk.cfg
  sudo vim /etc/haproxy/haproxy.cfg

  global
    daemon
    nbproc 1
    pidfile /var/run/haproxy.pid

  defaults
    mode tcp
    retries 2
    option redispatch
    option abortonclose
    maxconn 4096
    timeout connect 5000ms
    timeout client 30000ms
    timeout server 30000ms
    log 127.0.0.1 local0 err

  listen test1
    bind 0.0.0.0:3306
    mode tcp
    server s1 192.168.1.10:3306
    server s2 192.168.1.11:3306

  sudo /etc/init.d/mysql start
  sudo /etc/init.d/mysql stop
  sudo service haproxy start
  sudo service haproxy restart
  sudo service haproxy stop
  sudo service haproxy status
  ps -ef | grep haproxy

  lvs:
  lsmod | grep ip_vs
  sudo ls -alRUv /lib/modules/$(uname -r)/kernel | grep ipvs
  find /lib/modules/$(uname -r)/ -iname "**.ko*" | cut -d/ -f5- | grep ipvs

  sudo apt-get install ipvsadm
  cat /proc/sys/net/ipv4/ip_forward
  #echo "1">/proc/sys/net/ipv4/ip_forward

  DR模式 TUN模式 NAT模式 Director RealServer

  sudo ifconfig eth0:0 192.168.1.200 netmask 255.255.255.0 broadcast 192.168.1.200
  sudo route add -host 192.168.1.200 dev eth0:0

  sudo ipvsadm -A -t 192.168.1.200:3306 -s rr
  sudo ipvsadm -a -t 192.168.1.200:3306 -r 192.168.1.10 -g
  sudo ipvsadm -a -t 192.168.1.200:3306 -r 192.168.1.11 -g

  #ipvsadm -D -t 192.168.1.200

  sudo ifconfig lo:0 192.168.1.200 netmask 255.255.255.255 broadcast 192.168.1.200 up
  sudo route add -host 192.168.1.200 dev lo:0

  sudo ipvsadm -L -n
  sudo ipvsadm -L -c


27.apache:
  sudo apt-get install apache2
  
  sudo apt-get install libapache2-mod-python
  sudo apt-get install libapache2-mod-wsgi
  
  http://127.0.0.1
  
  ls /etc/apache2
  
  sudo /etc/init.d/apache2 start
  sudo /etc/init.d/apache2 restart
  sudo /etc/init.d/apache2 stop
  
  /var/www/html
  sudo vim /etc/apache2/apache2.conf
  sudo vim /etc/apache2/sites-available/000-default.conf
  sudo cp /var/www/html/index.html /var/www/

  
  //sudo apt-get --purge remove apache2-common
  sudo apt-get --purge remove apache2
  sudo find /etc -name "*apache*" |xargs  rm -rf
  sudo rm -rf /var/www
  //sudo rm -rf /etc/libapache2-mod-jk
  dpkg -l |grep apache2|awk '{print $2}'|xargs dpkg -P

28.ZooKeeper
  tar -xzvf zookeeper-3.4.10.tar.gz -C /home/thomas/server/
  conf/zoo_sample.cfg --> conf/zoo.cfg

  vim zoo.cfg
  dataDir=/home/thomas/server/zookeeper-3.4.10/data
  dataLogDir=/home/thomas/server/zookeeper-3.4.10/logs
  server.1=localhost:2888:3888

  data/myid -->1

  sudo vim /etc/profile
  export ZOOKEEPER_HOME=/home/thomas/server/zookeeper-3.4.10
  export PATH=$PATH:$ZOOKEEPER_HOME/bin
  source /etc/profile

  zkServer.sh start
  zkCli.sh -server localhost:2181
  zkServer.sh status
  zkServer.sh stop

  create /mynode
  get /mynode
  rmr /mynode

  zkserver
  zkserver status
  zkserver stop
  zkcli -server localhost:2181

  集群搭建:
  tar -xzvf zookeeper-3.4.14.tar.gz
  conf/zoo_sample.cfg --> conf/zoo.cfg

  vim zoo.cfg
  dataDir=/home/thomas/server/zookeeper-3.4.14/data
  dataLogDir=/home/thomas/server/zookeeper-3.4.14/logs
  server.1=192.168.1.7:2888:3888
  server.2=192.168.1.10:2888:3888
  server.3=192.168.1.11:2888:3888

  echo 1 >> /home/thomas/server/zookeeper-3.4.14/data/myid
  echo 2 >> /home/thomas/server/zookeeper-3.4.14/data/myid
  echo 3 >> /home/thomas/server/zookeeper-3.4.14/data/myid

  cd /home/thomas/server/zookeeper-3.4.14/bin
  ./zkServer.sh start
  ./zkServer.sh status
  ./zkServer.sh stop


29.kafka
  tar -xzvf kafka_2.12-1.0.0.tgz -C /home/thomas/server/
  #bin/zookeeper-server-start.sh config/zookeeper.properties
  bin/kafka-server-start.sh config/server.properties
  bin/kafka-server-stop.sh config/server.properties

  bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
  bin/kafka-topics.sh --list --zookeeper localhost:2181
  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
  bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 2>/dev/null

  bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test

  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic

  vim server.properties
  vim zookeeper.properties
  vim producer.properties
  vim consumer.properties

  cd E:\golang\src\hcxy\iov\iov_server
  cd E:\golang\src\hcxy\iov\iov_client

  bin\windows\kafka-server-start.bat config\server.properties
  bin\windows\kafka-server-stop.bat config\server.properties

  bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 10 --topic TspToIov
  bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
  bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic TspToIov
  bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --topic TspToIov --from-beginning 2>/dev/null

  集群搭建:
  tar -xzvf kafka_2.12-2.2.1.tgz

  cd /home/thomas/server/kafka_2.12-2.2.1/config
  vim server.properties

  broker.id=1
  listeners=PLAINTEXT://192.168.1.7:9092
  zookeeper.connect=192.168.1.7:2181,192.168.1.10:2181,192.168.1.11:2181

  broker.id=2
  listeners=PLAINTEXT://192.168.1.10:9092
  zookeeper.connect=192.168.1.7:2181,192.168.1.10:2181,192.168.1.11:2181

  broker.id=3
  listeners=PLAINTEXT://192.168.1.11:9092
  zookeeper.connect=192.168.1.7:2181,192.168.1.10:2181,192.168.1.11:2181

  cd /home/thomas/server/kafka_2.12-2.2.1/bin
  ./kafka-server-start.sh -daemon ../config/server.properties
  ./kafka-topics.sh --create --zookeeper 192.168.1.7:2181 --replication-factor 2 --partitions 3 --topic xiaoming
  ./kafka-topics.sh --describe --zookeeper 192.168.1.7:2181 --topic xiaoming
  ./kafka-console-producer.sh --broker-list 192.168.1.7:9092 --topic xiaoming
  ./kafka-console-consumer.sh --bootstrap-server 192.168.1.10:9092 --from-beginning --topic xiaoming
  ./kafka-topics.sh --zookeeper 192.168.1.7:2181 --list

  ./kafka-consumer-groups.sh --new-consumer --bootstrap-server 192.168.1.7:9092 --list
  ./kafka-consumer-groups.sh --new-consumer --bootstrap-server 192.168.1.7:9092 --group groupid1 --describe

30.netstat
  netstat -a
  netstat -ap
  sudo netstat -apn | grep 3306
  ps -aux | grep pid
  lsof -i:3306

31.protobuf
  git clone https://github.com/google/protobuf.git
  ./autogen.sh
  ./configure
  make
  sudo make install
  protoc -I=$SRC_DIR --cpp_out=$DST_DIR /path/to/file.proto

  sudo ldconfig 
  #sudo vim /etc/ld.so.conf

  go get -u github.com/golang/protobuf/protoc-gen-go
  go get -u github.com/golang/protobuf/proto
  protoc --go_out=. *.proto
  protoc --go_out=plugins=grpc,import_path=mypackage:. *.proto
  protoc --go_out=plugins=grpc:. *.proto

  #go get -u google.golang.org/grpc
  go get -u github.com/grpc/grpc-go

32.node.js
  curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -
  sudo apt-get install -y nodejs

  curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -
  sudo apt-get install -y nodejs
  
  sudo npm install -g cnpm --registry=https://registry.npm.taobao.org
  sudo cnpm install vue
  sudo cnpm install --global vue-cli

  vue init webpack my-project
  cnpm install
  cnpm run dev

  npm cache clean --force
  npm config set registry http://registry.npmjs.org
  npm config set registry https://registry.npm.taobao.org
  npm config list
  npm config get registry
  npm config get prefix

  sudo npm uninstall -g cnpm
  sudo npm install cnpm -g
  sudo npm install -g cnpm --registry=https://registry.npm.taobao.org

  npm config list
  npm config ls
  npm config ls -l

  npm config set registry "http://registry.npmjs.org"
  npm config set registry http://registry.cnpmjs.org
  npm config set registry https://registry.npm.taobao.org
  npm config set registry https://registry.npmjs.org

  npm cache clean -f
  npm cache clean --force
  npm cache verify

  npm info underscore
  npm --registry http://registry.cnpmjs.org info underscore

  sudo npm install npm -g

33.nohup
  nohup /root/start.sh >/dev/null 2>&1 &
  nohup node app.js >>aaa.log 2>&1 &
  exit退出终端才可以

  forever start app.js
  forever start -l forever.log -o out.log -e err.log app.js 
  forever stop app.js
  forever list

34.挂载
  sudo fdisk -l
  sudo df -h
  ls -l /dev/disk/by-id/
  sudo file -s /dev/vdb

  sudo fdisk /dev/vdb
  n
  p
  1
  wq

  sudo fdisk -l
  ls -l /dev/disk/by-id/

  sudo mkfs.ext3 /dev/vdb1

  mkdir ~/ethereum
  mount /dev/vdb1 /home/ubuntu/ethereum

  lsblk -f

  sudo cp /etc/fstab /etc/fstab.backup

  sudo vim /etc/fstab
  /dev/disk/by-id/virtio-disk-o7pgds1a-part1 /home/ubuntu/ethereum ext3 defaults,nofail 0 1

  sudo mount -a

  扩容
  sudo umount -v /home/ubuntu/ethereum
  wget -O /tmp/devresize.py http://mirrors.tencentyun.com/install/virts/devresize.py
  sudo python /tmp/devresize.py /dev/vdb
  sudo mount -a

35.MinGW Clang
  gcc main.c -o appc.exe
  g++ main.cpp -o appcpp.exe

  #clang -std=c11 -Wall --target=i686-pc-mingw32 main.c -o appc.exe
  #clang++ -std=c++14 -Wall --target=i686-pc-mingw32 main.cpp -o appcpp.exe

  #clang --target=i686-w64-mingw32 main.c -o appc.exe
  #clang --target=i686-pc-mingw32 main.c -o appc.exe
  #clang++ --target=i686-pc-mingw32 main.cpp -o appcpp.exe
  clang --target=x86_64-w64-mingw32 main.c -o appc.exe
  clang++ --target=x86_64-w64-mingw32 main.cpp -o appcpp.exe

  add_definitions("--target=i686-pc-mingw32")
  set(CMAKE_C_COMPILER "D:/Program Files/LLVM/bin/clang.exe")
  set(CMAKE_CXX_COMPILER "D:/Program Files/LLVM/bin/clang++.exe")
  cmake -DCMAKE_CXX_COMPILER="D:/Program Files/LLVM/bin/clang++.exe"
  cmake -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DCMAKE_MAKE_PROGRAM=mingw32-make
  message(STATUS "This is BINARY dir " ${CMAKE_BINARY_DIR})

  cmake -G"Unix Makefiles" .
  cmake -G"MinGW Makefiles" .

  cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++  .
  cmake -DCMAKE_TOOLCHAIN_FILE=test.toolchain.cmake -G"MinGW Makefiles" .

  findstr /s /i "__float128" *.*

36.Ethereum
  go get -d github.com/ethereum/go-ethereum
  git checkout v1.8.12
  make geth

  geth --datadir data --rpc --rpcaddr 0.0.0.0 --rpccorsdomain "*" --syncmode "fast" --cache=2048 console 2>>eth.log
  nohup geth --datadir data --rpc --rpcaddr 0.0.0.0 --rpccorsdomain "*" --syncmode "fast" --cache=2048 >>eth.log 2>&1 &
  geth attach ipc:/home/ubuntu/ethereum/node1/data/geth.ipc

  net.listening
  net.peerCount
  admin.peers
  admin.nodeInfo
  eth.accounts
  personal.listAccounts
  miner.setEtherbase("0x348b4276c75425016be89a2627bd4797d4c638dc")

  personal.unlockAccount(eth.coinbase)
  eth.getBalance(eth.coinbase).toNumber()
  eth.syncing
  eth.mining
  eth.blockNumber
  miner.setEtherbase(eth.coinbase)
  miner.start(4)
  miner.stop()

  sudo cnpm install solc -g
  sudo add-apt-repository ppa:ethereum/ethereum
  sudo apt-get update
  sudo apt-get install solc
  sudo cnpm install web3 -g
  sudo cnpm install truffle -g
  sudo cnpm install webpack -g
  sudo cnpm install webpack-cli -g

  vim private.json
  geth --datadir data init private.json
  geth --rpc --rpcaddr 0.0.0.0 --rpccorsdomain "*" --datadir data --networkid 1448 console
  geth attach ipc:/home/thomas/eth/data/geth.ipc

  personal.newAccount('13717064390')
  eth.getBalance('0xe89d4872b78ab5c5c903583725fe5d485686d6ce')
  eth.getBalance("0x044b8ab7c603f0938f53e72b7586ec38f3eff044")
  eth.getBalance('0xced5d036328e9b6da0ed9a7ce1a7770b951fc636')
  miner.start(1)
  miner.stop()

  personal.unlockAccount("0xe89d4872b78ab5c5c903583725fe5d485686d6ce",'13717064390')
  personal.unlockAccount("0x044b8ab7c603f0938f53e72b7586ec38f3eff044",'13717064390')
  personal.unlockAccount("0xced5d036328e9b6da0ed9a7ce1a7770b951fc636",'13717064390')
  eth.getBlock(1)
  web3.toDecimal(0xffffffff)
  web3.toWei('1','ether')
  web3.fromWei('22000000000000', 'ether')
  wei kwei mwei gwei microether milliether ether
  txpool.status
  eth.coinbase
  eth.sendTransaction({from: '0xe89d4872b78ab5c5c903583725fe5d485686d6ce', to: '0x044b8ab7c603f0938f53e72b7586ec38f3eff044', value: web3.toWei(1, "ether")})  0x10ada6672609311159ae15510801e2c27db91c7e29c2129eb7f74c172a6f58bf
  eth.sendTransaction({from: '0x044b8ab7c603f0938f53e72b7586ec38f3eff044', to: '0xced5d036328e9b6da0ed9a7ce1a7770b951fc636', value: web3.toWei(1, "ether")})
  eth.getTransaction("0x1b21bba16dd79b659c83594b0c41de42debb2738b447f6b24e133d51149ae2a6")

  truffle unbox webpack
  npm install
  truffle compile
  truffle migrate
  miner.start(1);admin.sleepBlocks(1);miner.stop();

  1.获得地址和私钥
  2.交易
  3.编写Solidity智能合约
  4.编译智能合约
  5.安装智能合约
  6.升级智能合约
  7.交易gas、合约gas
  8.solcjs solc --bin --abi -o ./ ./hello.sol
  9.truffle
  10.web3

  1.solidity编码
  2.solidity编译
  3.移植智能合约ABI
  4.用JSON-RPC与智能合约交互

  MetaCoin.deployed().then(function(instance){return instance.getBalance(web3.eth.accounts[0]);}).then(function(value){return value.toNumber()});
  MetaCoin.deployed().then(function(instance){return instance.sendCoin(web3.eth.accounts[1], 500);});

  curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"eth_accounts","params":[],"id":67}' 127.0.0.1:8545
  curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"eth_getBalance","params":["0xe89d4872b78ab5c5c903583725fe5d485686d6ce","latest"],"id":67}' 127.0.0.1:8545
  curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"eth_blockNumber","params":[],"id":67}' 127.0.0.1:8545

  curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0","method":"eth_sendTransaction","params": [{                                                                               
    "from": "0xe89d4872b78ab5c5c903583725fe5d485686d6ce",
    "to": "0x044b8ab7c603f0938f53e72b7586ec38f3eff044",
    "value": "0x1"
  }],
  "id":67}' 127.0.0.1:8545

  {“jsonrpc”:”2.0”,”method”:”eth_call”,”params”:[{“to”:”0xcF9b0ea3D6Cd99C17531eaC74D9B8C845520D688”,”data”:”0xfe50cc72”},”latest”],”id”:67}
  "{\"jsonrpc\":\"2.0\",\"method\":\"web3_clientVersion\",\"params\":[{\"to\":\"0x3b37B585D69720a10241b0690310daED9FBD521E\",\"data\":\"0xfe50cc72\"},\"latest\"],\"id\":67}";

  curl -X POST -H "Content-Type":application/json --data '{"jsonrpc":"2.0", "method":"net_version","params":[],"id":67}' 127.0.0.1:8545

  solc --abi contracts/Greeter.sol
  abigen --abi contracts/Greeter.abi --pkg greeter --out greeter.go


  solc --abi Store.sol
  solc --bin Store.sol
  abigen --bin=Store_sol_Store.bin --abi=Store_sol_Store.abi --pkg=store --out=Store.go

  privkey:24e37b26f354d123eb4d2675bac002fd802e4db39d6df10fcd2faae8c111b586
  address:0xf036eF6F352048b27C291295B6f6DCD237973a0d

  contractAddr:0x2b01981B95904CA40B0F390b5D196D4fe56e2f0E

  privkey:a166eae0879b0eee88a7a5960cf4f4e0b589c2b2f47121eaddd0d7f3df585059
  address:0x23ACbdE81993A95F762Fe858fC75F201636056e5

37.fabric

38.EOS
  mkdir /home/thomas/eos/contracts

  docker pull eosio/eos:v1.4.5
  docker logs --tail 10 eosio

  curl http://127.0.0.1:8888/v1/chain/get_info

  cleos --wallet-url http://127.0.0.1:5555 wallet list
  curl --request POST --url http://127.0.0.1:5555/v1/wallet/list_wallets --header 'content-type: application/x-www-form-urlencoded; charset=UTF-8'

  cleos --url http://127.0.0.1:8888/ --wallet-url http://0.0.0.0:5555/ push action eosio.token transfer '[ "alice", "bob", "25.0000 SYS", "m" ]' -p alice@active

  git clone --recursive https://github.com/eosio/eosio.cdt --branch v1.2.1 --single-branch
  cd eosio.cdt
  ./build.sh SYS
  sudo ./install.sh
  
  docker run --name eosio \
    --publish 8888:8888 \
    --publish 5555:5555 \
    --volume /home/thomas/eos/contracts:/home/thomas/eos/contracts \
    --detach \
    eosio/eos \
    /bin/bash -c \
    "keosd --http-server-address=0.0.0.0:5555 & exec nodeos -e -p eosio --plugin eosio::producer_plugin --plugin eosio::producer_api_plugin --plugin eosio::chain_api_plugin --plugin eosio::history_plugin --plugin eosio::history_api_plugin --plugin eosio::http_plugin --data-dir /home/thomas/eosio/data --config-dir /home/thomas/eosio/config --http-server-address=0.0.0.0:8888 --access-control-allow-origin=* --contracts-console --http-validate-host=false --filter-on='*'"

  docker exec -it eosio bash
  cleos wallet create --to-console -n sunlight

  wallet password: PW5J5AvkFzVdyy5Z4JoHAZVYV7nv1wgx4dwe48st6LBDELw6Rp7aJ

  cleos wallet open -n sunlight
  cleos wallet unlock -n sunlight
  #cleos wallet lock -n sunlight
  #cleos wallet list
  cleos wallet create_key -n sunlight

  public key: EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA

  cleos wallet import -n sunlight

  private key: 5KQwrPbwdL6PhXujxW37FSSQZ1JiwsST4cqQzDeyXtP79zkvFD3
  public key: EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV

  cleos wallet keys

  cleos create account eosio bob EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA
  cleos create account eosio alice EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA

  eosio-cpp -abigen hello.cpp -o hello.wasm
  cleos create account eosio hello EOS6pXnMFJTyKbTTYKaMNQhRERWnvW5i9L7PeTg34mNuuWKmDUuSA -p eosio@active
  cleos set contract hello /home/thomas/eos/contracts/hello -p hello@active
  cleos push action hello hi '["bob"]' -p bob@active
  cleos push action hello hi '["alice"]' -p alice@active

  eosio-cpp -I include -o eosio.token.wasm src/eosio.token.cpp -abigen
  cleos create account eosio eosio.token EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV
  cleos set contract eosio.token /home/thomas/eos/contracts/eosio.contracts/eosio.token --abi eosio.token.abi -p eosio.token@active
  
  eosio-cpp -I include -o eosio.bios.wasm src/eosio.bios.cpp -abigen
  cleos create account eosio eosio.bios EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV
  cleos set contract eosio.bios /home/thomas/eos/contracts/eosio.contracts/eosio.bios -p eosio.bios@active

  eosio-cpp -I include -o eosio.system.wasm src/eosio.system.cpp -abigen
  cleos create account eosio eosio.system EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV
  cleos set contract eosio.system /home/thomas/eos/contracts/eosio.contracts/eosio.system -p eosio.system@active

  cleos push action eosio.token create '{"issuer":"eosio", "maximum_supply":"1000000000.0000 SYS"}' -p eosio.token@active
  cleos push action eosio.token issue '[ "alice", "10000.0000 SYS", "memo" ]' -p eosio@active
  cleos push action eosio.token transfer '[ "alice", "bob", "2.0000 SYS", "m" ]' -p alice@active
  
  cleos get currency balance eosio.token bob SYS
  cleos get currency balance eosio.token alice SYS
  cleos get currency balance eosio.token eosio SYS
  cleos get currency balance eosio.token eosio.token SYS

39.boost
  sudo apt-get install build-essential g++ python-dev autotools-dev libicu-dev build-essential libbz2-dev libboost-all-dev
  tar -xjvf boost.tar.bz2
  sudo ./bootstrap.sh --prefix=/usr/local/boost
  sudo ./b2 install

40.bitcoin
  docker pull freewil/bitcoin-testnet-box
  docker run -t -i -p 19001:19001 -p 19011:19011 freewil/bitcoin-testnet-box

  docker exec -it 85551da51200 /bin/bash

  bitcoind -datadir=1 -daemon
  bitcoind -datadir=2 -daemon

  bitcoin-cli -datadir=1 -getinfo
  bitcoin-cli -datadir=2 -getinfo

  bitcoin-cli -datadir=1 generate 1
  bitcoin-cli -datadir=2 generate 1

  bitcoin-cli -datadir=1 getnewaddress
  bitcoin-cli -datadir=2 getnewaddress

  bitcoin-cli -datadir=1 help

  bitcoin-cli -datadir=1 getbalance
  bitcoin-cli -datadir=2 getbalance

  bitcoin-cli -datadir=1 getwalletinfo
  bitcoin-cli -datadir=1 listwallets
  bitcoin-cli -datadir=1 listaccounts

  bitcoin-cli -datadir=1 getaccount "2NAW7hR3trVq9z3XnoKPdatSGSkus6qT5Gz"
  bitcoin-cli -datadir=1 getaccountaddress ""
  bitcoin-cli -datadir=1 getaccountaddress thomas
  bitcoin-cli -datadir=1 getaddressesbyaccount ""
  bitcoin-cli -datadir=1 getaddressesbyaccount thomas

  bitcoin-cli -datadir=1 getpeerinfo

  bitcoin-cli -datadir=1 gettransaction 441b096b2059eb782a4c60b7bc8538bc6cb7e0f5fb3c0e7c70aa46cfa1d55f01

  bitcoin-cli -datadir=1 getblock 
  bitcoin-cli -datadir=1 getblockchaininfo 
  bitcoin-cli -datadir=1 getblockcount 
  bitcoin-cli -datadir=1 getblockhash 107 

  1:2N1H1D5pWTJvWpeJoZYL1z4NpuW48LWvGhr
  2:2MzxYfzCuVUwaKTaxB48cX4Noh7RqcWByGM

  bitcoin-cli -datadir=1 sendtoaddress 2MzxYfzCuVUwaKTaxB48cX4Noh7RqcWByGM 10
  036b236789297d837906dd6b8a2b35913372e1a674ef699d9e9e32e5050a6f1a
  ebd1454eadd38f7ef7e3bd1988c114911899efba8510bcee5e4c44c276515764

  bitcoin-cli -datadir=1 listtransactions
  bitcoin-cli -datadir=1 getrawtransaction 036b236789297d837906dd6b8a2b35913372e1a674ef699d9e9e32e5050a6f1a
  bitcoin-cli -datadir=1 decoderawtransaction

  bitcoin-cli -datadir=1 listreceivedbyaddress

  bitcoin-cli -datadir=1 listunspent
  bitcoin-cli -datadir=1 createrawtransaction
  bitcoin-cli -datadir=1 signrawtransaction
  bitcoin-cli -datadir=1 sendrawtransaction
  bitcoin-cli -datadir=1 gettransaction

41.book
  bee run -gendoc=true -downdoc=true
  注册：
  邮箱 秘密

42.Vue Element-UI
  sudo npm install -g @vue/cli
  vue create hello-world
  npm run serve
  npm run build
  vue add element

  npm install --save axios
  npm install --save qs
  npm install --save js-sha256
  npm install --save vue-router

  git clone https://github.com/lin-xin/vue-manage-system.git (npm install & npm run serve)
  git clone https://github.com/PanJiaChen/vue-element-admin.git (npm install & npm run dev)
  git clone https://github.com/PanJiaChen/vue-admin-template.git (npm install & npm run dev)
  

43.Kubernetes
  集群部署：
  sudo usermod -aG docker hadoop
  exit
  
  docker rm `docker ps -a -q`
  docker rmi -f `docker images -q`

  sudo apt-get update
  sudo apt-get install -y apt-transport-https

  sudo su
  curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
  cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
  deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
  EOF
  exit

  sudo apt-get update
  sudo apt-get install -y kubelet kubeadm kubectl

  echo "source <(kubectl completion bash)" >> ~/.bashrc
  source ~/.bashrc

  kubeadm config images list

  k8s.gcr.io/kube-apiserver:v1.15.0
  k8s.gcr.io/kube-controller-manager:v1.15.0
  k8s.gcr.io/kube-scheduler:v1.15.0
  k8s.gcr.io/kube-proxy:v1.15.0
  k8s.gcr.io/pause:3.1
  k8s.gcr.io/etcd:3.3.10
  k8s.gcr.io/coredns:1.3.1

  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:v1.15.0
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:v1.15.0
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:v1.15.0
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.15.0
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.3.10
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.3.1

  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:v1.15.0 k8s.gcr.io/kube-apiserver:v1.15.0
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:v1.15.0 k8s.gcr.io/kube-controller-manager:v1.15.0
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:v1.15.0 k8s.gcr.io/kube-scheduler:v1.15.0
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.15.0 k8s.gcr.io/kube-proxy:v1.15.0
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 k8s.gcr.io/pause:3.1
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.3.10 k8s.gcr.io/etcd:3.3.10
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1

  docker pull quay.io/coreos/flannel:v0.11.0-amd64
  #docker save -o flannel.tar quay.io/coreos/flannel:v0.11.0-amd64
  #scp flannel.tar hadoop@slave01:/home/hadoop/kube
  #docker load -i flannel.tar

  #free
  #sudo swapon -a
  #sudo vim /etc/fstab

  未操作：{
  chrony
  iptables
  
  ipvs:
  lsmod | grep ip_vs
  lsmod | grep -e ip_vs -e nf_conntrack_ipv4
  ps -ef | grep ip_vs

  modprobe -- ip_vs
  modprobe -- ip_vs_rr
  modprobe -- ip_vs_wrr
  modprobe -- ip_vs_sh
  modprobe -- nf_conntrack_ipv4

  sudo apt-get install ipset ipvsadm
  }

  sudo swapoff -a
  sudo kubeadm init \
      --apiserver-advertise-address=192.168.1.10 \
      --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
      --kubernetes-version v1.15.0 \
      --pod-network-cidr=10.244.0.0/16

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

  sudo kubeadm join 192.168.1.10:6443 --token km2889.j89uvmvccd3p9uog \
    --discovery-token-ca-cert-hash sha256:ee216c2f3767a9e12987973e96228cc6ca6cd1cd24a21aebaac60d77501e3824

  kubectl get nodes
  kubectl get cs
  kubectl describe node ubuntuthomas
  kubectl get pod -n kube-system -o wide
  kubeadm token list
  kubeadm token create --print-join-command
  kubectl get pod --all-namespaces -o wide
  kubectl describe pod kube-flannel-ds-amd64-rxvvg --namespace=kube-system
  
  kubectl create deployment nginx --image=nginx:alpine
  kubectl get deployments
  kubectl delete deployment ubuntu
  kubectl scale deployment nginx --replicas=2
  kubectl get pods -l app=nginx -o wide
  kubectl expose deployment nginx --port=80 --type=NodePort
  kubectl get services nginx

  kubectl run -it curl --image=radial/busyboxplus:curl
  nslookup nginx
  curl http://nginx/
  curl 10.244.1.2

  #kubectl taint node k8s-master node-role.kubernetes.io/master-
  #kubectl taint node k8s-master node-role.kubernetes.io/master=:NoSchedule

  #kubectl drain slave01 --delete-local-data --force --ignore-daemonsets
  #kubectl delete node slave01

  #sudo kubeadm reset
  #sudo rm -rf /etc/kubernetes
  #sudo rm -rf /var/lib/kubelet

  #rm -rf $HOME/.kube
  #sudo rm -rf /var/lib/etcd

  #rm -rf $HOME/.helm

Helm:
  tar -xzvf helm-v2.12.3-linux-amd64.tar.gz
  sudo cp linux-amd64/helm /usr/local/bin

  sudo apt-get install socat
  kubectl config view

  #create a role and service account name
  vim ~/helm/rbac-config.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: tiller
    namespace: kube-system
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: tiller
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
    - kind: ServiceAccount
      name: tiller
      namespace: kube-system

  #kubectl create -f rbac-config.yaml
  #helm init --service-account tiller

  helm version #注意tiller版本
  kubectl create serviceaccount --namespace kube-system tiller
  kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

  helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.12.3  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

  kubectl get pods -n kube-system
  kubectl describe pod tiller-deploy-5687f55748-gbxwg --namespace=kube-system
  
  helm repo remove stable
  helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
  helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/
  helm repo update
  helm repo list

  helm search
  helm search incubator
  helm list

  helm reset --force
  rm -rf $HOME/.helm

  #helm mysql
  sudo /etc/init.d/mysql stop

  #sudo iptables -P FORWARD ACCEPT
  kubectl get pv
  kubectl get pvc

  helm install --name my-release stable/mysql
  helm install --name my-release \
  --set mysqlRootPassword=secretpassword,mysqlUser=my-user,mysqlPassword=my-password,mysqlDatabase=my-database \
    stable/mysql
  helm install --name my-release -f values.yaml stable/mysql

  helm ls --all
  kubectl get pod --all-namespaces -o wide
  kubectl describe pod my-release-mysql-86547664d5-z22sl

  #helm del --purge my-release
  #helm status my-release

  MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default my-release-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo)

  kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il
  apt-get update && apt-get install mysql-client -y
  mysql -h my-release-mysql -p

  MYSQL_HOST=127.0.0.1
  MYSQL_PORT=3306
  export POD_NAME=$(kubectl get pods --namespace default -l "app=my-release-mysql" -o jsonpath="{.items[0].metadata.name}")
  kubectl port-forward $POD_NAME 3306:3306
  mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}

  kubectl delete pod ubuntu

44.Mesos&Marathon

45.OpenStack

46.ipfs
  ipfs init  QmZsv3ZxUJyTUGvzbC7WUWbLGCUMhyFkRy1Cihdtun43KL
  ipfs init --profile server

  ipfs id

  ipfs cat /ipfs/QmS4ustL54uo8FzR9455qaxZwuMiUhyvMcX9Ba8nUH4uVv/readme
  ipfs cat /ipfs/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/readme

  ipfs cat /ipfs/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/quick-start

  ipfs daemon
  ipfs swarm peers

  ipfs cat /ipfs/QmW2WQi7j6c7UgJTarActp7tDNikE4B2qXtFCfLPdsgaTQ/cat.jpg >cat.jpg

  hash=`echo "I <a3 IPFS -$(whoami)" | ipfs add -q`
  ipfs cat /ipfs/$hash

  curl "https://ipfs.io/ipfs/$hash"
  curl "http://127.0.0.1:8080/ipfs/$hash"

  curl http://192.168.1.136:5001/webui
  curl http://127.0.0.1:5001/webui


  ipfs add myfile.txt QmQECo2p8LdVcjtkEWDVNVM7Hrsc7arW52P5vz5BVuvEgR
  ipfs name publish　/ipfs/QmQECo2p8LdVcjtkEWDVNVM7Hrsc7arW52P5vz5BVuvEgR
  ipfs name resolve QmQQ5t88W44Je5WgvmgpV1xSZTg1y5UXdYFHcQQ7EayxwR
  https://ipfs.io/ipfs/QmQECo2p8LdVcjtkEWDVNVM7Hrsc7arW52P5vz5BVuvEgR
  https://ipfs.io/ipns/QmQQ5t88W44Je5WgvmgpV1xSZTg1y5UXdYFHcQQ7EayxwR

  ipfs key gen --type=rsa --size=2048 keyname
  ipfs key list -l
  ipfs name publish --key=keyname  QmPoyokqso3BKYCqwiU1rspLE59CPCv5csYhcPkEd6xvtm
  ipfs resolve /ipns/QmYTpEqtNYvNFUwFysuRsEufNDJJBBEJfqsqrFXDT93sEE
  
47.Network
  #sudo vim /etc/init.d/rc.local
  #sudo ifconfig ens33 down
  #sudo ifconfig ens33 hw ether 00:0c:29:9e:42:56
  #sudo ifconfig ens33 up
  #cat /sys/class/net/ens33/address

  sudo vim /etc/network/interfaces
  auto ens33
  # iface ens33 inet dhcp
  iface ens33 inet static
  address 192.168.1.146
  netmask 255.255.255.0
  gateway 192.168.1.1
  hwaddress ether 00:0c:29:9e:42:56

  sudo vim /etc/resolvconf/resolv.conf.d/base
  nameserver 192.168.1.146
  nameserver 114.114.114.114

  sudo vim /etc/hostname
  ubuntuthomas1

  sudo vim /etc/hosts
  127.0.0.1   localhost
  127.0.1.1   ubuntuthomas1 delete

  192.168.1.145    ubuntuthomas
  192.168.1.146    ubuntuthomas1
  192.168.1.147    ubuntuthomas2

  #sudo /etc/init.d/networking restart
  sudo reboot

48.Hadoop
  sudo useradd -m hadoop -s /bin/bash
  sudo passwd hadoop
  sudo adduser hadoop sudo

  sudo chmod u+w /etc/sudoers
  sudo vim /etc/sudoers
  hadoop  ALL=(ALL:ALL) ALL
  sudo chmod u-w /etc/sudoers

  su - hadoop

  sudo apt-get install openssh-server
  ssh localhost
  exit
  cd ~/.ssh/
  ssh-keygen -t rsa
  cat ./id_rsa.pub >> ./authorized_keys
  ssh localhost

  http://mirrors.hust.edu.cn/apache/hadoop/common/
  sudo tar -zxvf  hadoop-2.6.0.tar.gz -C /usr/local
  cd /usr/local
  sudo mv hadoop-2.6.0 hadoop
  sudo chown -R hadoop:hadoop hadoop/

  vim ~/.bashrc
  export HADOOP_HOME=/usr/local/hadoop
  export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
  export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
  export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

  export HADOOP_INSTALL=$HADOOP_HOME
  export HADOOP_MAPRED_HOME=$HADOOP_HOME
  export HADOOP_COMMON_HOME=$HADOOP_HOME
  export HADOOP_HDFS_HOME=$HADOOP_HOME
  export YARN_HOME=$HADOOP_HOME

  vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh
  export JAVA_HOME=/usr/local/java/jdk1.8.0_202
  export HADOOP_OPTS="-Djava.library.path=${HADOOP_HOME}/lib/native"

  vim /usr/local/hadoop/etc/hadoop/core-site.xml
  <configuration>
    <property>
      <name>hadoop.tmp.dir</name>
      <value>file:/usr/local/hadoop/tmp</value>
      <description>Abase for other temporary directories.</description>
    </property>
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://localhost:9000</value>
    </property>
  </configuration>

  vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml
  <configuration>
    <property>
      <name>dfs.replication</name>
      <value>1</value>
    </property>
    <property>
      <name>dfs.namenode.name.dir</name>
      <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
  </configuration>

  集群部署：
  vim etc/hadoop/core-site.xml
  <configuration>
    <property>
      <name>hadoop.tmp.dir</name>
      <value>file:/usr/local/hadoop/tmp</value>
      <description>Abase for other temporary directories.</description>
    </property>
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://ubuntuthomas:9000</value>
    </property>
  </configuration>

  vim etc/hadoop/hdfs-site.xml
  <configuration>
    <property>
      <name>dfs.replication</name>
      <value>2</value>
    </property>
    <property>
      <name>dfs.namenode.name.dir</name>
      <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
    <property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>ubuntuthomas:9001</value>
    </property>
  </configuration>

  vim etc/hadoop/mapred-site.xml
  <configuration>
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
  </configuration>

  <property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <property>
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>

  vim etc/hadoop/yarn-site.xml
  <configuration>
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>ubuntuthomas</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.log-aggregation-enable</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.log-aggregation.retain-seconds</name>
      <value>604800</value>
    </property>
  </configuration>

  vim etc/hadoop/workers
  ubuntuthomas1
  ubuntuthomas2

  scp -r etc/hadoop/* hadoop@ubuntuthomas1:/usr/local/hadoop/etc/hadoop/
  scp -r etc/hadoop/* hadoop@ubuntuthomas2:/usr/local/hadoop/etc/hadoop/

  rm -rf logs tmp
  hdfs namenode -format
  start-all.sh
  stop-all.sh

  echo "My name is Li Thomas. This is a example program called WordCount, run by Li Thomas " >> testWordCount
  hadoop fs -mkdir /wordCountInput
  hadoop fs -put testWordCount /wordCountInput
  hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar wordcount /wordCountInput /wordCountOutput
  hadoop fs -ls /wordCountOutput
  hadoop fs -cat /wordCountOutpart-r-00000

49.Hive
  mysql -u root -p
  create user 'hive'@'%' identified by 'hive';
  select Host,User from mysql.user;
  grant all privileges on *.* to 'hive'@'%' with grant option;
  flush privileges;
  create database if not exists hive_metadata;
  grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive';
  grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive';
  flush privileges;
  exit
  sudo /etc/init.d/mysql restart
  mysql -u hive -p hive
  show databases;
  #create database hive_metadata;

  http://www.eu.apache.org/dist/hive/
  sudo tar -zxvf apache-hive-2.3.4-bin.tar.gz -C /usr/local
  mv apache-hive-2.3.4-bin hive
  sudo chown -R hadoop:hadoop hive/
  cp hive-default.xml.template hive-site.xml

  vim conf/hive-site.xml
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive_metadata?createDatabaseIfNotExist=true</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
  </property>

  <property>
    <name>system:java.io.tmpdir</name>
    <value>/usr/local/hive/tmpdir</value>
  </property>

  <property>
    <name>system:user.name</name>
    <value>hive</value>
  </property>

  <--property>
    <name>datanucleus.schema.autoCreateAll</name>
    <value>true</value>
  </property>

  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>

  cp hive-env.sh.template hive-env.sh
  vim hive-env.sh
  export HADOOP_HOME=/usr/local/hadoop
  export HIVE_CONF_DIR=/usr/local/hive/conf

  vim hive-config.sh
  export JAVA_HOME=/usr/local/java/jdk1.8.0_91
  export HIVE_HOME=/usr/local/hive
  export HADOOP_HOME=/usr/local/hadoop

  vim ~/.bashrc
  export HIVE_HOME=/usr/local/hive
  export PATH=$PATH:$HIVE_HOME/bin

  cd /usr/local/hive
  wget http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz
  tar -zxvf mysql-connector-java-5.1.47.tar.gz
  cp mysql-connector-java-5.1.47-bin.jar ../lib/

  schematool -dbType mysql -initSchema
  hive

50.HBase
  wget http://mirror.bit.edu.cn/apache/hbase/1.4.9/hbase-1.4.9-bin.tar.gz

  sudo tar -zxvf hbase-1.4.9-bin.tar.gz -C /usr/local/
  sudo mv hbase-1.4.9 hbase
  sudo chown -R hadoop:hadoop hbase/

  vim ~/.bashrc
  export HBASE_HOME=/usr/local/hbase
  export PATH=$PATH:$HBASE_HOME/bin

  vim conf/hbase-env.sh
  export JAVA_HOME=/usr/local/java/jdk1.8.0_91
  export HBASE_CLASSPATH=/usr/local/hbase/conf
  export HBASE_MANAGES_ZK=false

  vim conf/hbase-site.xml
  <configuration>
    <property>
      <name>hbase.master</name>
      <value>thomas:60000</value>
    </property>
    <property>
      <name>hbase.cluster.distributed</name>
      <value>true</value>
    </property>
    <property>
      <name>hbase.rootdir</name>
      <value>hdfs://thomas:9000/hbase</value>
    </property>

    <property>
      <name>hbase.master.maxclockskew</name>
      <value>180000</value>
    </property>

    <property>
      <name>hbase.zookeeper.quorum</name>
      <value>thomas:2181,slave01:2181,slave02:2181</value>
      <description>The directory shared by RegionServers.
      </description>
    </property>

    <property>
      <name>hbase.zookeeper.property.clientPort</name>
      <value>2181</value>
    </property>

    <property>
      <name>hbase.zookeeper.property.dataDir</name>
      <value>/usr/local/zookeeper/data</value>
      <description>Property from ZooKeeper config zoo.cfg.
      The directory
      where the snapshot is stored.
      </description>
    </property>

  </configuration>

  vim conf/regionservers
  slave01
  slave02

  scp -r hbase/* hadoop@slave01:/usr/local/hbase/
  scp -r hbase/* hadoop@slave02:/usr/local/hbase/

  start-hbase.sh
  hbase shell
  create 'member', 'm_id', 'address', 'info'
  list

51.Scala
  java -version
  sudo tar -zxvf scala-2.12.8.tgz -C /usr/local
  sudo mv scala-2.12.8 scala

  vim ~/.bashrc
  export SCALA_HOME=/usr/local/scala
  export PATH=$PATH:$SCALA_HOME/bin

  cd /usr/local
  sudo mkdir scala
  sudo chown -R hadoop:hadoop scala

  scp -r scala/* hadoop@slave01:/usr/local/scala/


52.Spark
  sudo tar -zxvf spark-2.4.0-bin-hadoop2.7.tgz -C /usr/local
  sudo mv spark-2.4.0-bin-hadoop2.7 spark
  sudo chown -R hadoop:hadoop spark

  cp spark-env.sh.template spark-env.sh
  vim conf/spark-env.sh
  export SCALA_HOME=/usr/local/scala
  export JAVA_HOME=/usr/local/java/jdk1.8.0_91
  export HADOOP_HOME=/usr/local/hadoop
  export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
  SPARK_LOCAL_DIRS=/usr/local/spark
  SPARK_DRIVER_MEMORY=1G
  export LD_LIBRARY_PATH=/usr/local/hadoop/lib/native/:$LD_LIBRARY_PATH

  export SPARK_MASTER_IP=192.168.1.7
  export SPARK_MASTER_HOST=192.168.1.7
  export SPARK_LOCAL_IP=192.168.1.7        #每个节点不一样
  export SPARK_WORKER_MEMORY=1g
  export SPARK_WORKER_CORES=2
  export SPARK_HOME=/usr/local/spark
  export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)

  cp slaves.template slaves
  vim slaves
  slave01
  slave02

  sudo mkdir spark
  sudo chown -R hadoop:hadoop spark

  scp -r spark/* hadoop@slave01:/usr/local/spark/
  scp -r spark/* hadoop@slave02:/usr/local/spark/

  cd /usr/local/spark/sbin
  ./start-all.sh

  cd /usr/local/spark/bin
  ./run-example SparkPi 10 --slave01 local[2]

53.go-micro
  protobuf --> protoc
  protobuf+golang --> protoc-gen-go
  protobuf+golang+grpc --> protoc-gen-go

  go + protobuf:
  protoc --go_out=. *.proto
  
  go + grpc + protobuf:
  protoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld

  unzip consul_1.4.4_linux_amd64.zip
  cp consul ~/golang/bin/
  consul agent -dev
  consul members -detailed
  curl localhost:8500/v1/catalog/nodes
  dig @127.0.0.1 -p 8600 thomas.node.consul

  consul kv put redis/config/minconns 1
  consul kv get redis/config/minconns
  consul kv get -recurse
  
  go run main.go --registry=consul

54.etcd
  WAL：Write Ahead Log(预写式日志)
  Snapshot、Entry、HTTP Server、Store、Raft
  静态配置启动、etcd自身服务发现、通过DNS进行服务发现
  
  tar -xzvf etcd-v3.3.13-linux-amd64.tar.gz -C ~/server/
  ./etcd
  export ETCDCTL_API=3
  ./etcdctl --endpoints=localhost:2379 put key value
  ./etcdctl --endpoints=localhost:2379 get key

  rm -rf /var/lib/etcd/member/*

  scp etcd-v3.3.13-linux-amd64.tar.gz thomas@node01:/home/thomas/server
  scp etcd-v3.3.13-linux-amd64.tar.gz thomas@node02:/home/thomas/server
  tar -xzvf etcd-v3.3.13-linux-amd64.tar.gz

  cd /home/thomas/server/etcd-v3.3.13-linux-amd64
  rm -rf data.etcd

  nohup ./etcd --data-dir=data.etcd --name et-wang-1 \
    --initial-advertise-peer-urls http://192.168.1.7:2380 --listen-peer-urls http://192.168.1.7:2380 \
    --advertise-client-urls http://192.168.1.7:2379 --listen-client-urls http://192.168.1.7:2379 \
    --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-3=http://192.168.1.11:2380 \
    --initial-cluster-state new \
    --initial-cluster-token token-wang-group-1 \
    >>etcd.log 2>&1 &

  nohup ./etcd --data-dir=data.etcd --name et-wang-2 \
    --initial-advertise-peer-urls http://192.168.1.10:2380 --listen-peer-urls http://192.168.1.10:2380 \
    --advertise-client-urls http://192.168.1.10:2379 --listen-client-urls http://192.168.1.10:2379 \
    --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-3=http://192.168.1.11:2380 \
    --initial-cluster-state new \
    --initial-cluster-token token-wang-group-1 \
    >>etcd.log 2>&1 &


  nohup ./etcd --data-dir=data.etcd --name et-wang-3 \
    --initial-advertise-peer-urls http://192.168.1.11:2380 --listen-peer-urls http://192.168.1.11:2380 \
    --advertise-client-urls http://192.168.1.11:2379 --listen-client-urls http://192.168.1.11:2379 \
    --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-3=http://192.168.1.11:2380 \
    --initial-cluster-state new \
    --initial-cluster-token token-wang-group-1 \
    >>etcd.log 2>&1 &


  export ETCDCTL_API=3
  ./etcdctl --endpoints=192.168.1.7:2379 member list
  ./etcdctl --endpoints=192.168.1.7:2379 put wang king
  ./etcdctl --endpoints=192.168.1.7:2379 get wang
  ./etcdctl --endpoints=192.168.1.10:2379 get wang

  ./etcdctl --endpoints=192.168.1.7:2379 put web1 value1
  ./etcdctl --endpoints=192.168.1.7:2379 put web2 value2
  ./etcdctl --endpoints=192.168.1.7:2379 put web3 value3
  ./etcdctl --endpoints=192.168.1.7:2379 get web --prefix

  ./etcdctl --endpoints=192.168.1.7:2379 put key beer
  ./etcdctl --endpoints=192.168.1.7:2379 get key
  ./etcdctl --endpoints=192.168.1.7:2379 del key

  ./etcdctl --endpoints=192.168.1.7:2379 put k1 value1
  ./etcdctl --endpoints=192.168.1.7:2379 put k2 value2
  ./etcdctl --endpoints=192.168.1.7:2379 get k --prefix
  ./etcdctl --endpoints=192.168.1.7:2379 del k --prefix

  ./etcdctl --endpoints=192.168.1.7:2379 put user1 bad
  ./etcdctl --endpoints=192.168.1.7:2379 txn --interactive
  compares:
  value("user1") = "bad"
  success requests (get, put, delete):
  del user1
  failure requests (get, put, delete):
  put user1 good

  ./etcdctl --endpoints=192.168.1.7:2379 get user1

  ./etcdctl --endpoints=192.168.1.7:2379 watch stock1
  ./etcdctl --endpoints=192.168.1.7:2379 put stock1 1000
  ./etcdctl --endpoints=192.168.1.7:2379 watch stock --prefix
  ./etcdctl --endpoints=192.168.1.7:2379 put stock1 10
  ./etcdctl --endpoints=192.168.1.7:2379 put stock2 20

  ./etcdctl --endpoints=192.168.1.7:2379 lease grant 30
  ./etcdctl --endpoints=192.168.1.7:2379 put sample value --lease=5e156b838cfa311e
  ./etcdctl --endpoints=192.168.1.7:2379 get sample
  ./etcdctl --endpoints=192.168.1.7:2379 lease keep-alive 5e156b838cfa311e
  ./etcdctl --endpoints=192.168.1.7:2379 lease revoke 5e156b838cfa311e
  ./etcdctl --endpoints=192.168.1.7:2379 get sample

  ./etcdctl --endpoints=192.168.1.7:2379 lock mutex1

  ./etcdctl --endpoints=192.168.1.7:2379,192.168.1.10:2379,192.168.1.11:2379 --write-out=table endpoint status
  ./etcdctl --endpoints=192.168.1.7:2379 endpoint health

  ./etcdctl --endpoints=192.168.1.7:2379 elect one p1

  ./etcdctl --endpoints=192.168.1.7:2379 snapshot save my.db
  ./etcdctl --endpoints=192.168.1.7:2379 --write-out=table snapshot status my.db

  ./etcdctl --endpoints=192.168.1.7:2379 member list
  ./etcdctl --endpoints=192.168.1.7:2379 member remove 1e91165f86b1da62
  ./etcdctl --endpoints=192.168.1.7:2379 member add et-wang-4 --peer-urls=http://192.168.1.11:2380

  rm -rf data.etcd
  ./etcd --data-dir=data.etcd --name et-wang-4 \
    --initial-advertise-peer-urls http://192.168.1.11:2380 --listen-peer-urls http://192.168.1.11:2380 \
    --advertise-client-urls http://192.168.1.11:2379 --listen-client-urls http://192.168.1.11:2379 \
    --initial-cluster et-wang-1=http://192.168.1.7:2380,et-wang-2=http://192.168.1.10:2380,et-wang-4=http://192.168.1.11:2380 \
    --initial-cluster-state existing \
    --initial-cluster-token token-wang-group-1

  ./etcdctl --endpoints=192.168.1.7:2379 role add root
  ./etcdctl --endpoints=192.168.1.7:2379 role grant-permission root readwrite fookey
  ./etcdctl --endpoints=192.168.1.7:2379 role get root
  ./etcdctl --endpoints=192.168.1.7:2379 role list

  ./etcdctl --endpoints=192.168.1.7:2379 user add root
  ./etcdctl --endpoints=192.168.1.7:2379 user grant-role root root
  ./etcdctl --endpoints=192.168.1.7:2379 user get root
  ./etcdctl --endpoints=192.168.1.7:2379 user list

  ./etcdctl --endpoints=192.168.1.7:2379 auth enable

  ./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 put fookey people
  ./etcdctl --endpoints=192.168.1.7:2379 get fookey
  ./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 get fookey
  ./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 get foo1

  ./etcdctl --endpoints=192.168.1.7:2379 --user=root:123456 auth disable


  go build -o app *.go
  cd golang/src/
  mkdir -p ant/conf

  scp app thomas@node01:/home/thomas/golang/src/ant
  scp conf/app.yaml thomas@node01:/home/thomas/golang/src/ant/conf
  scp app thomas@node02:/home/thomas/golang/src/ant
  scp conf/app.yaml thomas@node02:/home/thomas/golang/src/ant/conf

55.GRPC
  service
  method
  message

  Message
  NewMessage
  NewMessageWithExtensionRegistry
  NewMessageWithMessageFactory
  AsDynamicMessage
  AsDynamicMessageWithExtensionRegistry
  AsDynamicMessageWithMessageFactory

  Stub
  NewStub
  NewStubWithMessageFactory

  MessageFactory
  NewMessageFactoryWithExtensionRegistry
  NewMessageFactoryWithKnownTypeRegistry
  NewMessageFactoryWithDefaults
  NewMessageFactoryWithRegistries

  ExtensionRegistry
  NewExtensionRegistryWithDefaults

  KnownTypeRegistry
  NewKnownTypeRegistryWithDefaults
  NewKnownTypeRegistryWithoutWellKnownTypes

  MessageRegistry
  NewMessageRegistryWithDefaults

56.ELK
Elasticsearch:
  sudo dpkg -i elasticsearch-7.2.0-amd64.deb

  sudo systemctl daemon-reload
  sudo systemctl enable elasticsearch.service
  sudo systemctl start elasticsearch.service
  sudo systemctl stop elasticsearch.service

  sudo /etc/init.d/elasticsearch start

  curl -X GET "localhost:9200/"

  /etc/elasticsearch
  /usr/share/elasticsearch
  /var/log/elasticsearch/

  path.data: /var/lib/elasticsearch
  path.logs: /var/log/elasticsearch

Kibana:
  sudo dpkg -i kibana-7.2.0-amd64.deb
  sudo systemctl daemon-reload
  sudo systemctl enable kibana.service
  sudo systemctl start kibana.service
  sudo systemctl stop kibana.service

  /var/log/kibana/
  /etc/kibana/kibana.yml
  /var/lib/kibana path.data

  192.168.1.7:5601

Metricbeat:
  sudo dpkg -i metricbeat-7.2.0-amd64.deb

  sudo metricbeat modules enable system
  sudo metricbeat setup -e
  sudo service metricbeat start
  sudo service metricbeat stop
  sudo service metricbeat restart

  ps -ef | grep metricbeat
  sudo vim /etc/metricbeat/metricbeat.yml

  http://192.168.1.7:5601/app/kibana#/dashboard/Metricbeat-system-overview-ecs

Filebeat:
  sudo dpkg -i filebeat-7.2.0-amd64.deb

  /usr/share/filebeat
  sudo vim /etc/filebeat/filebeat.yml

  sudo service filebeat start
  sudo service filebeat stop
  sudo service filebeat restart

  sudo filebeat setup -e \
    -E output.logstash.enabled=false \
    -E output.elasticsearch.hosts=['localhost:9200'] \
    -E output.elasticsearch.username=filebeat_internal \
    -E output.elasticsearch.password=YOUR_PASSWORD \
    -E setup.kibana.host=localhost:5601

  ps -ef | grep filebeat

Logstash:
  sudo -E dpkg -i logstash-7.2.0.deb
  #sudo /etc/init.d/logstash start
  #sudo systemctl start logstash.service

  ps -ef | grep logstash
  ps aux | grep pid
  netstat -tunlp | grep port
  
  vim /etc/logstash/metrics-pipelines.conf
  cd /usr/share/logstash
  sudo -E ./bin/logstash -f /etc/logstash/metrics-pipelines.conf

57.Prometheus
  tar -xzvf prometheus-2.11.1.linux-amd64.tar.gz -C /home/thomas/server/
  vim prometheus.yml
  ./prometheus --config.file=prometheus.yml

  192.168.1.7:9090
  192.168.1.7:9090/metrics
  192.168.1.7:9090/graph

  git clone https://github.com/prometheus/client_golang.git
  cd client_golang/examples/random
  go get -d
  go build

  ./random -listen-address=:8080
  ./random -listen-address=:8081
  ./random -listen-address=:8082

  docker run -p 9090:9090 prom/prometheus
  docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus
  docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus

Grafana:
  sudo dpkg -i grafana_6.2.5_amd64.deb

  sudo service grafana-server start
  admin / admin
  sudo update-rc.d grafana-server defaults

  sudo systemctl daemon-reload
  sudo systemctl start grafana-server
  sudo systemctl status grafana-server
  sudo systemctl enable grafana-server.service

  sudo vim /etc/grafana/grafana.ini

  http://192.168.1.7:3000/

  docker run -d -p 3000:3000 --name grafana grafana/grafana

InfluxDB:

58.Supervisor
  //sudo pip install --upgrade pip
  //sudo npm uninstall -g supervisor

  sudo pip install supervisor
  echo_supervisord_conf

  mkdir ~/supervisor
  cd ~/supervisor
  echo_supervisord_conf > supervisord.conf

  supervisord -c supervisord.conf
  ps -ef | grep supervisor
  supervisorctl -c supervisord.conf
  
59.MicroService
  #docker run -d -p 8080:8080 ants/app01:v1.0
  #docker run -i -t ants/app01:v1.0 /bin/bash
  #docker exec -it d92a084a8d89 /bin/sh 
  #docker exec -it ants/app01:v1.0 /bin/bash
  #docker save -o app01.tar ants/app01:v1.0
  #scp app01.tar thomas@node01:/home/thomas/golang/src/app01/
  #scp app01.tar thomas@node02:/home/thomas/golang/src/app01/
  #docker load -i app01.tar
  #dig -t A draveness.me +trace
  #dig +trace +additional draveness.me

  CGO_ENABLED=0 go build -o app01 main.go
  docker build -t ants/app01:v1.0 .

  scp * thomas@node01:/home/thomas/golang/src/app01/
  kubectl apply -f app01-deployment.yaml
  kubectl apply -f app01-service.yaml

  hostname:foo
  subdomain:bar
  namespace:my-namespace
  foo.bar.my-namespace.svc.cluster-domain.example

  kubectl get nodes
  kubectl get pods --all-namespaces -o wide
  kubectl get deployments --all-namespaces -o wide
  kubectl get services --all-namespaces -o wide
  kubectl get configmaps --all-namespaces -o wide
  kubectl get ingress --all-namespaces -o wide
  kubectl describe pod app01-deployment-d68448f69-484pp
  kubectl describe ingress ants-ingress
  kubectl -n kube-system get configmap coredns -oyaml
  kubectl delete --namespace=kube-system deployment coredns

  kubectl exec -ti app02-deployment-5989c68c56-zw6ff -- /bin/sh
  kubectl exec -ti nginx-ingress-controller-7995bd9c47-2n9v4 -n ingress-nginx -- /bin/bash

  curl 10.110.76.133:8080/ping
  curl 10.99.236.149:8081/ping
  curl 192.168.1.10:30001/ping
  curl 192.168.1.10:30002/ping
  curl nginx.default.svc.cluster.local
  curl http://app01-service.default.svc.cluster.local:8080/ping
  curl app01.app.com:30367/ping
  curl app02.app.com:30367/ping

  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

  POD_NAME=$(kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].metadata.name}')
  kubectl exec -it $POD_NAME -n ingress-nginx -- /nginx-ingress-controller --version

60.Istio
  control plane
  data plane
  Envoy
  sidecar
  Mixer
  Pilot
  Galley
  Citadel
  inbound and outbound

Envoy:
  docker pull envoyproxy/envoy:v1.11.1
  docker run --rm -d -p 10000:10000 envoyproxy/envoy:v1.11.1
  curl -v localhost:10000

61.Go Module
  export GO111MODULE=on
  export GOPROXY=https://goproxy.io

  go mod init packagename
  go mod download
  go mod tidy
  go get packagename@v1.2.3
  go mod vendor
  
62.Ceph
  HDFS
  Fastdfs
  Minio
  
63.NSQ
  tar -xzvf nsq-1.2.0.linux-amd64.go1.12.9.tar.gz
  cd /home/thomas/server/nsq-1.2.0.linux-amd64.go1.12.9/bin
  ./nsqlookupd
  ./nsqd --lookupd-tcp-address=127.0.0.1:4160
  ./nsqadmin --lookupd-http-address=127.0.0.1:4161
  ./nsq_to_file --topic=test --output-dir=/tmp --lookupd-http-address=127.0.0.1:4161
  curl -d 'hello world 1' 'http://127.0.0.1:4151/pub?topic=test'
  http://192.168.1.7:4171/

  curl -X POST http://127.0.0.1:4151/channel/create?topic=name&channel=name
  curl http://127.0.0.1:4151/stats
  curl http://127.0.0.1:4161/lookup
  curl http://127.0.0.1:4161/topics
  curl http://127.0.0.1:4161/channels
  curl http://127.0.0.1:4161/nodes

64.System
  电商系统 淘宝 京东 卖票 买书
  及时通讯 QQ 微信
  社交系统 微博 抖音
  视频网站 爱奇艺
  工具应用 地图 滴滴 ofo

  用户 商品 订单 支付 评价 财务 活动 客服 会员
  搜索 购买 支付 发货 收货 退货 退款

  用户 商品 订单

  卖家
  品牌 荣耀
  名称 mate30
  型号 128G
  种类 手机

  grpcurl -plaintext -d '{"phoneNumber":"12465465461", "name":"xiaoming"}' 127.0.0.1:6068 user.UserService/AddUser
  grpcurl -plaintext -d '{"userID":"fsadfgsdfsadf"}' 127.0.0.1:6068 user.UserService/DelUser
  grpcurl -plaintext -d '{"userID":"3258a18a000147adb3c47d88fce9115a"}' 127.0.0.1:6068 user.UserService/GetUser
  grpcurl -plaintext -d '{"userID":"18931f8fd0134a35bf7fcd2137254fca", "userInfo":{"name":"xiaowang"}}' 127.0.0.1:6068 user.UserService/ModifyUserInfo
  grpcurl -plaintext -d '{"phoneNumber":"12465465461"}' 127.0.0.1:6068 user.UserService/GetUserIdByPhoneNumber
  grpcurl -plaintext -d '{"name":"xiaoming"}' 127.0.0.1:6068 user.UserService/GetUsersByName

  mysql
  etcd
  redis
  mongodb
